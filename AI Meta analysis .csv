Title,Source,Author,Abstract,Year,Full article ,Web_Address
Impact of artificial intelligence on business strategy in emerging markets: A conceptual framework and future research directions,ResearchGate,"Zhilin Yang, Michael Hyman, Xinyyue Zhou, Gang Li","EDITORIAL on Artificial intelligence significantly impacts business strategy in emerging markets, and the proposed conceptual model helps firms understand how to adopt AI effectively for competitive advantage by considering regulatory, normative, and cultural factors, while maintaining efficiency and legitimacy.",2022,"Guest editorial: Impact of artificial intelligence on business strategy
in emerging markets: a conceptual framework and future research directions
1. Introduction
With the rapid advancement of artificial intelligence (AI) technologies, businesses in emerging markets are embracing AI applications to enhance their productivity and footprints. Recent developments in machine and deep learning have revolutionized cognitive computing and natural language processing, laying the foundations for proliferating AI business applications in nations like China (Bughin et al., 2017). AI is poised to reshape emerging markets; for example, finance, labor, human resource management, marketing, advertising, business strategy, supply chain management, services, retail and information systems. Consequently, savvy entrepreneurs are pondering the use of facial, image and speech recognition applications to mitigate costs and barriers, enhancing their productivity.
In emerging markets, AI provides a technological solution to the economic and social challenges faced by governments, firms and people at the bottom of the economic pyramid. Integrating data from multiple sources (e.g. websites, social media and traditional channels) can help firms build data management platforms, develop sound business strategies, lower barriers to doing business, create innovative business models and spur economic development (Arora and Rahman, 2017). Firms in developing countries may use innovative AI-based solutions to enhance autonomous goods and service delivery, implement production automation and develop mobile AI apps for services and credit access. AI-based technologies can create opportunities and expand markets by enhancing productivity, business process automation, financial solutions and government services. Powered by AI, emerging markets’ public and private sectors can find leapfrogging solutions and work together to reduce poverty and inequality while boosting economic mobility and prosperity New adoption, utilization, integration and implementation challenges have arisen in emerging markets as businesses embrace AI solutions. Conceptual studies have addressed the challenges of AI in services personalization, advertising sales management), industrial marketing), automation in business logistics systems (market research, smart warehousing readiness), AI platforms tourism management medical care (marketing strategy and ethics Nonetheless, academic studies about business-related AI in emerging markets remain scant. Institutional environments in developing countries differ vastly from those in developed countries, creating novel obstacles and legitimacy issues for AI-related business applications
Hence, more theoretical and empirical studies to tackle the challenges of AI in emerging markets are needed.
2. A conceptual model of AI-driven business strategy in emerging markets
Following the logic grounding the conceptual frameworks in Yang and Su (2013, 2014), we created a model that depicts AI-driven business strategy development in emerging markets. As sh our model highlights the critical role of institutional environments in shaping AI-driven business strategies and their impacts on the efficiency and legitimacy of AI-driven business.
We posit that emerging markets’ cultural, technological, social, economic and religious forces shape AI-related institutions (Yang and Su, 2013; Zhou et al., 2015). Businesses concerned about the legal, ethical and social issues created by AI applications must consider these forces to understand AI’s effect on the welfare of workers and consumers at the economic pyramid’s bottom . For example, all businesses, especially MNCs, should attend to economic inequality and the negative externalities of jobs returning from emerging markets to mature markets due to AI-induced productivity gains. We contend that businesses should ponder whether opaque AI applications will erode human self-determination
Regarding institutional influence on AI-driven business strategy, the three institutional pillars posited by Scott (2008) – regulative, normative and cultural-cognitive – provide a useful framework. Regulative institutions are the laws and regulations set by governments. Central and local governments are vital to delineating legal foundations, establishing industrial policies, providing capital and creating a roadmap to harness AI-related business opportunities in emerging markets. Normative institutions establish industrial associations’ norms and standards for data access, data privacy, security and public trust. Cultural- cognitive institutions refer to AI-related business behavior; for example, businesspeople and consumers may distrust AI-related technologies and AI-made decisions.
Scott’s institutions and stakeholders, such as governments, communities, non- government organizations (NGOs) and investors, are mutually influential. These dynamic influences can inform managers’ interpretations and evaluations of Scott’s institutions on AI- based business strategies. Specifically, do institutions impose constraints on or function as


Drivers of AI-related Institutions in Emerging Markets (e.g., Culture, Society, Economy, Technology, Philosophy, Religion)

  
Institutional Environments in Emerging Markets
     
Stakeholders in the Market (e.g., government, NGO, community, consumer, shareholder)

Cultural–
 
Regulative Normative Institutions Institutions Institutions
cognitive
     
Figure 1.
Conceptual model for AI-Driven business strategy in emerging markets
Institutional Logics
  
Legitimacy
Efficiency
    
AI Business Strategy in Emerging Markets
   
Adoption Utilization Integration Implementation
   

facilitators for gaining a competitive business advantage? Managers and consumers often question the efficiency and legitimacy of intelligent agents. AI applications in business enhance efficiency, convenience and cost-effectiveness while raising concerns such as algorithmic social and data bias, data privacy and data protection. Institutional logics often pressure businesses into finding creative solutions. Hence, a novel governance mechanism is desirable to jointly address legitimacy and efficiency issues (Yang et al., 2012). Managers can develop creative adoption, utilization, integration and implementation strategies in emerging markets; for example, the contingent role of chatbot sales–service ambidexterity may resolve the personalization–privacy paradox (Fan et al., 2022).
3. Future research directions
A goal of this thematic issue is to encourage new AI-related theories and conceptual frameworks, focusing on emerging markets. A scientifically sound mindset should prompt a reassessment of traditional theoretical assumptions. For example, the fundamental concept of microeconomic theory – rationality – should be redefined in an AI context to help make and augment rational decision-making (Parkes and Wellman, 2015). Machina economicus reflects AI’s profound effects on economic reasoning. Similarly, new conceptual frameworks for AI in business should suggest fruitful research streams Our posited model intimates several promising theoretical and methodological paths to enhance understanding of AI-driven business strategy in emerging markets.
Due to institutional distances, AI-related business strategies in emerging markets and developed countries differ. Hence, researchers should question AI-related institutions’ cultural, socioeconomic and technological forces in emerging nations. For example, how will AI change the traditional export-led path toward economic growth? Similarly, how should sustainability-focused businesses ensure their AI-driven practices respect fundamental human values such as dignity, freedom, equality and justice while complying with various stakeholders’ preferences?
The empirical studies summarized in this issue indicate ways businesses can leverage AI’s potential in emerging markets. To realize appropriate AI-derived values, business leaders should build and nurture adaptive organizations with an open and collaborative culture and teach workers skills that ensure a smooth intellectual transition Future empirical studies can reveal AI’s disruptive potential and the challenges of adopting, utilizing, integrating and applying AI to businesses in emerging markets. Because data access is now a competitive advantage, scholars contend that AI will widen technological and knowledge gaps between emerging and mature nations . To prevent this gap from further retarding economic development in emerging markets, businesses could use automated analysis of text, images, audio and video to help the poor and unprivileged (Kopalle et al., in press).
Pagani and Champion (2021) proposed a way to check safety, accountability and societal well-being per European Commission (2019) requirements for human-centric AI systems. They posit that programmers’ diverse backgrounds may preclude algorithmic biases in human–machine interaction design. Furthermore, businesses should adapt AI systems to local contexts; for example, marketers could customize popular AI platforms (e.g. Apple Siri, Microsoft Cortana, Amazon Alexa and Google Assistant) or create versions to reshape marketing strategies such as branding, advertising and promotion. Similarly, businesses in emerging markets could leverage AI systems to manage customer experience and engagement by enhancing various service aspects (e.g. service design, co-creation, real- time customer support, service recovery, satisfaction, loyalty, complaints and customer churn).
Guest editorial
919


IJOEM 17,4
920
Finally, business scholars should address critical methodological issues. They could apply AI-powered tools – such as facial recognition, speech recognition system, image recognition, machine learning and natural language processing – to relevant research on emerging markets. They should use innovative data analysis approaches to augment often- insufficient purposive and rule-based data analyses. To harness AI-related opportunities in emerging markets, scholars should attend to data generated via new forms of interaction (e.g. virtual reality, augmented reality, metaverse and chatbot) among consumers, businesses and NGOs (Hoffman et al., 2022). Furthermore, they should develop emerging market-compatible measures of AI-related constructs.
4. Articles in the special issue
This special issue is devoted to advancing knowledge of AI-driven business strategies in emerging markets. We are delighted to include 12 articles on adopting, utilizing, integrating and implementing AI in various industries. We now summarize each article per its main research questions (see Table 1).
In “The impact of artificial intelligence (AI) finance on financing constraints of non-SOE firms in emerging markets,” discuss AI finance’s influence on the financial constraints of non-SOE firms in China. The authors report that AI financing can alleviate these constraints, especially for smaller and more innovative businesses in developing areas. This finding has practical policy implications because non-SOE businesses are more constrained in obtaining external financing than their SOE peers.
“Artificial intelligence in peer-to-peer lending in India: A cross-case analysis,” by, presents research at the cusp of peer-to-peer (P2P) markets and AI in India, one of the fastest-growing markets for fintech. Six breakout segments comprise Indian fintech, with P2P as an important subsegment of “credit.” The article reveals (1) AI’s evolving role in Indian P2P lending markets; (2) how a disruptive technology like AI is revolutionizing P2P platforms with predictive intelligence for making credit decisions, thereby acting as a catalyst for them and (3) how Indian P2P lenders using automated processes and manual underwriting will eventually transition to totally automated processes.
The article by), entitled “How does intelligent technology investment affect employment compensation and firm value in Chinese financial institutions?,” reports on the nexus between intelligent technology investment and employee compensation and its impact on firm value. They found a persistent inhibitory effect on this nexus in emerging markets’ financial institutions and show increases in intelligent investment have a positive two-year lagged effect on firm value. Their findings may help financial firms better understand their need to address the subsequent growth-related costs of intelligent technology input.
In “How do AI applications in service marketing differ from human employee to influence consumer behaviors?,” address this question by examining the moderating role of service provider type (humanoid robot vs human employee) on consumer reactions. They found consumers served by a humanoid robot are more easily convinced about the utilitarian value of functional but not culturally mixed products. In contrast, consumers served by a human employee prefer to be persuaded by cultural connotations and culturally mixed rather than functional products. Furthermore, this effect is driven by perceived usefulness (vs perceived enjoyment) when served by a humanoid robot (vs human employee).
The article by, entitled “How can marketers design an AI chatbot creating profits while catering to various demands from customers?,” addresses this question by examining the contingent role that chatbot sales–service ambidexterity play in adapting to customers’ personalization–privacy paradox. In taking an organizational ambidexterity perspective to explore AI chatbot efficacy, they found the inherently negative (positive)

 
The development of AI finance can alleviate financing constraints for non- SOE firms. This effect is more pronounced for smaller firms, more innovative firms and firms in developing areas
A persistent inhibitory effect exists on the nexus of intelligent technology investment and employee compensation in financial institutions. The increase in intelligent investment has a positive two-year lagged effect on firm value
As the benefits of personalization decreased and the risk to privacy increased, the inherently negative (positive) effects of imbalanced (combined) chatbots’ sales–service ambidexterity had an increasing (decreasing) influence on customer experience When served by a humanoid service robot (vs. human employee), consumers exhibit more positive attitudes and purchase intentions toward functionally (vs culturally) mixed products Perceived benefit and ease of use are the most influential determinants of blockchain adoption Collaboration value is a building block for intelligent product ecosystems. These ecosystems are upgraded by coordinating products, platforms and networks Higher social class participants were more willing than lower social class participants to choose robot services in credence-based service settings. Risk aversion mediated the interaction effect
Contributions to the SI
Emerging market countries can ease financing constraints on non-SOE firms by promoting AI finance development
Help practitioners in emerging countries better understand that firms need to reasonably deal with the subsequent cost of growth caused by intelligent technology input
Enrich the literature on frontline ambidexterity and extend it to human-machine interaction
Offer insights for managers to develop service marketing for mixed products
Deliver a decision support system for business leaders to estimate the potential for blockchain adoption
Provide a framework for enterprises to build an intelligent product ecosystem
Help multinational enterprises (MNEs) develop strategies for scaling up robot services
Used a modified total interpretive structural modeling (m-TISM) approach
Cross-case study based on semi-structured interviews with 6 NBFC-P2P founders and 12 fintech and P2P industry experts Experimental data collected from 396 participants in China (study 1) and 300 participants in the UK (study 2)
Survey data collected from 209 survey participants from August to December 2020
Interview data collected from 66 SMEs across 20 industries in central China
essential for analyzing AI’s impact on a firm’s competitiveness Showcases AI’s evolving role in Indian peer-to-peer lending (P2P) markets. Findings indicate that AI has reached a tipping point in India
Moral standards declined for low-SES but not high-SES persons when using smart devices (vs non-smart devices)
Two dimensions of AI technology stimuli positively affect smart customer experience; the moderating effects of contrasting dimensions of technology readiness are significantly different; smart customer experience has a positive effect on consumers’ word-of- mouth intentions SMEs in central China are enthusiastic about intelligent transformation while facing internal and external pressures. They have been forced to take a step-by-step strategy based on actual needs instead of long-term overall system design, constrained by limited resources
Contributions to the SI
Detailed analysis of the ten factors can help tourism firm managers enhance competitiveness
Illustrate P2Ps still stuck to manual underwriting and show the merit in AI-driven processes
In emerging markets, managers and marketers should be aware of this morality erosion and use preventive measures in advance
Help enterprises better understand customer psychology and AI technology to promote a positive customer experience and improve consumers’ word-of-mouth intentions, especially in the emerging markets
Firms should attend to executives’ role in promoting intelligent transformation and fully use policy support to access additional resources

effects of imbalanced (combined) chatbots’ sales–service ambidexterity had an increasing (decreasing) influence on customer experience. Furthermore, customer experience fully mediated the association of chatbots’ sales–service ambidexterity with customer patronage.
In “Analysing the impact of artificial intelligence on the competitiveness of tourism firms: A modified total interpretive structural modeling (m-TISM) approach,” focus on tourism firms’ use of AI to boost their competitiveness. Researchers and practitioners are optimistic that AI can help tourism firms profitably deliver services and products at competitive prices while enhancing customers’ satisfaction. AI-based systems – such as chatbots and virtual assistants – are changing the tourism and hospitality industry. Using a modified total interpretive structural modeling approach, Sharma et al. (2022) explored the factors influencing tourism firms’ overall performance. Hierarchical interrelationships among these factors are crucial to realizing AI’s potential. The model also answers the “why” and “how” of these relationships. Adopting AI could help tourism and hospitality firms mitigate risks and challenges while sustaining competitiveness.

In “The impact of artificial intelligence technology stimuli on smart customer experience and the moderating effect of technology readiness,” construct an impact mechanism model for AI technology stimuli on word-of-mouth intentions. They found that (1) two dimensions of AI technology stimuli (i.e. passion and usability) positively affect smart customer experience, (2) different dimensions of technology readiness (i.e. optimism and discomfort) have different moderating effects on the relationship between AI technology stimuli and smart customer experience and (3) smart customer experience positively affects word-of-mouth intentions. These results can help businesses better understand customers’ psyches and use AI technology to create positive customer experiences that enhance word-of- mouth in emerging markets.
Rising labor costs and a robotic technology boom have grown the role of robots in China’s service sector. Considering this trend, it is imperative that marketers better understand the factors that affect consumers’ attitudes toward AI robots, especially in emerging markets. In “The impact of social class and service type on preference for AI service robots,” Yao et al. (2022) examine how social class and service type jointly affect consumers’ willingness to choose AI service robots. In essence, their research suggests that companies should adopt different strategies when deploying AI service robots in different markets. In particular, current AI robots are most appropriate for credence-based service markets with high upward social mobility.
In “What is the impact of artificial intelligence on people’s morality in emerging markets?,” Sui and Zhou (2022) examine the moderating effect of socioeconomic status (SES) on smart device usage and moral standards in the UK and China. They found that using smart devices (vs non-smart devices) lowers moral standards for low-SES persons but not high-SES persons. Although smart device usage is the norm in developed markets, such usage is rising among employees in emerging markets. To avoid moral degradation and deviant behaviors in the workplace, managers can train their employees before adopting emerging technologies.
Smart product ecology can improve the collaborative value creation of physical products by triggering major organizational strategic and structural changes. In “Product intellectualization ecosystem: A framework through grounded theory and case analysis,” Dong et al. (2022) develop a conceptual framework for a product intellectualization ecosystem by studying two companies: Haier Group Company (Haier) and Xiaomi Corporation (Xiaomi). After identifying a three-stage process that entails smart product unit, smart product system and smart product ecology, they establish a coordination mechanism and three coordination modes: product coordination, platform coordination and network coordination. Their posited model delineates the collaborative values achievable via different coordination modes across the stages.
In “What are determining factors of blockchain technology adoption in an emerging market?,” respond to this question by using a machine learning method to predict blockchain adoption by Palestinian firms. They relied on a Bayesian network examination to develop an extrapolative decision support system, highlighting the determinants – perceived benefit and ease of use – -that most influence managers’ predictions for their company’s technology adoptions. The findings provide insight into the literature by showing reduced technological complexity is unrelated to perceived benefit, perhaps because Palestinian SMEs are eager to adopt a new technology despite a perceived difficulty and complexity of use.
Finally, “How to survive in the age of artificial intelligence? Exploring the intelligent transformation of SMEs in Central China,” by, explores how AI transformed these SMEs. After interviews across 20 industries, they found that SMEs in central China are enthusiastic about intelligent transformation despite internal and external pressures. Constrained by limited resources, these SMEs were forced into a step-by-step strategy based on immediate needs rather than overall system design. Although they should

attend to executives’ roles in promoting intelligent transformation, overemphasizing social responsibility will hinder SMEs’ intelligent transformation. These findings can help businesses in labor-intensive and resource-deficient emerging markets.
AI’s rapid development provides an opportunity for innovative research in emerging markets. To refine and extend existing theories and build new ones, we propose a conceptual model of AI-driven business strategy to depict how businesses should adopt, utilize, integrate and implement AI to gain a competitive advantage. By identifying the main forces, businesses can better understand the regulatory, normative and cultural-cognitive institutions that promote or constrain AI-driven businesses. Stressing the confluence of legitimacy and efficiency of AI-driven business, our model suggests several promising
research streams. The essential issue is how businesses can better leverage AI. Advances in institutional
theory can help businesses interpret, manipulate, revise and elaborate on business marketing institutions. We hope this special issue will encourage business scholars to work and think innovatively about institutional theory in the context of ever-expanding business markets.
",https://www.researchgate.net/publication/359116734_Impact_of_artificial_intelligence_on_business_strategy_in_emerging_markets_A_conceptual_framework_and_future_research_directions
National strategic artificial intelligence plans: A multi-dimensional analysis,Economic Analysis & Policy V67,"Samar Fatima, Kevin D’Souza, Gregory Dawson","META ANALYSIS OF STRATEGIC PLANS CROSS CHECKED AGAINST THIS LIST OF REFERENCES FOR CROSSOVER To capture how each country perceives the role that AI could play in the public and private sectors; 2. To understand how each country plans to deal with key technical elements of AI systems, such as data and algorithms; 3. To determine how each country plans to develop its AI capacity and address governance challenges that arise from AI systems.",2020,"Abstract
Nations have recognized the transformational potential of artificial intelligence (AI). Advances in AI will impact all facets of society. A spate of recently released national strategic AI plans provides valuable insights into how nations are considering their future trajectories. These strategic plans offer a rich source of evidence to understand national-level strategic actions, both proactive and reactive, in the face of rapid technological innovation. Based on a comprehensive content analysis of thirty-four national strategic plans, this article reports on (1) opportunities for AI to modernize the public sector and enhance industry competitiveness, (2) the role of the public sector in ensuring that the two most critical elements of AI systems, data and algorithms, are managed responsibly, (3) the role of the public sector in the governance of AI systems, and (4) how nations plan to invest in capacity development initiatives to strengthen their AI capabilities.

Introduction
Advances in artificial intelligence (AI) have attracted the interest of public sector agencies across the globe (Yeung, 2020). AI systems are being deployed across the public sector (Agarwal, 2018, Desouza, 2018) and are modernizing the delivery of public services (Sun and Medaglia, 2019). AI solutions can reduce the administrative burden in the public sector by automating routine work (Pencheva et al., 2018). In addition, these systems can serve as interfaces between agencies and citizens, thereby promoting higher-quality service delivery (Sousa et al., 2019). Besides efficient and effective public services, the adoption of AI technologies in the public sector can foster innovation in a number of ways that were not feasible previously, such as the use of data from social media platforms to inform the design and evaluation of public policies (Berryhill et al., 2019).
It is anticipated that the AI revolution will bring about significant disruptions to various socio-economic elements (Berryhill et al., 2019). AI and the rise of automation will impact the future of work and employment (Makridakis, 2017). On one hand, labor-intensive work will be automated (Helbing, 2015, Schwab, 2015), while on the other hand, there might be a rise in new jobs (Wilson et al., 2017). However, it is not clear if the loss of jobs will be compensated for by the increase in new jobs (Bessen, 2018). The deployment of AI has also raised ethical considerations (Jobin et al., 2019). For instance, various facial recognition tools used by law enforcement agencies have recently been scrutinized for their inaccuracy and their propensity to be biased towards people of color and minorities (Allyn, 2020). The design of AI systems, especially the level of transparency and auditability of learning algorithms, remains an ongoing concern (Raji et al., 2020). AI-enabled systems can also be weaponized to disrupt public agencies and various processes (e.g., political campaigns and elections) in democratic societies (Desouza et al., 2020).
Given the significant disruption that is expected due to advances in AI, it is not surprising that nations are now contemplating the practicalities of current and future developments in AI, the impacts of AI-related affordances, and how to address an emerging set of technical, societal, and public policy conundrums that come with AI deployment in a society. The significance of AI as part of a national agenda is highlighted by the growing number of countries crafting national strategic AI plans (OPSI, 2020). These plans provide insights into a nation’s agenda in terms of harnessing AI and the constellation of related technologies. In addition, they provide insights into how each nation considers various public policies and economic issues that environ AI technologies. Finally, these strategic plans outline how each nation will coordinate its investment and implementation efforts both within and beyond the public sector to leverage AI for the public good (Fatima et al., 2020).
While the private sector has made substantial progress in embracing AI and crafting digital transformation strategies (Marr and Ward, 2019, Moore, 2019), these practices cannot readily be imported to the public sector. Deploying technologies in the public sector requires greater attention to how public value is maximized (Benington and Moore, 2010), which is a more complex undertaking when compared to maximizing shareholder value. In addition, designing public policies on emerging technologies requires one to navigate a myriad of social, political, and economic considerations while simultaneously ensuring that all sectors and segments of society are accounted for (Morçöl, 2013). Finally, the public sector has a greater requirement to engage citizens in the process of designing public policies and innovations that impact the future of public services (Voorberg et al., 2015).
Given that we are in the early days of witnessing the materialization of AI affordances in the public sector (Berryhill et al., 2019), research is needed to understand how the trajectory of AI in society might be shaped by national-level strategies and public policies. Toward this end, we gathered and performed a comprehensive analysis of 34 national strategic AI plans. Our objectives were threefold:
1. To capture how each country perceives the role that AI could play in the public and private sectors  2. To understand how each country plans to deal with key technical elements of AI systems, such as data and algorithms  3. To determine how each country plans to develop its AI capacity and address governance challenges that arise from AI systems 
Access through your organization
Check access to the full text by signing in through your organization.

Section snippets
Background
Long-range planning and the crafting of strategic plans are important undertakings in the public sector (Bryson, 1988). Long-range plans offer insights into governments’ social and economic development initiatives for a period of five years or more (Wu et al., 2019). The planning process allows agencies to engage input from a diverse set of stakeholders, both inside and outside the public sector (Bryson et al., 2002, Taylor, 1984), and this process generates debates to determine choices,
Dataset
We built a dataset of national strategic AI plans that existed as of January 31, 2020. The Observatory of Public Sector Innovation (OPSI) (OPSI, 2020) listed 50 nations that have developed or were in the process of developing national AI strategies. We excluded countries that were still in the process of developing their strategic plans. We made the decision to include only published plans based on the belief that in-process plans had not been as thoroughly vetted and agreed upon as those that
Results
We started by understanding the motivation that inspired different countries to develop their national AI plans. Most plans discussed the major motivation for embarking on AI, identified its enablers, and identified the beneficiaries of the effort and high-level targets of infusion. Most of the plans were aspirational and touched on lofty goals and ideals. Malta’s plan was typical in this regard, noting, “We plan to gradually infuse AI into education, healthcare, and a range of public services
Public sector functions and industries
Most plans detailed two aspects (a) how national governments should leverage AI to modernize the public sector, and (b) how industries and industrial sectors should take advantage of AI affordances to maintain and extend their competitiveness. Some plans went a step further to clarify the specific role that the government should play. Consider the USA’s plan, which notes, “The Federal government should therefore emphasize AI investments in areas of strong societal importance that are not aimed
Discussion
Our analysis has uncovered that strategic AI plans are a rich source of information when it comes to understanding how nations see (1) opportunities to modernize the public sector and transform industries, (2) critical data and algorithmic elements that need to be managed, and (3) planning for capacity building and governance frameworks to support AI development efforts. We now discuss salient insights from our analysis.
China’s strategic AI plan had the largest coverage of public sector
Conclusions
Our work is exploratory in nature and represents only a first step in understanding how nations strategize their futures in the context of advances in AI. Future research is needed to delve more deeply into the components of these plans. Research is needed to more rigorously assess how substantially, or how superficially, critical concepts are covered in each plan. Given that nations have varying maturity when it comes to their digital government capabilities, future research can also look at
Declaration of Competing Interest
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.",https://www.sciencedirect.com/science/article/abs/pii/S0313592620304021
The Forthcoming Artificial Intelligence (AI) Revolution: Its Impact on Society and Firms,Futures Journal,"Makridakis, S.",REVIEW Artificial intelligence is poised to revolutionize society and firms globally with its profound impact on various aspects of operations and decision-making within organizations.,2017,"
The rise of powerful AI will be either the best or the worst thing ever to happen to humanity. We do not yet know which. Stephen Hawking
Over the past decade, numerous predictions have been made about the forthcoming Artificial Intelligence (AI) Revolution and its impact on all aspects of our society, firms and life in general. This paper considers such predictions and compares them to those of the industrial and digital ones. A similar paper was written by this author and published in this journal in 1995, envisioning the forthcoming changes being brought by the digital (information) revolution, developing steadily at that time, and predicting its impact for the year 2015 (Makridakis, 1995). The current paper evaluates these 1995 predictions and their impact identifying hits and misses with the purpose of focusing on the new ones being brought by the AI revolution. It must be emphasized that the stakes of correctly predicting the impact of the AI revolution are far reaching as intelligent machines may become our “final invention” that may end human supremacy (Barrat, 2013). There is little doubt that AI holds enormous potential as computers and robots will probably achieve, or come close to, human intelligence over the next twenty years becoming a serious competitor to all the jobs currently performed by humans and for the first time raising doubt over the end of human supremacy.
This paper is organized into four parts. It first overviews the predictions made in the 1995 paper for the year 2015, identifying successes and failures and concluding that major technological developments (notably the Internet and smartphones) were undervalued while the general trend leading up to them was predicted correctly. Second, it investigates existing and forthcoming technological advances in the field of AI and the ability of computers/machines to acquire real intelligence. Moreover, it summarizes prevailing, major views of how AI may revolutionize practically everything and its impact on the future of humanity. The third section sums up the impact of the AI revolution and describes the four major scenarios being advocated, as well as what could be done to avoid the possible negative consequences of AI technologies. The fourth section discusses how firms will be affected by these technologies that will transform the competitive landscape, how start-up firms are founded and the way success can be achieved. Finally, there is a brief concluding section speculating about the future of AI and its impact on our society, life, firms and employment.
summarizes the major inventions of the industrial, digital and AI revolutions. The first two columns have been taken from the 1995 paper (Makridakis, 1995) replacing “Widespread use of” with “Actual use in 2015” while the third one is new referring to the AI revolution and its existing and new, widespread used inventions by the year 2037. The challenge is to correctly predict the impact of AI inventions and how the role of humans will be affected when machines of equal, or superior intelligence could substitute, supplement and/or amplify practically all mental tasks that until now have been the exclusive province of humans. As with technological predictions made in the past, as well as those in the 1995 paper, it is necessary not to extrapolate in a linear fashion in order not to underestimate the rate of technological change and its impact on all aspects of our society, life, work and firms. In my opinion the forthcoming technologies of the AI revolution and their impact over the next twenty years will probably be many times the magnitude of those of the digital one from 1995 to 2015 and probably even greater than those of the Industrial revolution.
1. The 1995 paper: hits and misses
The 1995 paperwas written at a time when the digital (at that time it was called information) revolution was progressing at a steady rate. The paper predicted that by 2015 “the information revolution should be in full
48 S. Makridakis / Futures 90 (2017) 46–60
swing” and that “computers/communications” would be in widespread use, which has actually happened, although its two most important inventions (the Internet and smartphones) and their significant influence were not foreseen as such. Moreover, the paper predicted that “a single computer (but not a smartphone) can, in addition to its traditional tasks, also become a terminal capable of being used interactively for the following:” (p. 804–805)
 Picture phone and teleconference  Television and videos  Music  Shopping
 On line banking and financial services  Reservations  Medical advice  Access to all types of services
 Video games  Other games (e.g., gambling, chess etc.)  News, sports and weather reports  Access to data banks
The above have all materialized and can indeed be accessed by computer, although the extent of their utilization was underestimated as smartphones are now being used widely. For instance, the ease of accessing and downloading scientific articles on one's computer in his/her office or home would have seemed like science fiction back in 1995, when finding such articles required spending many hours in the library (often in its basement for older publications) and making photocopies to keep them for later use. Moreover, having access, from one's smartphone or tablet, to news from anywhere in the world, being able to subscribe to digital services, obtain weather forecasts, purchase games, watch movies, make payments using smartphones and a plethora of other, useful applications was greatly underestimated, while the extensive use of the cloud for storing large amounts of data for free was not predicted at all at that time. Even in 1995 when the implications of Moore's law leading to increasing computer speed and storage while reducing costs were well known, nevertheless, it was hard to imagine that in 2016 there would be 60 trillion web pages, 2.5 billion smartphones, more than 2 billion personal computers and 3.5 billion Google searches a day.
The paper correctly predicted “as wireless telecommunications will be possible the above list of capabilities can be accessed from anywhere in the world without the need for regular telephone lines”. What the 1995 paper missed, however, was that in 2015 top smartphones, costing less than s500, would be as powerful as the 1995 supercomputer, allowing access to the Internet and all tasks that were only performed by expensive computers at that time, including an almost unlimited availability of new, powerful apps providing a large array of innovative services that were not imagined twenty years ago. Furthermore, the paper correctly predicted super automation leading to unattended factories stating that “by 2015 there will be little need for people to do repetitive manual or mental tasks”. It also foresaw the decline of large industrial firms, increased global competition and the drop in the percentage of labour force employed in agriculture and manufacturing (more on these predictions in the section The Impact of the AI Revolution on Firms). It missed however the widespread utilization of the Internet (at that time it was a text only service), as well as search engines (notably Google), social networking sites (notably Facebook) and the fundamental changes being brought by the widespread use of Apple's iPhone, Samsung's Galaxy and Google's Android smartphones. It is indeed surprising today to see groups of people in a coffee shop or restaurant using their smartphones instead of speaking to each other and young children as little as three or four years of age playing with phones and tablets. Smartphones and tablets connected to the Internet through Wi-Fi have influenced social interactions to a significant extent, as well as the way we search for information, use maps and GPS for finding locations, and make payments. These technologies were not predicted in the 1995 paper.
2. Towards the AI revolution
The 1995 paper referred to Say, the famous French economist, who wrote in 1828 about the possibility of cars as substitutes for horses:
“Nevertheless no machine will ever be able to perform what even the worst horses can - the service of carrying people and goods through the bustle and throng of a great city.” (p. 800)
Say could never have dreamed of, in his wildest imagination, self-driving cars, pilotless airplanes, Skype calls, super computers, smartphones or intelligent robots. Technologies that seemed like pure science fiction less than 190 years ago are available today and some like self-driving vehicles will in all likelihood be in widespread use within the next twenty years. The challenge is to realistically predict forthcoming AI technologies without falling into the same short-sighted trap of Say and others, including my 1995 paper, unable to realize the momentous, non-linear advancements of new technologies. There are two observations to be made.
First, 190 years is a brief period by historical standards and during this period we went from horses being the major source of transportation to self-driving cars and from the abacus and slide rules to powerful computers in our pockets. Secondly, the
S. Makridakis / Futures 90 (2017) 46–60 49
length of time between technological inventions and their practical, widespread use is constantly being reduced. For instance, it took more than 200 years from the time Newcomen developed the first workable steam engine in 1707 to when Henry Ford built a reliable and affordable car in 1908. It took more than 90 years between the time electricity was introduced and its extensive use by firms to substantially improve factory productivity. It took twenty years, however, between ENIAC, the first computer, and IBM's 360 system that was mass produced and was affordable by smaller business firms while it took only ten years between 1973 when Dr Martin Cooper made the first mobile call from a handheld device and its public launch by Motorola. The biggest and most rapid progress, however, took place with smartphones which first appeared in 2002 and saw a stellar growth with the release of new versions possessing substantial improvements every one or two years by the likes of Apple, Samsung and several Chinese firms. Smartphones, in addition to their technical features, now incorporate artificial intelligence characteristics that include understanding speech, providing customized advice in spoken language, completing words when writing a text and several other functions requiring embedded AI, provided by a pocket computer smaller in size than a pack of cigarettes.
From smart machines to clever computers and to Artificial Intelligence (AI) programs: A thermostat is a simple mechanical device exhibiting some primitive but extremely valuable type of intelligence by keeping temperatures constant at some desired, pre-set level. Computers are also clever as they can be instructed to make extremely complicated decisions taking into account a large number of factors and selection criteria, but like thermostats such decisions are pre-programmed and based on logic, if-then rules and decision trees that produce the exact same results, as long as the input instructions are alike. The major advantage of computers is their lightning speed that allows them to perform billions of instructions per second. AI, on the other hand, goes a step further by not simply applying pre-programmed decisions, but instead exhibiting some learning capabilities.
The reading of handwritten digits (first utilized to determine the written amount on bank checks) by the neural net device in 1990 (see Table 1) is a predecessor of efforts to achieve learning. Handwritten digits could be inscribed in almost infinite ways so programming a machine to correctly read them was no small challenge, beginning a new direction in programming aimed at mimicking the human mind that can effortlessly understand all types of handwritten digits. The same applies for Polly (see Table 1) that could learn to pronounce a number of words, or ASIMO that could navigate in open places of constantly changing environmental settings, rendering preprogramming impossible to account for all possible cases. ASIMO's decisions were quite different than those of the extremely fast supercomputer Deep Blue (see Table 1) that used “brute force” to identify and analyze up to 60 billion moves within the three minute period allowed to chess players to make their next move. Deep Blue was incapable of learning as its programming was based on logic, if-then rules and decision trees which meant that it could not learn and could make the same mistakes over and over again.
The story of the Watson computer beating Jeopardy's two most successful contestants is more complicated, since retrieving the most appropriate answer out of the 200 million pages of information stored in its memory is not a sign of real intelligence as it relied on its lightning speed to retrieve information in seconds. What is more challenging according to Jennings, one of Jeopardy's previous champions, is “to read clues in a natural language, understand puns and the red herrings, to unpack just the meaning of the clue” (May, 2013). Similarly, it is a sign of intelligence to improve its performance by “playing 100 games against past winners”. (Best, 2016). Watson went several steps beyond Deep Blue towards AI by being able to understand spoken English and learn from his mistakes (New Yorker, 2016). However, he was still short of AlphaGo that defeated Go Champions in a game that cannot be won simply by using “brute force” as the number of moves in this game is infinite, requiring the program to use learning algorithms that can improve its performance as it plays more and more games
Computers and real learning: According to its proponents, “the main focus of AI research is in teaching computers to think for themselves and improvise solutions to common problems” (Clark, 2015). But many doubt that computers can learn to think for themselves even though they can display signs of intelligence. David Silver, an AI scientist working at DeepMind, explained that “even though AlphaGo has affectively rediscovered the most subtle concepts of Go, its knowledge is implicit. The computer parse out these concepts – they simply emerge from its statistical comparisons of types of winning board positions at GO” (Chouard, 2016). At the same time Cho Hyeyeon, one of the strongest Go players in Korea commented that “AlphaGo seems like it knows everything!” while others believe that “AlphaGo is likely to start a ‘new revolution’ in the way we play Go” as “it is seeking simply to maximize its probability of reaching winning positions, rather than as human players tend to do – maximize territorial gains” (Chouard, 2016). Does it matter, as Silver said, that AlphaGo's knowledge of the game is implicit as long as it can beat the best players? A more serious issue is whether or not AlphaGo's ability to win games with fixed rules can extend to real life settings where not only the rules are not fixed, but they can change with time, or from one situation to another.
From digital computers to AI tools: The Intel Pentium microprocessor, introduced in 1993, incorporated graphics and music capabilities and opened computers up to a large number of affordable applications extending beyond just data processing. Such technologies signalled the beginning of a new era that now includes intelligent personal assistants understanding and answering natural languages, robots able to see and perform an array of intelligent functions, self-driving vehicles and a host of other capabilities which were until then an exclusive human ability. The tech optimists ascertain that in less than 25 years computers went from just manipulating 0 and 1 digits, to utilizing sophisticated neural network algorithms that enable vision and the understanding and speaking of natural languages among others. Technology optimists therefore maintain there is little doubt that in the next twenty years, accelerated AI technological progress will lead to a breakthrough, based on deep learning that imitates the way young children learn, rather than the laborious instructions by tailor-made programs aimed for specific applications and based on logic, if-then rules and decision trees (Parloff, 2016).
50 S. Makridakis / Futures 90 (2017) 46–60
For instance, DeepMind is based on a neural program utilizing deep learning that teaches itself how to play dozens of Atari games, such as Breakout, as well or better than humans, without specific instructions for doing so, but by playing thousands of games and improving itself each time. This program, trained in a different way, became the AlphaGo that defeated GO champion Lee Sodol in 2016. Moreover, it will form the core of a new project to learn to play Starcraft, a complicated game based on both long term strategy as well as quick tactical decisions to stay ahead of an opponent, which DeepMind plans to be its next target for advancing deep learning (Kahn, 2016). Deep learning is an area that seems to be at the forefront of research and funding efforts to improve AI, as its successes have sparked a burst of activity in equity funding that reached an all-time high of more than $1 billion with 121 projects for start-ups in the second quarter of 2016, compared to 21 in the equivalent quarter of 2011 (Parloff, 2016).
Google had two deep learning projects underway in 2012. Today it is pursuing more than 1000, according to their spokesperson, in all its major product sectors, including search, Android, Gmail, translation, maps, YouTube, and self-driving cars (The Week, 2016). IBM's Watson system used AI, but not deep learning, when it beat the two Jeopardy champions in 2011. Now though, almost all of Watson's 30 component services have been augmented by deep learning. Venture capitalists, who did not even know what deep learning was five years ago, today are wary of start-ups that do not incorporate it into their programs. We are now living in an age when it has become mandatory for people building sophisticated software applications to avoid click through menus by incorporating natural-language processing tapping deep learning (Parloff, 2016).
How far can deep learning go? There are no limits according to technology optimists for three reasons. First as progress is available to practically everyone to utilize through Open Source software, researchers will concentrate their efforts on new, more powerful algorithms leading to cumulative learning. Secondly, deep learning algorithms will be capable of remembering what they have learned and apply it in similar, but different situations (Kirkpatrick et al., 2017). Lastly and equally important, in the future intelligent computer programs will be capable of writing new programs themselves, initially perhaps not so sophisticated ones, but improving with time as learning will be incorporated to be part of their abilities. Kurzweil (2005) sees nonbiological intelligence to match the range and subtlety of human intelligence within a quarter of a century and what he calls “Singularity” to occur by 2045, bringing “the dawning of a new civilization that will enable us to transcend our biological limitations and amplify our creativity. In this new world, there will be no clear distinction between human and machine, real reality and virtual reality”.
For some people these predictions are startling, with far-reaching implications should they come true. In the next section, four scenarios associated with the AI revolution are presented and their impact on our societies, life work and firms is discussed.
3. The four AI scenarios
Until rather recently, famines, wars and pandemics were common, affecting sizable segments of the population, causing misery and devastation as well as a large number of deaths. The industrial revolution considerably increased the standards of living while the digital one maintained such rise and also shifted employment patterns, resulting in more interesting and comfortable office jobs. The AI revolution is promising even greater improvements in productivity and further expansion in wealth. Today more and more people, at least in developed countries, die from overeating rather than famine, commit suicide instead of being killed by soldiers, terrorists and criminals combined and die from old age rather than infectious disease (Harari, 2016). Table 1 shows the power of each revolution with the industrial one aiming at routine manual tasks, the digital doing so to routine mental ones and AI aiming at substituting, supplementing and/or amplifying practically all tasks performed by humans. The critical question is: “what will the role of humans be at a time when computers and robots could perform as well or better and much cheaper, practically all tasks that humans do at present?” There are four scenarios attempting to answer this question.
The Optimists: Kurzweil and other optimists predict a “science fiction”, utopian future with Genetics, Nanotechnology and Robotics (GNR) revolutionizing everything, allowing humans to harness the speed, memory capacities and knowledge sharing ability of computers and our brain being directly connected to the cloud. Genetics would enable changing our genes to avoid disease and slow down, or even reverse ageing, thus extending our life span considerably and perhaps eventually achieving immortality. Nanotechnology, using 3D printers, would enable us to create virtually any physical product from information and inexpensive materials bringing an unlimited creation of wealth. Finally, robots would be doing all the actual work, leaving humans with the choice of spending their time performing activities of their choice and working, when they want, at jobs that interest them.
The Pessimists: In a much quoted article from Wired magazine in 2000, Bill Joy (Joy, 2000) wrote “Our most powerful 21st- century technologies – robotics, genetic engineering, and nanotech – are threatening to make humans an endangered species”. Joy pointed out that as machines become more and more intelligent and as societal problems become more and more complex, people will let machines make all the important decisions for them as these decisions will bring better results than those made by humans. This situation will, eventually, result in machines being in effective control of all important decisions with people dependent on them and afraid to make their own choices. Joy and many other scientists (Cellan-Jones, 2014) and philosophers (Bostrom, 2014) believe that Kurzweil and his supporters vastly underestimate the magnitude of the challenge and the potential dangers which can arise from thinking machines and intelligent robots. They point out that in the utopian world of abundance, where all work will be done by machines and robots, humans may be reduced to second rate status
S. Makridakis / Futures 90 (2017) 46–60 51
(some saying the equivalent of computer pets) as smarter than them computers and robots will be available in large numbers and people will not be motivated to work, leaving computers/robots to be in charge of making all important decisions. It may not be a bad world, but it will definitely be a different one with people delegated to second rate status.
Harari is the newest arrival to the ranks of pessimists. His recent book (Harari, 2016, p. 397) concludes with the following three statements:
	  “Science is converging to an all-encompassing dogma, which says that organisms are algorithms, and life is data processing” 
	  “Intelligence is decoupling from consciousness” 
	  “Non-conscious but highly intelligent algorithms may soon know us better than we know ourselves”  Consequently, he asks three key questions (which are actually answered by the above three statements) with terrifying implications for the future of humanity: 
	  “Are organisms really just algorithms, and is life just data processing?” 
	  “What is more valuable – intelligence or consciousness?” 
	  “What will happen to society, politics and daily life when non-conscious but highly intelligent algorithms know us better  than we know ourselves?” Harari admits that nobody really knows how technology will evolve or what its impact will be. Instead he discusses the  implications of each of his three questions: 
	  Ifindeedorganismsarealgorithmsthenthinkingmachinesutilizingmoreefficientonesthanthosebyhumanswillhavean advantage. Moreover, if life is just data processing then there is no way to compete with computers that can consult/exploit practically all available information to base their decisions. 
	  The non-conscious algorithms Google search is based on the consultation of millions of possible entries and often surprise us by their correct recommendations,. The implications that similar, more advanced algorithms than those utilized by Google search will be developed (bearing in mind Google search is less than twenty years old) in the future and be able to access all available information from complete data bases are far reaching and will “provide us with better information than we could expect to find ourselves”. 
	  Humansareproudoftheirconsciousness,butdoesitmatterthatself-drivingvehiclesdonothaveone,butstillmakebetter decisions than human drivers, as can be confirmed by their significantly lower number of traffic accidents?  When AI technologies are further advanced and self-driving vehicles are in widespread use, there may come a time that legislation may be passed forbidding or restricting human driving, even though that may still be some time away according to some scientists (Gomes, 2014). Clearly, self-driving vehicles do not exceed speed limits, do not drive under the influence of alcohol or drugs, do not get tired, do not get distracted by talking on the phone or sending SMS or emails and in general make fewer mistakes than human drivers, causing fewer accidents. There are two implications if humans are not allowed to drive. First, there will be a huge labour displacement for the 3.5 million unionized truck drivers in the USA and the 600 thousand ones in the UK (plus the additional number of non-unionized ones) as well as the more than one million taxi and Uber drivers in these two countries. Second, and more importantly, it will take away our freedom of driving, admitting that computers are superior to us. Once such an admission is accepted there will be no limits to letting computers also make a great number of other decisions, like being in charge of nuclear plants, setting public policies or deciding on optimal economic strategies as their biggest advantage is their objectivity and their ability to make fewer mistakes than humans.  One can go as far as suggesting letting computers choose Presidents/Prime Ministers and elected officials using objective criteria rather than having people voting emotionally and believing the unrealistic promises that candidates make. Although such a suggestion will never be accepted, at least not in the near future, it has its merits since people often choose the wrong candidate and later regret their choice after finding out that pre-election promises were not only broken, but they were even reversed. Critics say if computers do eventually become in charge of making all important decisions there will be little left for people to do as they will be demoted to simply observing the decisions made by computers, the same way as being a passenger in a car driven by a computer, not allowed to take control out of the fear of causing an accident. As mentioned before, this could lead to humans eventually becoming computers’ pets.  The pragmatists: At present the vast majority of views about the future implications of AI are negative, concerned with its potential dystopian consequences (Elon Musk, the CEO of Tesla, says it is like “summoning the demon” and calls the consequences worse than what nuclear weapons can do). There are fewer optimists and only a couple of pragmatists like Sam Altman and Michio Kaku (Peckham, 2016) who believe that AI technologies can be controlled through “OpenAI” and effective regulation. The ranks of pragmatists also includes John Markoff (Markoff, 2016) who pointed out that the AI field can be distinguished by two categories: The first trying to duplicate human intelligence and the second to augment it by expanding human abilities exploiting the power of computers in order to augment human decision making. Pragmatists mention chess playing where the present world champion is neither a human nor a computer but rather humans using laptop computers (Baraniuk, 2015). Their view is that we could learn to exploit the power of computers to augment our own 
52 S. Makridakis / Futures 90 (2017) 46–60
skills and always stay a step ahead of AI, or at least not be at a disadvantage. The pragmatists also believe that in the worst of cases a chip can be placed in all thinking machines/robots to render them inoperative in case of any danger. By concentrating research efforts on intelligence augmentation, they claim we can avoid or minimize the possible danger of AI while providing the means to stay ahead in the race against thinking machines and smart robots.
The doubters: The doubters do not believe that AI is possible and that it will ever become a threat to humanity. Dreyfus (1972), its major proponent, argues that human intelligence and expertise cannot be replicated and captured in formal rules. He believes that AI is a fad promoted by the computer industry. He points out to the many predictions that did not materialize such as those made by Herbert A. Simon in 1958 that “a computer would be the world's chess champion within ten years” and those made in 1965 that “machines will be capable within twenty years, of doing any work a man can do” (Crevier, 1993). Dreyfus claims that Simon's optimism was totally unwarranted as they were based on false assumptions that human intelligence is based on an information processing viewpoint as our mind is nothing like a computer. Although, the doubters’ criticisms may have been valid in the last century, they cannot stand for the new developments in AI. Deep Blue became the world's chess champion in 1997 (missing Simon's forecast by twenty one years) while we are not far today from machines being capable of doing all the work that humans can do (missing Crevier's prediction by about a decade). There are also self- driving vehicles, nurse-robots taking care of the elderly, and Google Search which may know better than us what we are looking for. These technologies are hard to explain by those that doubt AI technologies will materialize and affect us profoundly.
A more sophisticated attack comes from doubters who state that it is wrong to believe that once computers have been provided with sufficiently advanced algorithms, they will be able to improve and then replicate the way our mind works. According to them (Jankel, 2015) computers will not be able to achieve the highest of human ability that of being creative as doing so requires breaking the rules and being anti-algorithmic. In other words creative breakthroughs cannot be predicted so any algorithm developed by AI to do so will fail, leaving a big vacuum to the sole province of the human mind that will always be more valuable than all algorithmic AI technologies put together. It is like the paintings by the masters which are by far superior to those of the millions of average painters put together. This would mean that all tasks requiring creativity, including innovative breakthroughs, strategic thinking, entrepreneurship, risk taking and similar ones could never, or at least not in the foreseeable future, be done algorithmically, providing humans with a clear superiority versus intelligent machines.
The timing and expected impact of the AI revolution: Kurzweil predicted that computers will reach human intelligence around 2029 (Kurzweil, 2005) while Singularity will come by 2045. In 2009 Barrat and Goertzel (2011) asked the participants of an Artificial General Intelligence (AGI) Conference to answer the question: “I believe that AGI (however I define it) will be effectively implemented in the following timeframe”. The answers of the 60 participants and the percentages of their answers are shown below:
Time frame
Before 2030 2030–2049 2050–2099 After 2100 Never
Response Response Percent Count
43.3 26 25.0 15 20.0 12 10.0 6 1.7 1
The second prediction being asked was: “I believe that AGI (however I define it) will be a net positive event for humankind”. Among the 60 participants, 51, that is 85% answer “Yes” while the remaining 15% said “No”.
In a similar survey Muller and Bostrom (2013) asked hundreds of AI experts at a series of conference the following: “For the purposes of this question, assume that human scientific activity continues without major negative disruption. By what year would you see a (10%/50%/90%) probability for such High Level Machine Intelligence (HLMI) to exist?”
The median answer for the 10% probability was 2022, the 50% probability was 2040 and the 90% probability the year 2075. The timing from the answers of the two surveys is not far apart from those of Kurzweil (although they may have been influenced by them), agreeing that AGI or HLMI are not so far off and with the majority of scientists believing that AI will have a positive effect for humankind.
In a new survey conducted in early March 2016, Etzioni (2016) posed the following question: “In his book, Nick Bostrom has defined Superintelligence as ‘an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.’ When do you think we will achieve Superintelligence?”
The answers of the 80 responders (a 41% rate) are summarized below:
0% In the next 10 years 7.5% In the next 10–25 years 67.5% In more than 25 years 25.0% Never
These dates put the start of superintelligence later than that of Kurzweil or those of the previous surveys but the answers refer to “superintelligence” and includes “scientific creativity, general wisdom and social skills” rather than simply AI. It is also interesting that only 25% of respondents answer “never”.
4. Firms and employment: from the industrial and digital to the AI revolution
The industrial revolution brought far reaching changes to firms and employment while those of the digital continued the decrease of employment in agriculture and manufacturing while contributing to strong increases in services, in particular in computers, the Internet and the mobile phone markets. The digital revolution also resulted in the decrease of the large industrial firm. The expected changes being brought by AI technologies will be just as, or even more significant as those of the Industrial revolution and much harder to predict for two reasons. First, they will depend on the speed that AI technologies will succeed in automating non-repetitive mental tasks currently performed by humans and replacing them in the process, and secondly the extent of the accelerated process of technological change as intelligent computer programs will become available and capable of developing new programs on their own. There is no doubt, therefore, that AI technologies coupled with the exponential growth of the Internet will affect how firms operate, how they sell their products/services as well as how they are managed, influencing employment patterns.
In 1995 when the digital revolution was in its infancy, in a Newsweek article in February of this year, Clifford Stoll wrote “Baloney. Do our computer pundits lack all common sense? The truth is no online database will replace your daily newspaper, no CD-ROM can take the place of a competent teacher and no computer network will change the way government works” (Stoll, 1995). Yet, twenty years later most people access newspapers electronically, we have Google search, Amazon is the 18th largest company in the Fortune 2016 list while Facebook had more than 1.8 billion users at the end of 2016. The extent of change can be seen in Table 2 that shows the ten largest firms on the Fortune 500 list of 1995 and 2016. The transformation is fundamental, demonstrating how the US economy went from the industrial age to a varied structure of firms in retail, oil and gas, hi tech, finance and health with the two largest industrial companies that were number one and two in 1995 to have fallen to the eighth and ninth position in 2016. Moreover, we see Walmart, a traditional retailer, at the top of the list which is interesting for two reasons. First, it confirms the shift from the industrial to a consumer oriented era, and secondly it proves that a traditional, brick-and-mortar firm can adapt to the digital revolution by successfully competing with Amazon (Petro, 2016), one of the most successful digital firms, on both low prices as well as superior customer service.
What is even more interesting than just “largeness” is the market capitalization of digital versus traditional firms. Table 3 lists four digital firms and four traditional ones comparing their market capitalization, revenues and number of employees. There are some important differences between the two groups. The number of employees of digital firms is about 15% of those of traditional ones while the market cap of the former is more than 67% that of the latter, whereas the revenues of traditional firms is more than 218% that of digital ones. Given the significantly lower number of employees, the higher market cap and the lower revenues of digital firms, the market cap per employee of high tech firms is an astonishing $4.13 million while their revenues per employee is $0.95 million versus $0.38 and $0.31 million respectively for traditional firms. There are probably several reasons for such large differences, three major ones being great productivity improvements through the extensive use of digital technologies including the Internet, continuous innovation of the products/services being offered to consumers and of course, high expected growth rates in their future revenues, skyrocketing their share price. For instance, Facebook's market cap per
employee of $22.1 million is more than nine times that of Johnson and Johnson (J&J) $2.43 million. But even its revenues per employee of $1.15 million are more than ten times higher than that of J&J, demonstrating the elevated productivity of Facebook's workforce.
The four digital firms are at the forefront of AI, investing huge amounts of money for internal AI research as well as buying promising start ups (CB Insights, 2016; Metz, 2016) with the purpose of offering AI technologies to their customers as well as developing new applications for the market place. Their huge market capitalization is a definite factor allowing them to acquire whatever start up is considered valuable for them to stay ahead in the AI race. The interesting question is if these four digital firms will remain at the top twenty years from now when the AI revolution will be the dominant force determining success and influencing the high value added products and services.
4.1. The successful, dominant firm of the AI revolution and its management
Revolutions, by definition, are associated with major changes. The Industrial one brought the large, industrial firm that exploited the power of machines to substitute, supplement and amplify the manual work performed by humans, increasing productivity considerable and offering affordable products to consumers, significantly increasing the size of the market as well as living standards. The digital revolution exploited the power of computers to substitute, supplement and amplify the routine mental tasks performed by humans also improving productivity and further contributing to reduced prices. As previously mentioned, the AI revolution aims to substitute, supplement and amplify practically all tasks currently performed by humans, becoming in effect, for the first time, a serious competitor to them. Accepting this watershed reality, setting aside criticisms about AI similar to those levied in 1995 against the Internet (Stoll, 1995), and accepting our short- sightedness to predict bold technological inventions, I will attempt, nevertheless, to predict the dominant firm twenty years from now and the unavoidable shift in employment.
In 1995 from the four digital firms listed in Table 3 two (Google and Facebook) were not even founded, Amazon was just a year old while Apple was nineteen years old but in a grave financial state, with a profit of just $0.4 billion that year and a loss of $0.8 and $1 billion in 1996 and 1997. It was not possible, therefore, to have predicted in 1995 that the market capitalization of these four firms would be (at the end of 2016) close to $1.9 trillion (more than the GDP of the 120 poorest countries in the world) or that their market cap per employee would be $4.13 million. Similarly, there is no accurate way to currently predict the dominant, successful firms of 2037 or the critical factors leading to such success as it was impossible to do so in 1995. Needless to say, we can consider the success factors of the four digital firms and attempt to extrapolate them, assuming that they may still hold true between now and 2037.
First and foremost all four firms have been extremely innovative, each one in its own unique way. Second, they have all used the Internet in a super effective manner to provide their services, sell their products and streamline their operations. Third, they have been successful in hiring top talent and motivate their top employees with a pleasant work environment, high salaries and generous stock options. Finally, these four firms have grown significantly by acquiring other companies, often promising start-ups in the selective areas they want to expand or attain expertise. Interestingly none of these four characteristics can be automated, or become part of an algorithm, at least during the next twenty years. In my view, they will continue to remain critical factors for succeeding in the future and will depend greatly on people's decisions and actions to implement them. Furthermore, algorithms will have to be modified as competitive, market, environment and other factors will be changing and only humans will be able to identify when such changes have occurred (otherwise, the algorithms will be dysfunctional). Below each of these four success factors is discussed in further detail.
Innovative breakthroughs: There are different types of innovation from minor, marginal ones to distinctive breakthroughs as those implemented by the four digital firms of Table 3. There are numerous books and articles written
 a The information for these firms was collected on 12/12/2016. All firms are American except Toyota.
S. Makridakis / Futures 90 (2017) 46–60 55
about innovative breakthroughs like the iPhone, or Google search and how they have transformed the world and brought riches to those that invented them. But what must be understood is that such breakthroughs are exceptional and only identified as such after the fact. In an older but fascinating book about breakthroughs (Ketteringham and Nayak, 1986), its authors concluded that there was one common characteristic of all the sixteen cases they studied. Nobody believed that they would be successful, urging their developers to stop wasting their time and company resources. These breakthroughs succeeded only because of the persistence of their inventors that did not give up despite all hardship and continued their development often in their own spare time. The value of breakthroughs is hard to appreciate as they involve brand new applications people are not familiar with, the value of which they cannot understand, or do not believe as technically possible as was the case with fibre optics. Even the value of Google search was not apparent in the late 1990s as can be attested by the fact that its inventors could not sell it for the asking price of $1.6 million (Battelle, 2005, p. 83).
It cannot be assured that by identifying interesting innovative ideas they will turn into commercially successful applications. The road is long and full of risks as many things can go wrong, including technological problems, inability to ensure adequate financing, competitors developing similar ideas and possible delays until people realize their importance and decide to use them. Moreover, it is worth mentioning that the great majority of innovative ideas do not succeed in a big way, that new competitors can improve the original idea and capture market shares and that many inventions can become obsolete by more novel ones often introduced by garage start-ups (see below). Creative destruction is at the heart of market economies and the major contributor to productivity increases as well as the offering of a greater choice of product/services to consumers. No concrete advice on how to innovate, not to mention achieve breakthroughs, can be provided beyond saying that some organizations are better than others, at least for some period of time, in creating the right environment to cultivate innovation and exploit its benefits. It must be emphasized, therefore, that success is unpredictable and that the more valuable the innovative idea the harder it is to conceive and implement. Thus, it is unlikely that innovative breakthroughs can be predicted algorithmically during the next 20 years, if ever.
Technologies and their usage: The digital revolution has provided significant productivity improvements for the back office operations of firms and allowed consumers to buy goods and services online without having to physically go to a store or office. In the process it has eliminated a large number of repetitive mental tasks performed by people and shifted employment patterns. This is evident in Table 3 by comparing the number of employees between digital and traditional firms. For instance, the number of employees of Walmart is more than eight times higher than that of the other retailer Amazon, while the revenues of the former is only 4.5 higher than the latter, showing the great productivity improvements being realized by Amazon's effective utilization of digital technologies, and the Internet. It is likely that the AI revolution will continue or even accelerate that pattern, further improving productivity and reducing employment with some firms being more successful than others by exploiting emerging technologies and investing considerable sums to do so.
The uniqueness of the AI technologies is their potential to supplement, substitute and amplify practically all tasks currently performed by humans with critical consequences for firms that must achieve significant productivity improvements to stay competitive, but at the same time raising the possibility of increased unemployment (see below). Extrapolating from the impact of the Industrial and digital revolutions it seems that technology has created more jobs than it has destroyed (Stewart et al., 2015) although there may be a transitional period of increased unemployment until new opportunities are created to serve the emerging needs of those with increased incomes. The difficulty would be in knowing which of the AI technologies would provide the greatest benefits and determining the right time to start investing in them. It is doubtful that such an investment decision could be made algorithmically as it would require predictions about the pros and cons of future and uncertain technologies. This means that top executives will have to consider the advantages and drawbacks of the available investment possibilities and make the final decision(s) judgmentally using all information at hand.
Managing people: It is also highly likely that the trend towards a smaller payroll will continue as AI technologies will accelerate the number of tasks that can be performed by machines and robots. The more jobs being automated however, the greater the skills that would be required to adequately perform the remaining tasks, for both the efficient operation of firms as well as for utilizing AI and other technologies in the best possible way. This would require hiring talented employees and motivating them to get the most out of their performance in order to attain and maintain competitive advantages over other firms. The major differentiator would therefore come from these talented individuals conceiving and implementing innovative ideas and winning strategies that would steer the organization onto a successful path. Hiring, motivating and successfully managing talented individuals would probably be one of the most critical success factors for firms in the AI era and would also be impossible to put into an algorithm.
Growth by acquisition: The four digital firms of Table 3 have grown substantially through acquisitions which also allowed them to obtain and secure desired expertise in vital areas. Together they have acquired more than 400 firms (Whittick, 2016) with Google leading the way with 190, Apple with 82, Amazon with 67 and Facebook with 62 (including WhatsApp bought for $19 billion, a huge price for a start up with only 55 employees and less than five years old). In the past, new technological inventions have fundamentally affected the revenues and profitability of firms and have become a critical success factor. It is exceedingly likely that this pattern will continue, if not accelerate with AI technologies creating uncertainty for established firms as such technologies, or who will introduce them first cannot be predicted. The huge market capitalization of the four firms of Table 3 makes it easier for them to acquire start-ups which they believe will be strategically important for their future expansion through the issuing of more shares if necessary. In the process they can maintain and/or increase their monopolistic/oligopolistic advantages while also squashing possible, future competition. It is interesting to consider the
56 S. Makridakis / Futures 90 (2017) 46–60 implications if this trend continues in the future and what it will mean for competition as it can create a “winner takes all”
environment favouring today's dominant firms. 4.2. Competitive requirements: big data, algorithmic decisions and operational excellence
Let us assume that an advanced algorithm is developed to predict exactly what customers want. What will the result be? Even if it is proprietary, competitors will inevitably figure out how it works and also utilize its predictions after some period of time. If it is available to everyone at a price, all competitors will swiftly adopt this algorithm to stay competitive. Those who do not utilize it will probably go out of business given its superiority to predict customers’ wants. There will be no special benefit as the advanced algorithm would simply become a competitive requirement for staying in the race. Big data ( Schonberger & Cukier, 2014) offers the opportunity of developing successful algorithms to understand what customers want and as such can be extremely useful for decision makers. The challenge is that both the data and the techniques to analyze them are available to practically everyone interested, turning their recommendations into a competitive requirement. Jankel (2015) is therefore probably right, stating that “computers will never create disruptive innovations” nor be able to provide advice on the three major factors (selecting the right technologies, managing people and making the appropriate acquisitions) which characterize the four digital firms of Table 3.
4.3. The 2037 successful firm
The successful 2037 AI era firms cannot be predicted but they will probably be closer to the digital than the traditional ones of Table 3. They will exploit global opportunities to the limit in order to design, produce and sell their products/services and they must be willing to assume entrepreneurial risks to innovate and grow. The speed of technological change from the forthcoming AI revolution will open huge opportunities for growth and profitability but also new challenges and competition from new garage type start-ups as breakthrough ideas can come from anywhere and their development and financing will be easier through crowd sourcing and venture capital. Only time will tell if the successful firms of the digital revolution (as the four listed in Table 3) will still be dominant in 2037 or if they will be replaced by new ones. In my opinion, there are accrued competitive advantages that could be exploited to stay at the top, but at the same time big technological breakthroughs can fundamentally change the competitive landscape and wipe out such advantages. But what is probable is that AI will bring revolutionary changes to the business environment and the way firms are inspired, founded and managed at a much greater scale than the digital era. The successful firms during the AI revolution will have in addition to the Chief Innovation Officer (CIO) to also appoint a Chief Artificial Intelligence Officer (CAIO) to be in charge of evaluating and exploiting AI technologies to gain the most out of their implementation in all aspects of the firm.
4.4. Garage start-ups, crowd sourcing and VC Funding
The way the four digital firms of Table 3 were created was quite different than that of traditional ones. They were conceived with a vision to change the world, even though making money was no doubt also part of their founders’ objective. Financing was secured mainly through Venture Capital (VC) (with the exception of Apple that was founded well before VC funding was readily available) that invested heavily in these firms that managed to grow rapidly over time (more than 20% a year) and achieve stellar market capitalization putting all four of them at the top ten in the USA. This is quite different to that of traditional firms that depend heavily on the personal financing of their owners, supplemented with bank loans, and apart from a few exceptions, experience low growth rates, taking decades to reach the top.
The widespread utilization of the Internet has provided two significant tools for start-ups that can utilize crowd funding and crowd creativity to improve their chances of success. Such tools coupled with VC funding will increase the number of new firms aimed at exploiting AI technologies and the chance to succeed in big ways like Google, Amazon or Facebook. Breakthrough ideas can come from anywhere and they do not require expensive laboratories or huge financing to develop and market. This means that garage operations, small offices in incubators or accelerators, as well as cubicles in universities, can provide an initial base for start-ups that can emerge everywhere outside Silicon Valley as their value in wealth creation is becoming obvious. Today a large number of countries and individuals understand the importance of innovation and entrepreneurship for their economies and encourage their youth to implement their innovative ideas by starting their own business. Israel has been extremely successful in doing so (Senor and Singer, 2009) as has China, Germany, UK, France, Chile and New Zealand among others. Characteristic of efforts to encourage and expand start-ups is France's latest mega- 35,000 m2 campus being built in Paris to house start-ups under one roof, or the vision of Chinese's President Xi Jinping to turn his country into a global technology powerhouse. The implications of these efforts will be the globalization and acceleration of technological innovation, greater competition and more equal distribution of employment and wealth across nations that encourage technological innovation.
4.5. Employment patterns
The 1995 paper showed the continuous decline in the agricultural and manufacturing (since it peaked in the 1950s) employment and the growth in that of services. Tables 4 and 5 indicate that the same pattern has continued since 1995 with
S. Makridakis / Futures 90 (2017) 46–60 57 USA: % of labor force in agriculture, manufacturing and services.
Table 5
UK: % of labour force in agriculture, manufacturing and services.
employment dropping from 2.5% to 0.7% in agriculture and from 17% to 14% in manufacturing in the USA and correspondingly from 2.1% to 1.3% and from 21% to 15% in the UK while witnessing increases in the percentage of employment in services. If current trends continue agriculture and manufacturing employment will further decrease as AI automation will affect both agricultural and manufacturing operations and jobs. This means that employment in services will have to broaden to compensate for the jobs being lost in agriculture and manufacturing. However, the service sector will also witness considerable realignments as many jobs will be eliminated and new ones will be created.
What critics fear is the speed of job obsolescence through AI technologies in particular in the service sector. They say that while it took more than two centuries to witness the full impact of the Industrial revolution and three or four decades to experience that of the digital one, it may be no more than a decade until we observe the full effects of the AI revolution (The New Yorker, 2016). Another concern is the extent of displaced jobs that some studies (Frey and Osborne, 2013) estimate as 47% of all occupations. What experts are afraid of is whether the social structure could withstand such a fast and huge reduction in jobs. They claim that even if there were vast retraining programs it may not be easy to avoid societal disruptions as the new jobs would require skills that may not be so easily attainable. In addition to the truck and taxi drivers already mentioned that could be replaced by self-driving vehicles, there are many other jobs at stake. According to PBS (Thoet, 2016) the newly announced Amazon Go retail store, using AI technologies to abolish employees, “could drastically change the way people shop and eventually eliminate the need for millions of workers, industry experts predict.” Other jobs that could be affected range from preparing and taking payments for fast food to skilled professions including financial advisers, medical specialists as well as high level IT tasks that could be automated using algorithms such as those available through Amazon's Web Services. The big question is where will all these lead and what will the implications be for employment and wealth distribution?”
A recent McKinsey study entitled “Where machines could replace humans – and where they can’t (yet)” (Chui et al., 2016) distinguishes three groups of occupational activities that are highly susceptible, less susceptible and least susceptible to machines and robots taking over jobs currently performed by humans (with the number in parentheses denoting the
estimated percentage substitution). The first group includes “data collection” (64%), “data processing” (69%) and “predictable physical work” (78%), the second covers “interactions among stakeholders” (20%) and “unpredictable physical work” (25%) while the third contains “managing others” (9%) and “applying expertise” (18%). Other studies found that jobs related to social skills are of outmost importance (Rifkin, 2014; Mensel and Tholl, 2015) and grew 10% a year between 1980 and 2012 while all others declined by 3% during the same period (Deming, 2016). However, will the new jobs being created compensate for, or even augment those lost by technology as in the past? Opinions differ greatly on this issue as some are arguing that there is a fixed supply of work to be done, while others, with Milton Friedman the most notable advocate, claims that human wants and needs are infinite and that clever entrepreneurs will always devise ways to fulfil them as long as sufficient buying power is available (Susskind and Susskind, 2016). Others worry, however, saying “what if the jobs of the future are also potentially automatable?” (The New Yorker, 2016).
In an article entitled “Will Humans Go the Way of Horses? Labour in the Second Machine Age” (Brynjolfsson and McAfee, 2015) its authors consider both sides of the argument and conclude that there are no obvious answers. Instead they propose: “It's time to start discussing what kind of society we should construct around a labor-light economy. How should the abundance of such an economy be shared? How can the tendency of modern capitalism to produce high levels of inequality be muted while preserving its ability to allocate resources efficiently and reward initiative and effort? What do fulfilling lives and healthy
communities look like when they no longer center on industrial-era conceptions of work?”
The critics claim that even if technological automation may not increase unemployment, it can destroy middle range jobs while increasing those on the low and high ends, augmenting social inequality as the pay between low and high end jobs is amplified. Governments must, therefore, enact policies to minimize such inequalities either by guaranteeing a minimum universal income, raising taxes for the superrich and/or increasing inheritance taxes. During the digital revolution the strong demand for computer and data specialists resulted in raising their salaries steeply while eliminating large numbers of clerical jobs. The prediction is that the same thing is happening for AI specialties, such as deep learning professionals, necessary for the further development of AI technologies (Metz, 2016). In all likelihood such demand will further intensify until more people are educated to be able to fill these jobs.
In addition to specialized AI professions, employment opportunities will exist by moving from traditional jobs susceptive to automation, to those demanding social and interpersonal skills as well as creativity and innovation. There will also be a demand for novel jobs that will aim to satisfy the needs of higher income people that can afford personal trainers, coaches, beauty advisors, diet consultants, nutritionists and teachers for their children, among others. These and similar jobs will offer employment opportunities and adequate payment, and will compensate for those lost when AI technologies will replace existing jobs.
Finally, the impact of the AI revolution will probably be more pronounced in developing countries than in advanced ones for two reasons. First, as unskilled and semiskilled labour will be replaced by computers and robots there will be no reason for firms to move their production to developing nations to exploit their cheap supply of labour as they can achieve the same or cheaper costs utilizing AI technologies, thus increasing the trend towards “reshoring” back to advanced countries (Ford, 2016). Second, developing countries will be at a disadvantage by not being able to invest in expensive AI technologies, particularly since such technologies will reduce the demand for human labour thus further increasing unemployment. Unfortunately, there will be no easy solutions, with the greatest challenge being how to educate their young people in AI technologies and by doing so become able to attract investments from abroad as well as manage to exploit the “sharing economy” (Rifkin, 2014). Otherwise, they will stay behind as the nations that did not manage to industrialize their economies.
4.6. Work and leisure
Machines, electrification, cars, computers, the Internet and smartphones among others inventions have affected our lives and shaped our work and social environment. The impact of AI technologies can be even more profound than that of both the Industrial and digital revolutions put together, as it holds the potential to affect practically all tasks currently performed by humans, diminishing the amount of work left for people and increasing wealth inequalities and their free time. Proponents of the AI technologies see this as a positive development liberating people from routine work, allowing them to pursue their own interests. There are two types of criticisms. The first argues that economic growth in developed countries has stalled during the last five decades (Gordon, 2016) and that technology has not been able to raise it resulting in declining real median wages and in stagnant median household incomes. The second points to increase inequalities as fewer people will hold the well-paying jobs and the majority will depend on part time work or limited employment opportunities and therefore receive a lower income. There are counter arguments to these criticisms with Cowen (2016) saying that the current slowdown is temporary and that economic and productivity growth rates cannot be predicted as for example the impact of autonomous vehicles that could revolutionize the automobile industry and create strong economic growth. Concerning inequalities it will all depend as Brynjolfsson and McAfee say “what kind of society we should construct around a labour-light economy? How should the abundance of such an economy be shared?” At the beginning of the Industrial revolution people used to work fifteen hours a day, six or seven days a week. Today a standard work week is less than half and there is no reason that it cannot be halved in the future as productivity increases.
Aristocrats did not work at all in the past, devoting all their time to leisurely activities, hobbies, holidays and travel and things they were interested in. The citizens of ancient Athens spent practically all their time philosophizing, exercising and
S. Makridakis / Futures 90 (2017) 46–60 59
concerned with democracy, while slaves did practically all the work. The optimists dream is that we can all become the “new” aristocrats, or the “modern” Athenians, with computers and robots as our slaves doing all the housework, the shopping and working at the office, the factory or the land. Whether this dream is a utopian or dystopian future is left up to the reader to decide, not underestimating however, that intelligent machines will eventually become at least as smart as us and a serious competitor to the human race if left unchecked and if their great potential to augment our own intellectual abilities is not exploited to the maximum.
5. Conclusions
The societal impact of the digital revolution has been significant as it has affected most aspects of our lives and work, having moulded the dominant firm, shaped our shopping and entertainment habits as well as our employment patterns. This paper argues that the AI revolution is on target and will come into full force within the next twenty years as did the digital one since 1995 and will probably have an even greater impact than both the Industrial and digital ones combined. What is uncertain is if such an impact will lead to a utopian or dystopian future, or somewhere in between, which according to the optimists will happen when some mutually beneficial coexistence is achieved by accepting our limitations and seeking ways to augment our own decision making abilities in a similar way that the world chess champion is now a human utilizing the help of a computer to determine his moves. The pessimists worry that when AI machines become smarter than us they will be making all our decisions for us, from the less important ones like SIRI or Cortana choosing a restaurant and ordering food and wine for their owners, to important ones like driving cars or managing nuclear plants. Pessimists are also concerned with social discontent as the amount of work available for people will diminish, and with the increasing wealth inequality between those employed or owning AI applications and the rest.
This paper has described the substantial uncertainty about the future impact of AI technologies and their potential to create a utopian or dystopian world. As we head towards unchartered waters and uncertain choices the obvious challenge is what can be done to maximize the chances of exploiting the benefits while avoiding the negative consequences of AI technologies? There are two positive aspects in dealing with this challenge. First, the dangers are well understood and secondly, there is plenty of time to debate the issues and take wise actions to deal with them effectively. Increased unemployment and greater wealth inequality are debatable given that the Industrial and digital revolutions decreased rather than increased unemployment (Stewart et al., 2015), although some claim that this may change drastically with the widespread introduction of AI technologies leading to massive job reductions and bringing us towards Huxley's Brave New World. Scientists like Etzioni (2016), however, do not believe that AI is a threat to humanity, maintaining that it would be a great pity to forfeit its great advantages out of unsubstantiated fears that it may get out of control. There is little doubt that new technological breakthroughs similar to those of the Internet and smartphones of the digital revolution will probably emerge during the next twenty years and that will greatly impact our societies, lives and work in general. The challenge will be to identify such breakthroughs as early as possible in order to exploit their benefits.
Concerns about the AI related risks are not unique. Possible catastrophes, in particular those that threaten to destroy the environment or end human civilization attract people's attention and are popular among both scientists seeking to publicize their work and journalists looking for “cool” stories. Nuclear wars, global warming, accidents in nuclear plants, unchecked epidemics, and geoengineering, among others, worry people who are concerned about their implications. At the same time it is clear that AI risks cannot be ignored even though the probability of their occurrence may be extremely small as their potential impact, according to critics, can be devastating, ending human supremacy when machines smarter than people are developed. At the same time progress cannot be halted which means that the only rational alternative is to identify the risks involved and devise effective actions to avoid their negative consequences. In ending this paper I would like to quote Faggella (2016) saying: “Thinking about the risks associated with emerging AI technology is hard work, engineering potential solutions and safeguards is harder work, and collaborating globally on implementation and monitoring of initiatives is the hardest work of all. But considering all that's at stake, I would place all my bets on the table and argue that the effort is worth the risk many times over”
",https://edisciplinas.usp.br/pluginfile.php/4310583/mod_resource/content/1/The forthcoming Artificial Intelligence (AI) revolution_ Its impact on society and firms.pdf
The Impact of Artificial Intelligence on Firm Performance: An Application of the Resource Based View to e-Commerce Firms,NCBI,"Donghua Chen, Jose Esperance, Shaofeng Wang","RESEARCH Construction of a higher order model linking artificial intelligence capability (AIC) to firm performance, demonstrating that AIC, enhanced by creativity and management, indirectly boosts performance through AI-driven decision making, with innovation culture and environmental dynamism further influencing these relationships.",2022,"The application of artificial intelligence (AI) technology has evolved into an influential endeavor to improve firm performance, but little research considers the relationship among artificial intelligence capability (AIC), management (AIM), driven decision making (AIDDM), and firm performance. Based on the resource-based view (RBV) and existing findings, this paper constructs a higher-order model of AIC and suggests a research model of e-commerce firm AIC and firm performance. We collected 394 valid questionnaires and conducted data analysis using partial least squares structural equation modeling (PLS-SEM). As a second-order variable, AIC was formed by three first-order variables: basic, proclivity, and skills. AIC indirectly affects firm performance through creativity, AIM, and AI-driven decision making. Firm creativity, AIM, and AIDDM are essential variables between AIC and firm performance. Innovation culture (IC) positive moderates the relationship between firm creativity and AIDDM as well as the relationship between AIDDM and firm performance. Environmental dynamism (ED) positive mediates the connection between AIM and AIDDM. Among the control variables, firm age negatively affects firm performance, and employee size does not. This study helps enterprises leverage AI to improve firm performance, achieve a competitive advantage, and contribute to theory and management practice.
Keywords: artificial intelligence capability, firm performance, resource-based view, PLS-SEM, firm creativity, driven decision making, environmental dynamism, innovative culture
Go to:
Introduction
The rapid evolution of artificial intelligence (AI) brings enterprises more business opportunities (Hughes et al., 2020; Obschonka and Audretsch, 2020; Shareef et al., 2021). Artificial intelligence is the machines (programs) that operates in the simulation of human intelligence (Łapińska et al., 2021) in technologies, such as machine learning, data mining, natural language processing, image recognition, etc. (Khalid, 2020). Artificial intelligence can bring efficiency gains, cost savings, product quality improvements, and customer service improvements (Bag et al., 2021c). Enterprise capabilities are critical for identifying business opportunities (Yao et al., 2021). While there is excellent potential for artificial intelligence capability (AIC) to improve a company’s performance (Mikalef and Gupta, 2021), there are also significant challenges to these companies applying AI (Yu et al., 2021). Businesses can utilize AI to improve the customer service experience by offering more appropriate recommendations and less costly options (Payne et al., 2021). According to the resource-based view (RBV; Majhi et al., 2021); artificial intelligence’s applied capability is an ensemble of implicit resources (Bag et al., 2021c). These resources include supporting resources, labor skills, and organizational coordination (Kim, 2019; Selz, 2020). Once a firm masters organizing resources that are impossible to copy effortlessly, it possesses a competitive advantage (Yasmin et al., 2020) and enhances firm performance (Chen and Lin, 2021). Therefore, there is an essential theoretical and practical value in exploring the mechanisms and critical factors of the impact of AIC on firm performance (Chen and Lin, 2021; Mikalef et al., 2021), especially in the e-commerce industry with direct customer contact (Wang and Fan, 2021).
A broad study of the impact of AI and its capability on business performance appears (Denicolai et al., 2021; Mikalef and Gupta, 2021). The existing literature dedicated to the study of the impact of AI on industries, such as banking and finance (Huynh et al., 2020), manufacturing (Bag et al., 2021c), automated retailing (Pillai et al., 2020), logistics (Chien et al., 2020), marketing (Keegan et al., 2022), coaching services (Kim et al., 2021b), and customer relationship management (Chatterjee et al., 2021a), among other areas. In comparison, these studies concentrated on the impact of AI on firm innovation processes and management practices, technological innovation (Liu et al., 2020a), and the relationship between AI learning and entrepreneurial performance (Khalid, 2020). In e-commerce, technology applications of AI are also proliferating nowadays (Volkova et al., 2021). For example, e-commerce firms predict the most acceptable promotion targets (Giannoulakis, 2020) and pricing strategies (Shang et al., 2020) founded on consumers’ recorded user profiles, trajectories, and consumption history. E-commerce firms’ consumer product recommendations are built on robust data analysis (Li et al., 2021). The AI customer service can help customers solve problems quickly (Varsha et al., 2021). E-commerce companies can deepen exploration and analysis under past data to capture market trends to improve operational efficiency (Cui et al., 2021). However, little is known about the mechanism of AIC composition of e-businesses and AIC’s impact on e-business performance (research gap 1).
Enterprise creativity is a key to generating new ideas, products, and services (Yao et al., 2021) and is a potential factor simulating business performance (Mikalef and Gupta, 2021). Big data can enhance AIC (Ghasemaghaei, 2021) and decision-making for more profitable business outcomes (Denicolai et al., 2021). The AI-related business management systems are essential factors in optimizing business performance (Rahman et al., 2021). Despite the great potential of AI technologies to facilitate firm performance, the current publications rarely focus on the firm creativity, artificial intelligence management (AIM), and artificial intelligence-driven decision making (AIDDM) in the relationship between the two (research gap 2). Dynamic changes in the business environment may influence the application of digital technologies represented by AI (Bag et al., 2020b) and firm performance (Dubey et al., 2020). Organizational culture of innovation may also be an essential variable affecting firm performance (Dubey et al., 2020). However, existing investigation have few moderating variables under innovation culture (IC) and environmental dynamism (ED) to demonstrate the relationship between AIC and firm performance (research gap 3).
To fill the current research gaps, this study will investigate the internal components of AIC. We will analyze the impact of AIC, management, driving decision making, firm creativity, innovation culture, and environmental dynamism on firm performance, using e-commerce firm performance. We constructed the higher-order variable of AIC, the theoretical model of the impact of AIC on e-commerce firm performance, and the corresponding research hypotheses relating to the RBV and existing relevant research results. We use partial least squares structural equation modeling (PLS-SEM) analysis to empirically analyze 394 valid questionnaires to test further hypotheses and theoretical models proposed in this study. Based on the data analysis, we discussed the effects of higher-order variables (AI capability), moderating variables (innovation culture and environmental dynamism), and mediating variables (firm creativity, AIM, and AIDDM) on firm performance. This study makes theoretical contributions to AI and firm performance and provides essential guidance for e-commerce companies to improve their performance and develop a competitive advantage.
Go to:
Literature Review and Conceptual Model
Resource-Based View
The RBV believes that essential resources determine firm performance (Barney, 1991; Chatterjee et al., 2021b). Resources can be tangible and intangible assets within an organization (Mikalef and Gupta, 2021). According to this theory, valuable, rare, inimitable, and irreplaceable resources can build a competitive advantage by creating value and improving firm performance (Barney, 1991; Ghasemaghaei, 2021). Such an advantage can persist over a long period (Bag et al., 2021c). Businesses can raise the value of their resources because the combined value of the complementary resources is higher than the sum of each resource (Ghasemaghaei, 2021; Mikalef et al., 2021).
Artificial intelligence capability is increasingly a critical and intangible resource for business performance advancement (Belhadi et al., 2021; Lou and Wu, 2021; Mikalef and Gupta, 2021). It suggests that artificial intelligence may bring a competitive advantage to businesses (Chaudhuri et al., 2021). AIC can deliver businesses access to valuable, rare, inimitable, and irreplaceable resources (Ghasemaghaei, 2021). Many studies have deemed “firm capability” as a mediator between resources and firm performance (Belhadi et al., 2021; Lou and Wu, 2021; Mikalef and Gupta, 2021). Firm capabilities are vital attributes required for business operations (Yao et al., 2021). These capabilities help deploy other necessary resources to improve firm performance (Yao et al., 2021). We focus on the firm capability in creating value because AIC can enhance the firm’s capabilities and improve firm performance (Chatterjee et al., 2021a). RBV is frequently used to demonstrate the association among firm resources, capabilities, and performance (Barney, 1991; Chen and Lin, 2021; Hossain et al., 2021; Rahman et al., 2021). Therefore, this study will utilize the RBV in the following processes.
Artificial Intelligence Capability
Artificial intelligence has gained much attention as a potential to increase the competitive advantage of firms (Hughes et al., 2020; Obschonka and Audretsch, 2020; Shareef et al., 2021). Firm capabilities of applied AI also appear to be particularly valuable (Yu et al., 2021). Combined with RBV, we define AIC of e-commerce firms as the ability of firms to build, integrate, and utilize AI-based related resources (Łapińska et al., 2021; Mikalef and Gupta, 2021). Effective and efficient implementation of AI in firms, on the other hand, requires significant infrastructural resources (tangible resources; Bag et al., 2021c; Chatterjee et al., 2021a), which includes financial support (Łapińska et al., 2021), data (Herhausen et al., 2020; Hwang and Kim, 2021), hardware devices and software (Zhang et al., 2020), and technical support (Rahman et al., 2021). The majority of the businesses across E-commerce companies are throughout the Internet, and thus they have a natural advantage in acquiring data resources (Wang and Fan, 2021). While the firm sets down the basic tangible resources, it is significant to efficiently employ the proclivity of using AI (intangible resources; Ashaari et al., 2021; Yu et al., 2021). If firms are not inclined to implement AI in planning, coordination, control, and implementation (Belhadi et al., 2021; Chen and Lin, 2021), even if they have access to very superior AI base resources will not help (Denicolai et al., 2021). With tangible resources (base resources) and intangible resources (proclivity), technical skills (human resources) should also be taken into account (Baldegger et al., 2020; Bag et al., 2021c). Employees’ skills expertise would restrict the difficulty of spreading AI-related technologies (Chatterjee et al., 2021a; Chen and Chen, 2021). Understanding the scope of AI applications and acquiring skills and expertise in using AI systems are prerequisites for employees to apply AI shortly (Vrontis et al., 2021). Therefore, this study constructs three types of resources (shown in Figure 1) for AIC in e-commerce firms: basic (tangible resources), proclivity (intangible resources), and skills (human resources).


Figure 1
Constructs of artificial intelligence capability (AIC) in e-commerce firms.
Hypothesis Development
Firm performance (FP) is an important indicator to assess the business’s financial, operational, marketing, and team cooperation (Dubey et al., 2020). Artificial intelligence positively impacts company health and business performance (Yasmin et al., 2020). AIDDM enables firms to systematically collect, evaluate, and analyze the analytics recommended by artificial intelligence systems enhancing decision-making quality and efficiency (Ashaari et al., 2021). Artificial intelligence can collect and comprehend realistic solutions to complex problems (Awan et al., 2021), providing a more reliable decision-making basis (Elia et al., 2021). Artificial intelligence systems provide business managers information transformed from data so that administrators and business executives can solve existing and potential problems (Ashaari et al., 2021). Corporate executives gradually pay more attention to AIDDM as the evidence reveals it can facilitate business innovation (Chaudhuri et al., 2021), supply chain resilience (Zhang et al., 2021), efficiency gains, cost savings, product quality improvements, and customer service improvements (Bag et al., 2021c). AI-assisted decision-making based on artificial intelligence significantly benefits organizations to improve business operational efficiency and performance (Ashaari et al., 2021). Firms that use AIDDM can achieve higher productivity and superior performance (Chatterjee et al., 2021a). Based on the analysis above, the proposed hypotheses are:
Hypothesis 1 (H1): A positive impact of AIDDM on FP.
Firm creativity (FC) is an essential driver of innovation and competitive advantage for firms (Kim et al., 2021a). This study defines firm creativity as the ability of a firm to create novel and valuable ideas (Ferreira et al., 2020). Today’s business environment has become increasingly complex and fluid, and creative organizations tend to be more likely to experiment with new technologies and incorporate them into their daily operations (Liu et al., 2020b). Firms that appreciate creativity are also more willing to attempt the latest technologies, such as artificial intelligence, big data, and cloud computing to convert business processes and decision-making mechanisms (Mikalef and Gupta, 2021). Firm creativity is a necessary antecedent that influences firms to compose strategic thinking and decisions (Dixit et al., 2021). Artificial intelligence management (AIM) refers to a firm support management system assisting the implementation of artificial intelligence (Bag et al., 2021a; Łapińska et al., 2021). Such initiatives depend heavily on the commitment of firms to implement AI technologies (Haesevoets et al., 2021). AIC is firm competitiveness requiring multiple resources to complement each other but can only be guaranteed through long-term monitoring activities (Raisch and Krakowski, 2021). The system development and its updating for artificial intelligence management can improve firms’ quality decisions (Saenz et al., 2020), thus driving better returns for firms (Blohm et al., 2020). The nature of artificial intelligence is a decision-making technique associated with artificial intelligence techniques (Verganti et al., 2020). Suggestions from artificial intelligence consulting systems can affect firms’ decisions (Keding and Meissner, 2021), and firms that master AIC may be willing to choose AI to navigate their decisions (Ashaari et al., 2021). We therefore propose:
Hypothesis 2 (H2): There is a positive impact of FC on AIDDM.
Hypothesis 3 (H3): There is a positive impact of AIM on AIDDM.
Hypothesis 4 (H4): There is a positive impact of AIC on AIDDM.
Artificial intelligence offers creative statements and solutions for firms (Paschen et al., 2020), positively affecting firms’ creativity enhancement (Amabile, 2020). The AIC takes many repetitive tasks and supplies more solutions for firms encountering complex issues (Raisch and Krakowski, 2021). With AIC, firms can devote more human resources to creative activities (Mikalef and Gupta, 2021). The value of AI is closely related to AIM systems (Bag et al., 2021a). Companies can also develop AIC while enhancing the formation and use of AIM systems (Metawa et al., 2021). The implementation of artificial intelligence management systems needs to be driven by various resources of AIC (Rahman et al., 2021). Firms can automate management activities with the help of AI technologies, and firms with AIC can advance AIM (Bag et al., 2020a). In summary, we hypothesize that:
Hypothesis 5 (H5): There is a positive impact of AIC on FC.
Hypothesis 6 (H6): There is a positive impact of AIC on AIM.
In this scenario, IC is an enterprise culture that encourages innovation (Khattak et al., 2021), encouraging motivation and adopting technologies like artificial intelligence within firms (Zhang et al., 2021), and it helps firms reach higher business goals (Chaudhuri et al., 2021). In the face of intense market competition, encouragement of firm innovation is more likely to transform processes and decisions with digital technologies represented by artificial intelligence to gain new business opportunities and improve performance (Yu et al., 2021). Innovation culture plays an essential role in organizations adopting new decision-making approaches and transforming organizations (Chen and Lin, 2021), and AI-based driven decision-making implies opportunities to enhance business performance (Ashaari et al., 2021). Therefore, the hypothesis proposed in this study is:
Hypothesis 7 (H7): There is a positive moderating effect of IC on the relationship between AIDDM and FC.
Hypothesis 8 (H8): There is a positive moderating effect of IC on the relationship between FC and AIDDM.
This study defines ED as modifications and uncertainties in a firm’s external business environment (Dubey et al., 2020; Haftor et al., 2021). Though external elements mainly influence ED, it affects internal management and decision-making (Belhadi et al., 2021). Firms also respond to changes in the external environment with appropriate strategies under management and decision-making (Haftor et al., 2021). Unforeseen circumstances may cause instability in customer demand and uncertainty in product supply, which may require more flexible management strategies to cope with it (de Haas et al., 2020; Sheth, 2020). Firm performance relies on the external environment because the firm cannot run the business independently without supply and demand (Dubey et al., 2020). The moderating effect of environmental dynamism affects firm performance with its antecedent variables (Wamba et al., 2020). Moderate environmental dynamism can also positively affect AIDDM and firm performance (Dubey et al., 2020). In short, we propose the hypothesis:
Hypothesis 9 (H9): There is a positive moderator of ED on the relationship between AIDDM and FP.
Hypothesis 10 (H10): There is a positive moderation of ED on the relationship between AIM and AIDDM.
Firm performance may vary depending on the different firm features (Zhang et al., 2020), such as the age (Bag et al., 2021b; Chen and Chen, 2021) and the number of employees (Pinheiro et al., 2021; Arias-Pérez and Vélez-Jaramillo, 2022). We construct these two features as control variables in this study.
Conceptual Model
This study aims to explore the impact of AIC on e-commerce firm performance. We incorporated the independent variables (AI capability), moderating variables (innovation culture and environmental dynamism), mediating variables (firm creativity, AIM, and driving decision making), and dependent variables (firm performance) based on the research results related to RBV, AI and firm performance. The proposed research model is shown in Figure 2.


Figure 2
Conceptual model.
Go to:
Materials and Methods
Measurement Scale
To achieve the objectives of this study, scales were developed for data collection (Ghasemaghaei, 2021; Kar et al., 2021). The scale used in this study was adapted under the extant literature (Payne et al., 2021; Yao et al., 2021), and the scale development was divided into three phases. We developed the initial phase’s initial scale in the first phase through a theoretical and literature review.
In the second phase, an English questionnaire was prepared and then translated into Chinese, with the Chinese version subsequently back-translated by a third party to ensure accuracy (Zhang et al., 2020; Wang and Fan, 2021). We sent the draft to six researchers and practitioners to acquire feedback on each questionnaire item to ensure that language did not restrict the understanding of the scales. Several consultations and revisions developed the measurement scales for the pilot survey (Akram et al., 2018a).
In the third phase of developing the questionnaire, we conducted a pilot survey of 30 executives in the Chinese e-commerce industry. We modified the wording of the instrument based on the feedback received. Table 1 presents each construct’s measurement questions and references to support the theoretical framework. The instrument was designed and developed based on a five-point Likert scale (1 = “strongly disagree”; 2 = “disagree”; 3 = “neutral”; 4 = “Agree”; 5 = “Strongly Agree”), which has been widely used by researchers in business management research in the past (Chatterjee et al., 2021a; Chaudhuri et al., 2021; Chen and Chen, 2021; Denicolai et al., 2021; Mostafiz et al., 2021; Rahman et al., 2021).
Table 1
Measurement scale.
Code
Items
Artificial intelligence management (AIM) adapted from Bag et al. (2021a).
AIM1
We employ an artificial intelligence system.
AIM2
We continuously monitor the progress of the AI system.
AIM3
We continuously update the AI system.
Firm performance (FP) adapted from Chatterjee et al. (2021a,b).
FP1
We are growing our market share faster.
FP2
We are not currently experiencing financial difficulties.
FP3
We continue to introduce new products and services.
FP4
AI implementation is helping to improve business performance.
Firm creativity (FC) adapted from Mikalef and Gupta (2021).
EC1
We generate many new and useful ideas.
EC2
Our firm climate helps generate new and useful ideas.
EC3
We believe it is important to generate new and useful ideas.
Artificial intelligence driven decision making (AIDDM) adapted from Ashaari et al. (2021).
AIDDM1
We believe it is important to have, understand and use AI.
AIDDM2
We rely on AI to support decision-making.
AIDDM3
We develop new strategies based on AI.
AIDDM4
We need AI for effective decision-making.
Environmental dynamism (ED) adapted from Dubey et al. (2020, 2021).
ED1
We can change the efficiency of our operations in response to demand.
ED2
Our marketing strategy is changing rapidly during the crisis.
ED3
The supply and demand side is very unpredictable during a crisis.
ED4
We are adopting artificial intelligence technologies to improve entrepreneurial performance in response to the crisis.
Artificial intelligence basic (AIB) adapted from Mikalef and Gupta (2021) and Belhadi et al. (2021).
AIR1
We have the hardware equipment (computers, etc.) to apply AI.
AIR2
We have the technical resources to apply AI.
AIR3
We have the software to apply AI (AI software, etc.).
AIR4
We have access to the data needed to run AI.
AIR5
We have arranged sufficient funding for AI projects.
Artificial intelligence skills (AIS) adapted from Zhang et al. (2020) and Mikalef and Gupta (2021).
AIS1
We understand the range of applications of AI.
AIS2
We can develop plans for the use of AI.
AIS3
We have the skills to apply AI.
AIS4
We have access to training in the use of AI.
AIS5
We can use AI technologies.
Artificial intelligence proclivity (AIP) adapted from Belhadi et al. (2021) and Mikalef and Gupta (2021).
AIP1
We have a recognition of the importance of innovation.
AIP2
We have a strategy for developing innovation efforts.
AIP3
We can implement innovation programs.
AIP4
We will introduce new products or technologies to improve business performance.
AIP5
We will take aggressive action to capitalize on growth opportunities.
Innovative culture (IC) adapted from Khattak et al. (2021).
IC1
Our flexible organizational structure helps integrate different perspectives.
IC2
We take risks by constantly trying new ways of doing things.
IC3
Our culture encourages innovation.
Open in a separate window
The Second-Order Formative Construct of AIC
When a latent variable is not directly associated with a measured variable, but is initially associated with a lower-order latent variable, and the lower-order latent variable is then associated with a measured variable, depending on the level of association, second-order variables, third-order variables, etc. can be formed (Hair et al., 2022). The association between variables can be reflective or formative (Becker et al., 2012). For second-order variables, four types exist: reflective-reflective, reflective-formative, formative-reflective, and formative-formative (Sarstedt et al., 2019). Artificial intelligence requires the application of many complementary resources to enhance business performance (Ghasemaghaei, 2021; Mikalef et al., 2021). Combining the current research outcomes, we constructed the AIC as a formative second-order latent variable consisting of three first-order variables (Table 2): basic, proclivity, and skills (Ashaari et al., 2021; Chatterjee et al., 2021a; Mikalef and Gupta, 2021).
Table 2
The formative construct of AIC.
Second-order
Type
First-order
Type
Artificial intelligence capability
Formative
Basic
(Tangible Resources)
Formative
Proclivity
(Intangible Resources)
Formative
Skills
(Human Resources)
Formative
Open in a separate window
Data Collection
We operated a cross-sectional survey to experiment with the theoretical model (Akram et al., 2020, 2021; Chen and Siau, 2020; Dubey et al., 2020; Haftor et al., 2021; Łapińska et al., 2021), and the online questionnaire was designed utilizing WJX.cn (an online questionnaire tool; Payne et al., 2021; Yao et al., 2021). This study conducted a targeted target audience survey through e-commerce associations, WeChat communities of e-commerce entrepreneurs (Chaudhuri et al., 2021). We received 441 responses within a 4-week data collection period. Respondents were informed of the purpose of the survey, data usage, and information protection at the time of invitation, and they also had the right to withdraw from the survey at any time (Bag et al., 2020a; Chatterjee et al., 2021b). We set up not to allow submission of questionnaires with omissions; thus, there were no incomplete questionnaires in this survey (Chen and Siau, 2020; Bag et al., 2021b). After data cleaning (Bag et al., 2021c; Chatterjee et al., 2021a; Zhang et al., 2021), we obtained 394 valid questionnaires with a validity rate of 89.3%. Table 3 presents the characteristics of the interviewed organizations.
Table 3
Characteristics of the sample.
Characteristics
Number (n = 394)
%
Firm age
 <1
67
17.0
 1–3
142
36.0
 4–6
131
33.2
 >6
54
13.7
No. of employees
 1–5
110
27.9
 5–10
141
35.8
 >10
143
36.3
Industry type
Online retail outlets

394

100
Open in a separate window
Data Analysis
Structural equation modeling (SEM) has the advantage of examining the interrelationships among multiple independent variables and one or more dependent variables (Belhadi et al., 2021; Łapińska et al., 2021). As an exploratory study, the most appropriate approach for this paper would be the PLS-SEM (Chatterjee et al., 2021b). The proposed research model contains higher-order variables (Ashaari et al., 2021) testing by the PLS-SEM (Bag et al., 2021a). In addition, PLS-SEM techniques in business performance (Chatterjee et al., 2021a; Chaudhuri et al., 2021; Pinheiro et al., 2021; Shao et al., 2021) and artificial intelligence research (Khalid, 2020; Bag et al., 2021b; Mikalef and Gupta, 2021; Rana et al., 2021) have been involved for a long time. Ultimately, we determined to use the PLS-SEM analysis in Smart PLS 3 software to test the hypotheses and theoretical models (Chen and Siau, 2020; Hair et al., 2022).
Common Method Bias
This study endeavors to reduce the impact of common method bias (CMB). At the time of the questionnaire, it was communicated that the study was intended for academic use and that the entire process would not involve respondents’ private information (Chatterjee et al., 2021b). After completion of the survey, we conducted Harman’s one-factor test with the help of SPSS 25 (Bag et al., 2020a; Nasiri et al., 2021), and comparing variance inflation factor (VIF) values (Bag et al., 2021b) to test for the presence of CMB. The results of Harman’s one-factor test indicated that the first factor explained 29.9% of the variance (<50%), and it can be inferred that CMB does not have a serious impact on the study results (Nasiri et al., 2021). Furthermore, the VIF of potential variables in the study model was below a threshold value of 5. These findings support that the CMB does not seriously impact the study results (Haftor et al., 2021; Hair et al., 2022).
Non-response Bias
Since we used a questionnaire for data collection, it was essential to test non-response bias (NRB; Chaudhuri et al., 2021). The presence of NRB was examined by comparing the data collected from the survey (top 25% of respondents and last 25% of respondents; Rahman et al., 2021). Our t-test results between early and late respondents using SPSS 25 showed no statistically significant difference between these two groups (p > 0.05; Bag et al., 2021a). Thus, we believe that NRB will not affect the results of our further analysis (Yao et al., 2021).
Go to:
Results
Partial least squares structural equation modeling assessment involves two key components: measurement model assessment and structural model assessment (Haftor et al., 2021; Hair et al., 2022). Measurement models examine convergent and discriminant validity, while structural models investigate the relationship among constructs (Ashaari et al., 2021). We used SmartPLS 3 software to analyze our data (Pinheiro et al., 2021).
Measurement Model
Table 4 shows the reliability and validity of the constructs. It shows that the Cronbach alpha is higher than 0.7 for all the constructs, the composite reliability (CR) is also higher than 0.7, the Rho_A values are not less than 0.7, and the factor loadings are higher than 0.7 for all the items. In addition, the average variance extracted (AVE) was also above 0.5, which confirms the convergent validity (Wang et al., 2021; Hair et al., 2022). The scales developed in this study were adapted from existing literature. The underwent multiple rounds of revision and pre-research, resulting in good content validity of the model in this study (Wang et al., 2021; Hair et al., 2022). Discriminant validity was determined using AVE square root, heterotrait-monotrait ratio (HTMT), and cross-loading (Wang et al., 2021; Hair et al., 2022). We found higher correlations between potential constructs than the square root of AVE for each construct. The loadings for each metric were higher than the respective cross-loadings, indicating sufficient discriminant validity of the measurement model (Table 5). The HTMT values for all constructs were less than the critical value of 0.9 (Table 6). Therefore, we determined that the measurement model had sufficient discriminant validity (Wang et al., 2021; Hair et al., 2022). In conclusion, we confirmed that the measurement model has sufficient reliability and validity for the next structural model analysis step.
Table 4
Reliability, convergent validity, and discriminant validity.
Construct
AIP
AIB
AIS
AIDDM
AIM
ED
FC
FP
IC
AIP
n/a








AIB
0.676
n/a







AIS
0.693
0.656
n/a






AIDDM
0.341
0.380
0.422
0.826





AIM
0.453
0.451
0.466
0.522
0.866




ED
0.204
0.115
0.144
0.293
0.218
0.843



FC
0.415
0.426
0.489
0.555
0.332
0.104
0.870


FP
0.344
0.357
0.387
0.650
0.421
0.249
0.532
0.842

IC
0.083
0.114
0.153
0.332
0.175
0.166
0.241
0.386
0.856
Cronbach’s Alpha
n/a
n/a
n/a
0.845
0.833
0.865
0.839
0.863
0.819
Rho_A
n/a
n/a
n/a
0.846
0.834
0.869
0.844
0.865
0.822
CR
n/a
n/a
n/a
0.896
0.900
0.908
0.903
0.907
0.892
AVE
n/a
n/a
n/a
0.683
0.749
0.711
0.757
0.710
0.733
Open in a separate window
Square root of average variance extracted (AVE) in diagonals.
Table 5
Factor loadings and cross loadings.
Items
AIDDM
AIM
AIP
AIB
AIS
ED
FC
FP
IC
AIDDM1
0.802
0.379
0.273
0.311
0.351
0.205
0.431
0.521
0.296
AIDDM2
0.839
0.482
0.283
0.324
0.359
0.271
0.501
0.557
0.287
AIDDM3
0.842
0.412
0.271
0.322
0.362
0.254
0.446
0.522
0.257
AIDDM4
0.820
0.445
0.300
0.297
0.323
0.234
0.453
0.546
0.257
AIM1
0.453
0.873
0.384
0.403
0.408
0.156
0.280
0.368
0.147
AIM2
0.466
0.863
0.419
0.385
0.407
0.211
0.289
0.368
0.165
AIM3
0.434
0.861
0.372
0.382
0.396
0.198
0.294
0.358
0.141
AIP1
0.269
0.362
0.806
0.563
0.569
0.152
0.344
0.276
0.069
AIP2
0.313
0.343
0.755
0.523
0.522
0.140
0.364
0.287
0.071
AIP3
0.242
0.319
0.799
0.542
0.521
0.171
0.306
0.245
0.067
AIP4
0.252
0.353
0.820
0.507
0.577
0.149
0.296
0.266
0.073
AIP5
0.291
0.432
0.820
0.569
0.583
0.203
0.353
0.300
0.050
AIB1
0.248
0.386
0.548
0.805
0.545
0.112
0.305
0.211
0.065
AIB2
0.327
0.357
0.548
0.865
0.552
0.099
0.338
0.296
0.102
AIB3
0.320
0.328
0.575
0.791
0.505
0.102
0.364
0.331
0.085
AIB4
0.313
0.339
0.506
0.780
0.509
0.026
0.367
0.315
0.153
AIB5
0.335
0.418
0.565
0.817
0.550
0.125
0.357
0.297
0.059
AIS1
0.356
0.326
0.539
0.547
0.826
0.154
0.396
0.312
0.124
AIS2
0.370
0.387
0.601
0.557
0.829
0.121
0.403
0.344
0.123
AIS3
0.356
0.392
0.585
0.551
0.816
0.097
0.416
0.314
0.086
AIS4
0.311
0.374
0.556
0.502
0.832
0.128
0.367
0.298
0.150
AIS5
0.355
0.449
0.589
0.560
0.842
0.098
0.441
0.335
0.152
ED1
0.261
0.187
0.207
0.098
0.144
0.870
0.075
0.240
0.100
ED2
0.254
0.224
0.131
0.108
0.129
0.843
0.093
0.215
0.172
ED3
0.247
0.155
0.193
0.111
0.139
0.822
0.116
0.194
0.114
ED4
0.222
0.163
0.154
0.069
0.066
0.838
0.067
0.187
0.181
FC1
0.433
0.276
0.350
0.362
0.408
0.091
0.857
0.445
0.185
FC2
0.490
0.294
0.415
0.406
0.488
0.072
0.907
0.462
0.207
FC3
0.522
0.296
0.316
0.341
0.375
0.110
0.845
0.483
0.236
FP1
0.519
0.351
0.296
0.313
0.326
0.206
0.438
0.823
0.329
FP2
0.561
0.353
0.286
0.286
0.316
0.195
0.462
0.878
0.315
FP3
0.560
0.336
0.254
0.297
0.310
0.201
0.438
0.863
0.358
FP4
0.548
0.381
0.323
0.308
0.354
0.240
0.454
0.802
0.299
IC1
0.275
0.127
0.068
0.078
0.128
0.099
0.207
0.307
0.850
IC2
0.282
0.148
0.067
0.132
0.120
0.131
0.177
0.317
0.850
IC3
0.295
0.172
0.076
0.083
0.144
0.190
0.232
0.365
0.869
Open in a separate window
Table 6
Assessment of discriminant validity using heterotrait-monotrait ratio (HTMT).
Construct
AIDDM
AIM
ED
FC
FP
IC
AIDDM






AIM
0.619





ED
0.340
0.255




FC
0.657
0.397
0.123



FP
0.760
0.497
0.288
0.626


IC
0.399
0.210
0.197
0.289
0.458

Open in a separate window
Formative Constructs Validation
As suggested by Hair et al. (2022), we tested the formative structure of AI ability with the help of SmartPLS 3. Table 5 displays that the significance among AIC and all three first-order constructs is less than 0.001, indicating that AIC is well-constructed second-order models (Figure 3; Becker et al., 2012; Hair et al., 2022). The results suggest that AIC are higher-order models constructed from three first-order constructs: basic, proclivity, and skills (Table 7).


Figure 3
Partial least squares structural equation modeling (PLS-SEM) results.
Table 7
Formative constructs validation.
Constructs
Measures
Weighting
t values
Significance
VIF
Basic
AIB1
0.240
3.143
p < 0.01
2.113
AIB2
0.106
1.301
NS
2.119
AIB3
0.294
4.487
p < 0.001
2.026
AIB4
0.239
3.279
p < 0.01
2.166
AIB5
0.355
4.975
p < 0.001
2.221
Skills
AIS1
0.165
2.334
p < 0.05
2.113
AIS2
0.310
4.853
p < 0.001
2.119
AIS3
0.310
4.848
p < 0.001
2.026
AIS4
0.088
1.242
NS
2.166
AIS5
0.323
4.362
p < 0.001
2.221
Proclivity
AIP1
0.268
4.05
p < 0.001
1.885
AIP2
0.304
4.857
p < 0.001
1.631
AIP3
0.168
2.679
p < 0.01
1.857
AIP4
0.136
1.866
NS
2.000
AIP5
0.368
5.318
p < 0.001
1.999
Constructs
Measures
Path coefficient
t values
Significance
VIF
Artificial intelligence capability
Basic
0.874
58.292
p < 0.001
1.000
Skills
0.900
71.305
p < 0.001
1.000
Proclivity
0.880
62.389
p < 0.001
1.000
Open in a separate window
Structural Model
After examining the measurement model’s reliability, validity, and formative structure, this study will analyze the data to examine the relationship among the variables (Ashaari et al., 2021; Hair et al., 2022). Figure 3 shows the results after bootstrapping. We found a positive effect of AIDDM on FP (β = 0.569; p < 0.001), indicating that AIDDM increased firm performance. We also found a positive effect of both FC (β = 0.398; p < 0.001) and AIM (β = 0.340; p < 0.001) on AIDDM. There was a positive effect of AIC on both FC (β = 0.515; p < 0.001) and AIM (β = 0.528; p < 0.001). For the control variables, firm age negative affects firm performance (β = −0.121; p < 0.01) and number of employees does not affect firm performance (β = 0.016; p > 0.05). Therefore, hypotheses H1, H2, H3, H5, H6, H7, H8, and H10 are supported, and hypotheses H4 and H9 are not supported. The R2 value of firm performance in the model is 0.507, and this result indicates the good explanatory power of the model (Hair et al., 2022).
The R2 value of firm performance in the model is 0.507, which suggests the model’s good explanatory power (Hair et al., 2022). The predictive relevance Q2 value in the model is 0.347 (>0), which verifies that the model has appropriate predictive relevance (Hair et al., 2022). The result of standardized root mean square residual (SRMR), an indicator of model fitness, was 0.079 (<0.08), implying that the model proposed in this study has a good fitness (Hair et al., 2022).
Moderating and Mediating Effect
We used bootstrapping in SmartPLS to examine the mediating and moderating effects (Akram et al., 2018b; Hair et al., 2022). The test results for mediating effects (Table 8) exhibited a mediating effect of FC and AIM on the relationship between AIC and AIDDM. Both AIM and FC indirectly affected FP through AIDDM. Four moderating effects were examined in this study (Figure 3 and Table 8). There was a positive moderating effect of IC on the relationship between AIDDM and FP (β = 0.203; p < 0.001), and IC also had a positive moderating effect on the relationship between FC and AIDDM (β = 0.156; p < 0.001). ED showed a positive moderating effect on the relationship between AIM and AIDDM (β = 0.139; p < 0.001). However, the moderating effect of ED between AIDDM and FP was insignificant (β = 0.016; p > 0.05).
Table 8
Results of meditation and moderation.
Effect
Relationships
Path coefficient
STDEV
t values
Results
Meditation
AIC → AIM → AIDDM
0.180
0.026
6.872***
Supported
AIC → FC → AIDDM
0.205
0.026
7.915***
Supported
AIC → AIDDM → FP
0.015
0.025
0.591
Not supported
AIM → AIDDM → FP
0.194
0.027
7.25***
Supported
FC → AIDDM → FP
0.227
0.030
7.478***
Supported
Moderation
ED × AIDDM → FP
0.018
0.038
0.475
Not supported
ED × AIM → AIDDM
0.139
0.025
5.492***
Supported
IC × AIDDM → FP
0.203
0.042
4.829***
Supported
IC × FC → AIDDM
0.156
0.030
5.127***
Supported
Open in a separate window
***p < 0.001.
Go to:
Discussion
Theoretical Implications
This paper investigates the performance of e-commerce enterprises and proposes a theoretical model incorporating RBV (Barney, 1991; Chatterjee et al., 2021b), which explores the construct of AIC in e-commerce enterprises, finds the effects of firm creativity, AIM, AIDDM on firm performance, and examined the moderating effects of an innovation culture and environmental dynamism. This study reveals the mechanisms that constitute AIC of e-commerce firms and ensures that the AIC of e-commerce firms affects firm performance through creativity, AIM, and AIDDM (Ashaari et al., 2021). This study extends RBV’s research findings on firm creativity, AIM, and AIDDM (Hossain et al., 2021; Rahman et al., 2021). Our research sheds light on the composition mechanism of AIC in e-commerce enterprises and its effect on corporate performance. The data analysis conducted that the AIC of e-commerce firms are second-order variables formed by three first-order variables: basic (tangible resources), proclivity (intangible resources), and skills (human resources; Mikalef and Gupta, 2021). Our proposed AIC for e-commerce firms is a second-order formative model suggesting AIC is constructed by the three complementary resources of basic, proclivity, and skill (Ghasemaghaei, 2021). This classification of the constitutive resources of AIC in e-commerce firms provides additional facilities ensuing investigation and management.
The study outcomes reveal the role of firm creativity, AIM, and driving decision-making in the relationship between AIC and firm performance. AIDDM significantly and positively affects firm performance, similar to conclusions of Ashaari et al. (2021). We found that AIC does not directly affect firm performance (Chen and Lin, 2021; Haftor et al., 2021) but indirectly affects AIDDM and firm performance through firm creativity and AIM. AIC, as firm capabilities requiring numerous resources need to demonstrate their business value through innovative measures and quality decisions (Shi et al., 2020). This study demonstrates the consequence of an innovation culture and environmental dynamism as moderating variables on the relationship between AIC and firm performance. Innovation culture has a positive moderating effect on the relationship between firm creativity and AIDDM (Zhang et al., 2021) and AIDDM and firm performance (Ashaari et al., 2021). Environmental dynamism positively affects the relationship between AIM and AIDDM (Belhadi et al., 2021). However, there is no moderating effect of environmental dynamism on the relationship between AIDDM and firm performance, suggesting difficulties in the external environment’s variability and unpredictability to profit from AIDDM. Thus, we should focus on the positive effects of an innovation culture and environmental dynamism as moderating variables in the research model.
Among the control variables, firm age negatively affects firm performance, and startups are more likely to desire to leverage new technologies and models to improve firm performance (Yao et al., 2021), while more established firms may be a more conservative view of new technologies. The number of firm employees does not affect firm performance, presenting no need to be concerned about the impact of firm employee size when considering the relationship between AI and firm performance.
Managerial Implications
Enterprises can cultivate firm AIC through three aspects: tangible resources (basic), intangible resources (proclivity), and human resources (skills). This study proves that e-commerce firms AIC is formed by three first-order variables: basic, proclivity, and skills, and the data analysis results indicate that AIC is a well-constructed second-order model. Firms need to make the real business value of AI technology to improve firm performance and cannot rely on either hardware devices or software, technical resources (Rahman et al., 2021), and data resources (Chaudhuri et al., 2021). Nevertheless, these complementary resources should be allowed to construct the superior competitiveness of the firm organically.
Use artificial intelligence to enhance firm creativity. Companies can adopt artificial intelligence technologies to perform repetitive tasks in business operations, release more human resources, and reduce costs (Mikalef and Gupta, 2021). In addition, businesses can also attempt to employ AI for innovative work, using deep mining of internal and external data to discover where the current needs of firm customers are going, thus giving them more time to optimize processes, products, and services. For example, AI technology can integrate solutions that consumers likely favor and record current browsing data, click data and sales data in time to predict the higher quality products and services to meet consumers’ needs.
Foster an internal culture of innovation and keep an eye on external environmental changes. This paper affirms the positive moderating role of an innovation culture and environmental dynamism present in the research model. E-commerce firms should cultivate a culture of innovation that incorporates the employees’ views at all levels within the firm and should also consider the opinions of external experts. Firms can also establish fault-tolerance mechanisms to allow new ideas and solutions, providing more opportunities to improve performance. Changes and unpredictability in the external environment can also affect business operations (Haftor et al., 2021). Firms can use AI technologies to observe changes in the external environment in real-time and recommend intervention strategies to give them insight into business opportunities in a highly competitive market.
Emphasis is on establishing AI to manage and drive decision-making to leverage the positive effects that AI can bring in decision-making. Establishing processes based on AIDDM is an important stage in leveraging AI to enhance business performance (Chatterjee et al., 2021a). Companies can genuinely appreciate the technological dividends of AI by making decisions in considerable areas, such as marketing, product development, and customer relationship management. The rapid development of digital technology also requires firms to establish AIM systems to monitor and update AI systems promptly.
Limitations and Future Research
This research contributes to current theoretical developments and AI practices, but it solely pays attention to e-commerce firms in China, and we can extend the established theoretical framework to more industries and other countries in the future. The firm performance is derived from the subjective evaluations of the respondents and does not cover the financial data of the sample firms; future studies could consider both qualitative and quantitative methods to explore more correlations and phenomena. This study used cross-sectional data at one point, and it did not consider longitudinal changes in AIC and firm performance. We could focus on other firm characteristics, such as risk-taking, R&D capability, market development capability, and productivity. Future studies will explore more organizational characteristics variables to improve the model presented in this model.
Go to:
Conclusion
We clarified and assessed the components of AIC critical for improving firm performance with AI. Therefore, we analyze the components of firms AIC relating to RBV. By reviewing the relevant literature, we proposed a research model of AIC and firm performance in e-commerce businesses, aiming to explain and predict the performance under AI application scenarios. Following a newly developed scale, we designed an online questionnaire and received 394 valid questionnaires. Further data analysis with the SmartPLS 3 utilizes the PLS-SEM analysis technique. The results illustrated that the model proposed in this study has sufficient explanatory power, predictive power, and fitness. We found that AIC as a second-order variable is formed by three first-order variables: basic, skills, and proclivity. AIC indirectly influences firm performance through firm creativity, AIM, and AIDDM. Corporate creativity, AIM, and AIDDM are significant mediating variables between AIC and firm performance. Innovation culture positively moderates the relationship between firm creativity and AIDDM and positively moderates the relationship between AIDDM and firm performance. Environmental dynamism positively moderates the relationship between AIM and AIDDM. Among the control variables, firm age negatively affects firm performance, and the number of firm employees does not affect firm performance. This study’s empirical findings help enterprises to improve firm performance and gain a competitive advantage with the help of AI, enrich the research on AI and firm performance, and contribute to theory and management practice.",https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9022026/
Artificial Intelligence to Enhance Corporate Governance,ResearchGate,"Amacieto Correia, Pedro Agua","ARTICLE provides a conceptual framework for how AI can enhance corporate governance, focusing on improving decision-making processes and accountability.",,"
Corporate governance (CG) has long been recognized
as a vital mechanism for ensuring the efficient
functioning and ethical conduct of corporations
(Tricker, 2015). CG involves the structures, processes,
and rules that control and direct corporations, with
the aim of balancing the interests of a company‘s
broad stakeholders set, such as shareholders,
management, customers, suppliers, financiers,
government, and the community (Freeman, 1984).
Despite the acknowledged importance of CG,
numerous cases of corporate mismanagement and
scandal, such as Enron, WorldCom, and Volkswagen,
to name just a few, underscore the persistent
challenges and the need for further enhancement of
CG mechanisms (Park, 2008; Jensen, 2001).
The ongoing digital transformation presents
an opportunity to rethink and reinvent CG.
This transformative period is characterized by
the emergence of powerful technologies such as
artificial intelligence (AI), which have the potential to
In this preliminary study, we explore the novel intersection of
corporate governance (CG) and artificial intelligence (AI),
addressing the crucial question: How can AI be leveraged to
enhance ethical and transparent decision-making within
the corporate environment? Drawing from current studies on
organizational governance, AI ethics, and data science, our
research raises the curtain on the potential of AI in augmenting
traditional governance mechanisms, while also scrutinizing
the ethical quandaries and challenges it may pose. We propose
a novel conceptual framework, rooted in the principles of
separation of ownership and control, and data ethics, to be
underpinned and validated, in the future, through an empirical
study. Given the current inception stage of the study, we expect
the results will illustrate a significant positive impact of AI on
CG effectiveness, particularly in enhancing transparency and
fostering ethical decision-making. We also propose future
studies to be done as a mix of econometric and machine
learning methods to empirically test the framework with
datasets gathered over a period of years.
Keywords: Artificial Intelligence, Corporate Governance, Data
Science, Ethical Decision-Making, Organizational Ethics,
Transparency

disrupt traditional business models and introduce
novel approaches to management and decision-making
(Brynjolfsson & McAfee, 2014). AI technologies, with
their advanced capabilities for pattern recognition,
predictive analysis, and decision automation,
provide a vast potential for improving CG. Yet,
despite the transformative potential of AI, its
application in CG remains under-explored, and its
implications under-theorized (Mikalef et al., 2019).
The application of AI in CG is challenging for
several reasons. First, AI can enhance decision-
making by providing valuable insights from large
volumes of data, and automate certain governance
processes, thereby reducing human error and bias,
and improving efficiency (Myatt, 2007). This could
lead to better alignment of corporate actions with
shareholder interests, and more effective compliance
with regulations, thereby enhancing the overall
governance of corporations.
Second, AI has the potential to foster greater
transparency in CG. Through its predictive and
analytical capabilities, AI can help uncover hidden
patterns and trends, providing valuable insights
which can be used to improve the transparency of
decision-making processes. This could enhance
the accountability of corporations to their
stakeholders, a key aspect of CG.
However, the application of AI in CG is not
without challenges. One of the main challenges
pertains to the ethical implications of AI. These
include issues related to privacy, fairness, and
accountability, among others (Russell et al., 2015).
For example, AI systems, with their capacity for data
analysis and prediction, could potentially infringe on
the privacy of individuals. Moreover, the decisions
made by AI systems may not always be fair or
unbiased, as they could be influenced by the biases
in the data used to train these systems. Another
challenge is the ―black box‖ problem associated with
AI. This refers to the difficulty of understanding how
AI systems make decisions, due to the complexity of
their algorithms and the large volumes of data they
process (Castelvecchi, 2016). This lack of transparency
could undermine trust in AI systems and hinder
their acceptance by stakeholders.
To address these challenges, there is a need for
a comprehensive framework for leveraging AI in CG,
which balances the potential benefits and ethical
and transparency issues. Such a framework would
guide how to integrate AI into CG mechanisms, and
how to address the ethical and transparency issues
that may arise.
To this end, our research aims to develop
a comprehensive AI-based CG framework. We will
explore how AI can be leveraged to enhance CG,
identify the potential ethical and transparency
issues that could arise, and propose strategies for
mitigating these issues. Our research is guided by
two key research questions:
RQ1: How can AI be leveraged to enhance CG?
RQ2: What are the potential ethical and
transparency issues that could arise from the use of
AI in CG and how can they be mitigated?
The benefits of AI in CG extend beyond
the boardroom. By improving decision-making and
efficiency, AI can enhance the competitiveness of
corporations, contributing to economic growth and
job creation. By fostering greater transparency, AI
can enhance the accountability of corporations to
their stakeholders, contributing to societal trust in
corporations. And by addressing the ethical
implications of AI, the research can contribute to
the responsible use of AI, ensuring that the benefits
of AI are realized in a way that is ethical and
sustainable.
The rest of this paper is structured as follows.
Section 2 provides a background on CG and
the potential role of AI in enhancing it. Section 3
presents the proposed AI-based CG framework and
discusses how it can help address the challenges
and issues in CG. Finally, Section 4 concludes
the paper with a summary of the findings and
implications for future research.
2. THEORETICAL BACKGROUND
Agency theory, originally proposed by Jensen and
Meckling (1976), has been the dominant theoretical
framework in CG research for decades. It postulates
that in modern corporations, there exists
a separation of ownership and control, where
the owners (principals) delegate the management of
the corporation to the managers (agents). This
separation gives rise to agency problems due to
the divergence of interests between the principals
and agents.
Principals, typically shareholders, are interested
in maximizing the value of their investments, while
agents, the managers, may have different objectives,
such as personal wealth maximization, career
security, or other non-economic interests
(Eisenhardt, 1989). This divergence of interests often
leads to issues such as moral hazard, where agents
take on excessive risk for personal gain at
the expense of the principals, and adverse selection,
where agents possess information that principals do
not, enabling the agents to act in ways that may not
be in the best interests of the principals (Fama,
1980; Eisenhardt, 1989).
To mitigate these agency problems, various CG
mechanisms have been put in place, including board
supervision, performance-based compensation,
shareholder voting rights, and external audits
(Fama, 1980; Jensen & Meckling, 1976). However,
these traditional governance mechanisms have their
limitations. One of the main limitations is that they
rely heavily on human judgment and decision-
making, which are prone to bias, subjectivity, and
error (Tversky & Kahneman, 1974). For instance,
board members may have cognitive biases that
affect their ability to effectively supervise
management, and auditors may have confirmation
biases that affect their ability to accurately assess
financial reports (Healy & Palepu, 2001; Bazerman
et al., 2002; Água & Correia, 2021, 2022). Another
limitation is the lack of transparency and
accountability, which can facilitate unethical
behaviour and misconduct by managers
(Jensen, 2001).
The effectiveness of CG mechanisms has been
questioned due to the abovementioned high-profile
corporate scandals and failures (Agle et al., 2008).
These cases demonstrate the limitations of
traditional CG mechanisms, which are often
characterized by human bias, subjectivity, lack of
transparency, and inadequate oversight (Jensen,
2001). Therefore, there is a pressing need for
innovative approaches to enhance CG.Corporate Board: Role, Duties and Composition / Volume 19, Issue 1, 2023
31
AI techniques have the potential to help
overcome these limitations. AI refers to the simulation
of human intelligence processes by machines,
especially computer systems (Russell & Norvig,
2016). AI has gained significant attention in business
research and practice due to its potential to
transform various aspects of business operations,
such as decision-making, process automation,
customer service, and strategy formulation
(Brynjolfsson & McAfee, 2014). AI‘s potential is
particularly relevant to CG.
AI can enhance decision-making by providing
valuable insights from large volumes of data,
reducing human error and bias (Davenport &
Ronanki, 2018). The ability to analyse and learn from
data allows AI to generate valuable insights that can
inform decision-making, automate complex
processes, and forecast future trends with a high
degree of accuracy (Goodfellow et al., 2016). AI can
enhance strategic decision-making by providing
predictive analytics based on comprehensive data
analysis, enabling corporations to anticipate market
trends, customer behaviour, and competitive moves
(Siegel, 2016). Moreover, AI can learn from the data
and improve its decision-making capabilities over
time through machine learning algorithms
(Goodfellow et al., 2016).
Furthermore, AI can automate routine tasks,
freeing up human resources for more strategic and
creative tasks, thus increasing operational efficiency
(Chui et al., 2018). AI can also automate certain
governance processes, such as financial auditing and
compliance monitoring, improving efficiency and
accuracy (Yoo et al., 2018). Transparency and
accountability can be enhanced by AI if an audit trail
of decision-making processes and outcomes is
provided, in an attempt to address the ―black box‖
problem through explainable AI techniques (Russell
et al., 2015; Castelvecchi, 2016). AI can also help
detect and prevent unethical behaviour and
misconduct by analysing patterns and anomalies in
corporate data (Dhar, 2016).
Despite the potential of AI to enhance CG, there
is a paucity of research that systematically
investigates this potential. A few studies have
started to explore this area, but their focus has been
on specific aspects of CG. For instance, Mikalef et al.
(2019) examined how AI can be used to enhance
board-level decision-making, while Yoo et al. (2018)
studied how AI could be used to automate certain
CG processes. However, these studies do not provide
a comprehensive framework for leveraging AI to
enhance CG, nor do they thoroughly address
the ethical and transparency issues that could arise
from such usage.
The application of AI in decision-making and
process automation also raises significant ethical
and transparency issues. For instance, the decisions
made by AI systems may be biased due to biased
training data or algorithms, leading to unfair
outcomes (Crawford, 2016). Moreover, the decision-
making process of AI systems can be opaque,
making it difficult for humans to understand
and explain the decisions, which can
undermine accountability and trust in AI systems
(Castelvecchi, 2016).
The ethical and transparency issues associated
with AI have been increasingly recognized as critical
challenges that need to be addressed (Russell
et al., 2015). These issues include privacy, fairness,
transparency, and accountability, among others.
Privacy issues arise from the extensive data
collection and processing involved in AI systems,
which can potentially infringe on individuals‘ privacy
rights (Zuboff, 2019). Fairness issues arise from
the potential biases in AI systems, which can lead to
unfair outcomes for certain groups of individuals
(Crawford, 2016). Transparency issues arise from
the opaque decision-making process of AI systems,
which can undermine accountability and trust in AI
systems (Castelvecchi, 2016). Accountability issues
arise from the difficulty of attributing responsibility
for the decisions made by AI systems, especially
when these decisions have significant societal
implications (Floridi & Taddeo, 2016). Addressing
these ethical and transparency issues is crucial to
ensure the responsible use of AI in business
and society.
In summary, agency theory provides a valuable
theoretical foundation for understanding the role of
AI in CG. By addressing the limitations of traditional
governance mechanisms and mitigating agency
problems, AI can significantly enhance CG. However,
the successful integration of AI into CG requires
a thoughtful and balanced approach that takes into
account both the potential benefits and the ethical
and transparency considerations. While AI holds
significant promise for enhancing CG and mitigating
agency problems, it also needs a comprehensive
framework that balances the potential benefits and
challenges and ensures ethical and transparent
AI use.
3. STUDY FRAMEWORK
The background section reveals a gap in the existing
research on the intersection of AI and CG. While
some studies have explored specific aspects of this
intersection, there is a lack of a comprehensive
framework that integrates AI into CG and addresses
the associated ethical and transparency issues.
Developing such a framework is important for
several reasons. First, it can provide a systematic
approach to leveraging AI in enhancing CG, which
can lead to more efficient and effective governance
processes, better decision-making, and improved
corporate performance. Second, it can guide how to
address the ethical and transparency issues
associated with the use of AI in CG, ensuring that
the benefits of AI are realized in a responsible and
sustainable manner.
Moreover, the integration of AI into CG could
potentially extend the agency theory by introducing
AI as a new mechanism to mitigate agency problems.
AI could enhance the monitoring and control
mechanisms, reduce human bias and error, and
provide more accurate and timely information for
decision-making, thus mitigating the divergence of
interests between principals and agents (Brynjolfsson
& McAfee, 2014; Davenport & Ronanki, 2018).
To develop the proposed AI-based CG framework,
it is necessary to conduct empirical research that
explores how AI can be leveraged to enhance CG,
and how the ethical and transparency issues can be
mitigated. Such research would involve qualitative
interviews to gain insights from corporate board
members, senior management, and AI experts;
a quantitative survey to gather data from a large andCorporate Board: Role, Duties and Composition / Volume 19, Issue 1, 2023
32
more diverse sample; and secondary data analysis to
examine the impact of AI on CG performance.
To ensure the comprehensiveness of
the proposed framework, it should address several
key aspects of CG, such as board structure and
processes, executive compensation, shareholder
rights, and corporate accountability and transparency.
The framework should also guide how to integrate
AI into these aspects in a way that enhances CG and
addresses ethical and transparency issues. Hence,
the proposed framework should take into account
the specific characteristics and capabilities of AI,
such as machine learning, predictive analytics, and
decision automation. It should guide how to leverage
these capabilities to enhance CG, and how to
address the associated challenges, such as the ―black
box‖ problem and potential biases in AI systems.
Furthermore, the framework should guide how
to ensure the ethical use of AI in CG. This involves
addressing the privacy, fairness, transparency,
and accountability issues associated with AI.
The framework should provide specific guidelines
and mechanisms for addressing these issues, such
as data privacy policies, fairness audits,
explainability tools, and accountability structures.
Finally, the framework should be flexible and
adaptable to different types of corporations and
different contexts. This requires considering
the variations in CG practices across different
industries, countries, and cultures, and the dynamic
nature of AI technologies.
The AI-based CG framework proposed in this
work (Figure 1) aims to integrate AI into the key
aspects of CG in a way that enhances CG and
addresses ethical and transparency issues. The
framework comprises the following five key aspects:
1) Board structure and processes: The framework
proposes the use of AI in enhancing board decision-
making and oversight. AI could be used to analyse
large volumes of data relevant to the board‘s
decision-making, thereby providing valuable insights
for the board (Brynjolfsson & McAfee, 2014).
AI could also be used to automate certain board
processes, such as compliance monitoring, thereby
improving efficiency and accuracy (Yoo et al., 2018).
2) Executive compensation: AI could play a role
in designing and implementing performance-based
compensation schemes. Machine learning algorithms
could be used to predict the impact of different
compensation structures on executive behaviour and
corporate performance, thereby enabling more
informed and effective compensation decisions
(Goodfellow et al., 2016).
3) Shareholder rights: AI could enhance
shareholder engagement and voting. AI could be
used to analyse shareholder feedback and voting
patterns, thereby providing insights into shareholder
sentiment and preferences (Bollen et al., 2011).
AI could also be used to facilitate electronic voting,
thereby making it easier and more convenient
for shareholders to exercise their voting rights
(Lu et al., 2020).
4) Corporate accountability and Transparency:
AI could enhance corporate accountability by
providing an audit trail of decision-making processes
and outcomes.
5) Corporate transparency: AI could enhance
corporate transparency by detecting and preventing
unethical behaviour and misconduct. AI could also
be used to generate real-time, interactive, and
user-friendly corporate reports, thereby enhancing
corporate disclosure (Dhar, 2016).
The proposed AI-based CG framework is
flexible and adaptable. It can be tailored to
the specific characteristics and needs of different
types of corporations and different contexts.
The framework also recognizes the dynamic nature
of AI technologies and the need for continuous
learning and adaptation. It encourages corporations
to regularly update their AI strategies and practices
in response to technological advancements and
changes in the business and regulatory environments.
Figure 1. AI-based CG framework
Board structure
and processes
Executive
compensation
Shareholder
rights
Corporate
accountability
Corporate
transparency
and transparency
CG key aspects
Corporate
governance
Figure 1 depicts the overall representation of
the AI-based CG framework and its components:
 AI in CG: The central focus of the framework
is given by the intersection of AI and CG (central
circle).
 Key aspects of CG: Around the central circle
are surrounding circles, each representing a key
aspect of CG that can be enhanced by AI. The key
aspects of CG, previously described, are board
structure and processes, executive compensation,
shareholder rights, corporate accountability, and
corporate transparency.
 Integration of AI: Each one of the key aspects
of CG can be enhanced via AI integration
(represented by arrows connecting surrounding
circles to the central circle).
 Dynamic nature of the framework: This
behaviour is suggested by the impact originated by
each key aspect of CG on the others key aspects, due
to the AI effect (arrows between the key aspects
of CG).
 Ethical and transparency factors: It guides
how to ensure the ethical use of AI in CG. It involves
implementing data privacy policies, conducting
fairness audits, using explainability tools, and
establishing accountability structures (Russell et al.,
2015) (outer layer surrounding the entire figure).
To support the proposed AI-based CG
framework, correlation studies are planned as this
research develops. These studies will measure how
AI technologies can improve CG by examining
the relationship between variables. We will search
for associations between the application of AI and
the enhancement of CG. With the study, it is
expected to gather evidence regarding the following
claims:
 AI‘s advanced capabilities in pattern
recognition, predictive analysis, and decision
automation have potential implications for
the structures, processes, and rules that control and
direct corporations (Brynjolfsson & McAfee, 2014).
 By leveraging AI‘s capability to process and
analyze large volumes of data, corporations can gain
valuable insights that enhance decision-making
processes, automate certain governance processes,
and reduce human error and bias, thereby improving
efficiency (Davenport & Ronanki, 2018). Consequently,
this could lead to a better alignment of corporate
actions with shareholder interests and more
effective compliance with regulations, thereby
enhancing the overall governance of corporations
(Hilb, 2020).
 The integration of AI into CG, if done in
an ethical and transparent manner, can significantly
improve an organization‘s efficacy and performance.
By uncovering hidden patterns and trends, AI could
provide valuable insights that can be used to improve
the transparency of decision-making processes
(Chui et al., 2016). This increased transparency could
enhance the accountability of corporations to their
stakeholders (Siau & Yang, 2017), a crucial aspect of
effective CG (Tricker, 2015).
 The decisions made by AI systems may not
always be fair or unbiased, as they could be
influenced by the biases in the data used to train
these systems (Barocas & Selbst, 2016).
 Due to its capacity for extensive data analysis
and prediction, AI could potentially infringe on
the privacy of individuals (Russell et al., 2015).
 The complexity of AI algorithms (the so-called
―black box‖ problem) and the large volumes of data
they process can make it difficult to understand how
AI systems make decisions (Castelvecchi, 2016). This
lack of transparency could undermine trust in AI
systems and hinder their acceptance by stakeholders
(Burrell, 2016).
To test hypotheses about these claimed
relationships, the proposed correlational studies to
be conducted should use a sample of several
corporations from various industries. The goal is to
gather vital information regarding the level of AI
integration within a company‘s governance
procedures. By leveraging well-established metrics
like board effectiveness, regulatory compliance,
stakeholder satisfaction, and shareholder value, we
aim to appraise their CG proficiency and
performance. We will also account for potential
confounding factors such as industry, company size,
and geographical location. Not only will this study‘s
findings add to the theoretical understanding of how
AI affects CG, but they will also provide valuable
practical insights for companies looking to capitalize
on AI to bolster their governance efforts.
A step further in the study to explain how or
why certain relationships exist is the use of
the mediation analysis (Baron & Kenny, 1986), is
a statistical technique used to investigate
the process by which a variable (the mediator)
influences the relationship between two other
variables (the independent and dependent variables).
The mediation analysis involves testing whether
the indirect effect of the independent variable
on the dependent variable through the mediator is
significant (Hayes, 2013). In the context of our
proposed AI and CG framework, AI‘s integration into
CG can be conceptualized as the independent
variable; corporate performance as the dependent
variable; and various aspects of the framework
(such as board structure and process, executive
compensation, shareholder rights, corporate
accountability, and corporate transparency) as
potential mediators. A first model to illustrate
the mediation analysis could include (Preacher &
Hayes, 2004):
 Independent variable (X): AI‘s integration
into CG.
 Mediators (M): Board structure and process,
executive compensation, shareholder rights,
corporate accountability, and corporate transparency.
 Dependent variable (Y): Corporate performance.
The mediation analysis would involve
the following steps:
 Regressing the mediators (M) on the
independent variable (X). This establishes whether
AI‘s integration into CG significantly influences
the various aspects of the framework.
 Regressing the dependent variable (Y) on
the independent variable (X). This tests the total
effect of AI‘s integration into CG on corporate
performance.
 Regressing the dependent variable (Y) on both
the independent variable (X) and the mediators (M).
This tests whether the mediators significantly
influence corporate performance when controlling
for AI‘s integration into CG.
The indirect effect of AI‘s integration into CG
on corporate performance through the mediators
would be considered significant if the confidenceCorporate Board: Role, Duties and Composition / Volume 19, Issue 1, 2023
34
intervals do not contain zero. This would suggest
that AI‘s impact on corporate performance is
mediated by its influence on the various aspects of
the framework. The actual mediation analysis may
involve more complex statistical techniques
depending on the nature of the data and the research
questions. Moreover, it will be important to interpret
the results of mediation analysis in light of
the theoretical underpinnings of the proposed
framework and the literature on AI and CG.
4. CONCLUSION
The aim of this research was the recognition
of the transformative potential of AI in CG and
the concomitant need for an ethical and transparent
approach. Our study provides a nuanced
understanding of how AI can enhance CG, as well as
the ethical and transparency issues that need to be
addressed.
Drawing on the findings, we proposed an AI-
based CG framework. This framework provides
a balanced approach to leveraging AI in CG,
integrating AI into decision-making and governance
processes, while also addressing the ethical and
transparency issues. This framework extends
the agency theory, suggesting a new approach to
mitigating agency problems in the era of AI.
Future research should delve deeper into
the ethical and transparency issues. This would
involve testing the proposed framework, conducting
larger and diverse interviews, using detailed and
comprehensive data, and developing specific ethical
and transparency guidelines and mechanisms.
In the age of AI, CG mechanisms need to evolve
to leverage the benefits of AI, while also addressing
the ethical and transparency challenges. Our
research provides a foundation for this evolution,
contributing to the enhancement of CG in the era
of AI.
",https://www.researchgate.net/publication/372423221_Artificial_intelligence_to_enhance_corporate_governance_A_conceptual_framework
Explainable Artificial Intelligence (XAI) Approaches for Transparency and Accountability in Financial Decision-Making,SSRN,"Nitin Rane, Saurabh Choudhary, Jayesh Rane","ARTICLE This paper examines how explainable AI can enhance transparency and accountability in financial decision-making, with relevance to corporate governance.",2023,"Recently, there has been a growing trend in incorporating Artificial Intelligence (AI) into financial decision- making, prompting concerns about the transparency and accountability of these intricate systems. This study investigates the impact of Explainable Artificial Intelligence (XAI) approaches in alleviating these concerns and improving transparency in financial decision-making processes. The paper commences by outlining the current landscape of AI applications in finance, underscoring the complex and opaque nature of advanced machine learning models. The lack of interpretability in these models presents a significant challenge, as stakeholders, regulators, and end-users often struggle to comprehend the reasoning behind AI-driven financial decisions. This opacity raises questions regarding accountability and trust, particularly in critical financial scenarios. The primary focus of the research centers on the analysis and implementation of XAI techniques to introduce transparency into financial AI systems. Various XAI methods, including rule-based systems, model-agnostic approaches, and interpretable machine learning models, are scrutinized for their effectiveness in producing understandable explanations for AI-driven financial decisions. The paper explores how these approaches can be tailored to meet the distinct requirements of the financial domain, where interpretability is essential for regulatory compliance and stakeholder confidence. Moreover, the research delves into the potential impact of XAI on accountability mechanisms within financial institutions. By offering interpretable explanations for model outputs, XAI not only enhances transparency but also empowers financial professionals to identify and rectify biases, errors, or unethical behaviour in AI algorithms. By promoting transparency and accountability, XAI not only addresses ethical concerns but also facilitates the responsible and trustworthy deployment of AI in the financial sector. This, in turn, contributes to the advancement of fair, reliable, and secure financial systems.
Keywords: Explainable Artificial Intelligence, Finance, Explainable AI, Risk assessment, Forecasting, Decision-making.
Introduction
In recent years, a significant transformation has occurred in the financial landscape as Artificial Intelligence (AI) and machine learning (ML) have been integrated into decision-making processes [1-4]. Financial institutions increasingly rely on advanced algorithms to analyze extensive data sets and inform critical decisions [5-8]. Consequently, the demand for transparency and accountability in these automated systems has become crucial [9- 12]. Explainable Artificial Intelligence (XAI) has emerged as a pivotal area of research, seeking to bridge the gap between the complexity of advanced AI models and the comprehensibility required for effective decision-making in finance [13-18]. Financial decisions, from credit scoring to investment strategies, are now frequently entrusted to AI systems capable of processing data at unprecedented speeds. However, the inherent opacity of many state- of-the-art AI models poses challenges for stakeholders seeking to understand, interpret, and trust the outcomes of these automated decisions. The repercussions of biased or inaccurate AI-driven financial decisions can be severe, impacting both individual consumers and the stability of financial markets. This paper explores the realm of XAI and its crucial role in enhancing transparency and accountability in financial decision-making processes.
The adoption of AI in financial institutions promises improved efficiency, reduced costs, and enhanced decision- making capabilities [3,10]. However, the intricate nature of advanced AI models, such as deep neural networks, often renders them inscrutable, resulting in the ""black box"" problem. Stakeholders, including regulators, investors, and consumers, grapple with understanding the rationale behind AI-generated decisions. XAI has emerged as a response to the need for transparency and accountability in AI systems [14,17]. It focuses on developing algorithms and methodologies that provide insights into the decision-making processes of complex models, making them more understandable to human users. In financial decision-making, XAI aims to enable stakeholders,
including regulators, auditors, and end-users, to comprehend how AI models arrive at specific conclusions and predictions. Various XAI approaches, from rule-based systems to model-agnostic methods like LIME and SHAP, have been proposed and developed, enhancing the applicability of XAI across a wide range of financial applications [19-27]. Transparency is a cornerstone of trust in financial systems. As AI algorithms become increasingly intertwined with financial decision-making, XAI provides the means to uncover the decision-making process, offering transparency and fostering trust in the outcomes of AI-driven financial decisions. Moreover, transparency in financial decision-making is not only a regulatory imperative but also a strategic advantage for financial institutions [28-35]. Clear and interpretable AI models empowers institutions to demonstrate compliance with regulatory requirements, mitigate legal risks, and enhance their reputation. Figure 1 shows the co-occurrence analysis of the keywords in literature.
Figure 1 Co-occurrence analysis of the keywords in literature
Deploying AI in financial decision-making poses the challenge of potential bias in algorithmic outcomes [36-40]. XAI plays a crucial role in identifying and mitigating bias by providing interpretable insights into the factors influencing model decisions. Techniques like fairness-aware machine learning and algorithmic auditing enable financial institutions to systematically evaluate the impact of their models on different demographic groups, contributing to a more inclusive and socially responsible financial ecosystem. Recognizing the growing influence of AI in financial decision-making, regulatory bodies worldwide are adapting to ensure ethical standards and legal frameworks are followed. XAI has emerged as a key enabler of regulatory compliance, providing a means to interpret, validate, and audit AI models to ensure adherence to regulatory requirements [15,17]. This research paper explores various XAI approaches and their application in the financial sector, contributing to the ongoing dialogue on responsible AI deployment in the financial industry. The adoption of XAI approaches stands as a crucial step towards ensuring ethical, transparent, and accountable financial decision-making processes as financial institutions navigate the evolving landscape of AI.
Methodology
The literature review delves into a systematic and exhaustive exploration of academic databases, journals, and conference proceedings. Key search terms, such as ""Explainable Artificial Intelligence,"" ""XAI,"" ""transparency,"" ""accountability,"" and ""financial decision-making,"" guide the search process. Inclusion and exclusion criteria are established to ensure the relevance and quality of the selected literature, with a focus on recent publications to
capture the latest advancements in XAI within the financial domain. During the review, emphasis is placed on identifying and categorizing relevant studies, theoretical frameworks, and practical implementations of XAI in financial contexts. Special attention is given to methodologies and approaches that enhance transparency and accountability, addressing challenges linked to the opacity of complex AI models in financial decision-making. The bibliometric analysis, on the other hand, quantitatively assesses the scholarly output in the field of XAI in financial decision-making. Utilizing the Scopus and Web of Science databases, the analysis includes extraction and examination of bibliometric data such as citation counts, publication trends, authorship patterns, and keyword co-occurrences, using Vosviewer software. The objective of the analysis is to pinpoint influential publications, prolific authors, and seminal works within the domain. Network analysis reveals connections and collaborations among researchers and institutions, while clustering algorithms identify thematic clusters and emerging topics in the field of XAI in financial decision-making. The findings from both the literature review and bibliometric analysis are then synthesized to present a comprehensive overview of the current state of XAI approaches for transparency and accountability in financial decision-making. This synthesized data is interpreted in the context of existing theories, practical applications, and the evolving landscape of AI in finance. Figure 2 shows the co- authorship analysis.
Figure 2 Co-authorship analysis
Results and discussion
Model-Specific Explainable AI approaches for financial decision-making
The focus of model-specific explainability lies in offering insights into the internal mechanisms of a specific AI model, elucidating its decision-making process [14-16]. Complex algorithms, such as deep neural networks and ensemble methods, often achieve high accuracy but lack interpretability. The opaqueness of these models poses challenges in understanding and justifying their decisions, particularly in situations where regulatory requirements demand transparency. This section explores diverse model-specific XAI approaches customized for financial decision-making. Table 1 shows the Explainable Artificial Intelligence (XAI) approaches in financial decision- making.
Decision Trees and Rule-Based Models:
Decision trees inherently offer interpretability by recursively partitioning the input space based on feature values. In financial decision-making, decision trees find applications in modeling credit scoring, fraud detection, and risk assessment [41-45]. Each node in the tree represents a decision based on a specific feature, providing a clear and intuitive decision path. Rule-based models, akin to decision trees, generate easily understandable decision rules typically in the form of ""if-then"" statements. This format makes them interpretable to both experts and non- experts, a valuable trait in financial domains where regulatory bodies often require institutions to justify their decisions.
Linear Models:
Linear models, such as linear regression and logistic regression, have been widely employed in finance due to their simplicity and interpretability. Assuming a linear relationship between input features and output, such models facilitate a straightforward analysis of the impact of each feature on the decision [46-49]. Coefficients assigned to each feature indicate the direction and magnitude of influence, aiding in risk assessment and portfolio optimization. Despite their simplicity, linear models may fall short in capturing complex non-linear relationships present in data [47,48,50-54]. In such cases, feature engineering or more complex models may be necessary.
Local Interpretable Model-agnostic Explanations (LIME):
LIME, as a model-agnostic approach, explains the predictions of any machine learning model by approximating its decision boundary locally. In financial decision-making, LIME can be applied to black-box models to generate understandable explanations for individual instances [20,23]. LIME achieves this by perturbing the input data and observing the corresponding changes in the model's predictions. It then fits a locally interpretable model, such as a linear model, to explain the model's behavior around the specific instance. This approach provides valuable insights into why a model made a particular decision, enabling stakeholders to comprehend and trust the model's output.
SHapley Additive exPlanations (SHAP):
SHAP values, rooted in cooperative game theory, allocate contributions of each feature to the model's output. In financial decision-making, SHAP values help understand the impact of individual features on a specific prediction or decision [25,27]. By assigning a Shapley value to each feature, one can quantify its contribution to the model's output, enhancing interpretability. SHAP values offer a comprehensive view of feature importance, considering interactions between features. This is crucial in finance, where intricate relationships between economic indicators can significantly influence outcomes. Understanding these interactions aids in risk assessment and scenario analysis.
The important equation used in explainable artificial intelligence (XAI) is the SHapley Additive exPlanations (SHAP) values formula. The SHAP values provide a way to fairly allocate contributions among features in a prediction model. The formula is given by:
Where, represents the SHAP value for feature i in the model f.
is the set of all features is a subset of features excluding i
is the model's prediction when considering only the features in is the model's prediction when adding feature i to
Tree-based Ensembles and Feature Importance:
Ensemble models like Random Forests and Gradient Boosting are potent tools in finance owing to their high predictive accuracy [55-60]. While the ensemble itself may be complex, analyzing the individual decision trees within it allows deriving feature importance. Feature importance in ensemble models is often measured by how much a particular feature contributes to the reduction of impurity (e.g., Gini impurity) in the decision trees [61- 68]. Features with higher importance values play a more significant role in the model's decision-making process. Communicating these importance scores to stakeholders helps them understand which factors are critical in driving model predictions. Figure 3 shows the Explainable Artificial Intelligence (XAI) approaches for transparency and accountability in financial decision-making.
Figure 3 Explainable Artificial Intelligence (XAI) approaches for transparency and accountability in financial decision-making
Model-Agnostic Explainable AI approaches for financial decision-making
Model-Agnostic Explainable AI (XAI) methodologies play a pivotal role in enhancing transparency and interpretability within financial decision-making processes [69-73]. As machine learning like models grow in complexity, the necessity to comprehend the reasoning behind their predictions becomes increasingly vital, particularly in high-stakes domains [3,5,74-78]. Model-Agnostic XAI encompasses techniques and methodologies applicable to interpreting predictions from any machine learning model, irrespective of its underlying architecture. This approach proves especially pertinent in scenarios where black-box models, such as deep neural networks or ensemble methods, are deployed. Despite their predictive accuracy, these models often lack transparency, posing challenges for stakeholders in grasping the factors influencing their decisions. The objective of Model-Agnostic XAI is to furnish meaningful insights into model predictions, enabling users to trust, validate, and act upon the model's outputs. This is particularly crucial in the financial industry, where decisions have far-reaching impacts on investments, risk management, and overall business strategies.
Within the financial sector, decision-making heavily relies on accurate predictions and risk assessments [14,17]. However, conventional machine learning models, including intricate algorithms like deep neural networks, are frequently treated as ""black boxes"" due to their complex internal structures. This lack of interpretability raises concerns regarding accountability, ethics, and the potential for biased outcomes. Model-Agnostic XAI addresses these challenges by offering transparency into the decision-making process. This transparency is imperative for financial institutions to adhere to regulations, mitigate risks, and foster trust with clients and stakeholders. By understanding the factors influencing predictions, users can make more informed decisions, validate model outputs, and identify potential biases.
LIME (Local Interpretable Model-agnostic Explanations):
LIME is a popular approach that generates locally faithful explanations for model predictions [20,23]. It achieves this by perturbing the input features of a given instance and observing changes in the model's output. By fitting a locally interpretable model to the perturbed instances, LIME provides insights into how the model behaves around a specific data point. In finance, LIME can elucidate individual predictions, aiding in understanding the model's decision-making process for specific transactions or investment recommendations.
SHAP (SHapley Additive exPlanations):
SHAP values, grounded in cooperative game theory, attribute each feature's contribution to the difference between the model's prediction and the average prediction [24-27]. These values offer a fair way to distribute the model's output among its features. In financial decision-making, SHAP values can be employed to attribute the impact of different factors to a specific prediction, helping stakeholders understand the relative importance of various features in investment or risk assessment.
Decisions on relationships
based linear
Limited to relationships
linear
LIME (Local Interpretable Model-agnostic Explanations)
Generates interpretable models for specific instances
Model-agnostic, applicable to black-box models
Global model behavior may not be fully captured
Predictive Analytics, Customer Support
SHAP (SHapley Additive exPlanations)
Assigns values to features, explaining their impact
Equitable distribution of feature importance
Computationally intensive for large models
Algorithmic Trading, Investment Strategies
Counterfactual Explanations
Provides alternative scenarios for understanding decisions
Intuitive for end-users
Generating meaningful counterfactuals can be challenging
Personalized Financial Advice, Credit Risk Assessment
Anchors
Identifies minimal input features necessary for a given prediction
Easy to understand
Limited applicability to specific model types and data
Insurance Underwriting, Mortgage Approval
Attention Mechanisms
Highlights relevant parts of input data
Captures important features
Interpretability challenges for non- experts
Automated Trading, Sentiment Analysis
Local Interpretable Model (LIM)
Trains a local interpretable model around the prediction
Provides insights for specific instances
Generalization may be limited
Personalized Financial Planning, Credit Scoring
Explanations as Model
Trains a separate interpretable model alongside the main model
Offers a more interpretable model
Increased computational complexity
Algorithmic Trading, Fraud Detection
      
DALEX (Model Agnostic Exploration, Explanation, and Learning):
DALEX is a comprehensive framework for model-agnostic XAI, facilitating the exploration of model behavior, comparison of different models, and generation of easy-to-understand visualizations [79-85]. In the financial sector, DALEX can analyze and compare the performance of various models, assisting decision-makers in selecting the most interpretable and reliable model for specific use cases [86-90].
Anchors:
Electronic copy available at: https://ssrn.com/abstract=4640316
Anchors aim to identify simple, human-understandable rules that are both sufficient and necessary for a model's prediction. Such rules can act as anchors that hold true for a particular instance, providing a clear explanation for the model's decision [91-95]. In finance, anchors can identify specific conditions or criteria significantly influencing investment recommendations or risk assessments [96-103].
Counterfactual Explanations:
Counterfactual explanations generate instances that, while similar to the original input, result in a different model prediction [104-107]. By showcasing what changes in the input would lead to an alternative outcome, counterfactual explanations help users understand the decision boundaries of the model [108-112]. In finance, this approach can be valuable for understanding how slight modifications to certain variables could impact investment decisions or risk assessments.
Challenges of implementing XAI in the financial sector
While strides have been made in developing explainable AI techniques for financial decision-making, persistent challenges underscore the need for ongoing innovation in this field.
Complexity of Financial Models:
Financial models, often intricate and involving numerous variables, pose challenges in interpreting decisions made by advanced models like deep neural networks [113-119]. Striking a balance between model complexity and interpretability remains an ongoing concern.
Trade-off between Accuracy and Explainability:
The perennial trade-off between model accuracy and explainability is evident [120-124]. Simpler models are more interpretable but may not capture systems' complexity as effectively [125-129]. Achieving the right balance is crucial to meet accuracy and explainability requirements.
Dynamic Nature of Financial Markets:
Financial markets' dynamic nature poses a challenge as traditional explanations may become outdated in a rapidly evolving landscape [130-136]. Explainable AI models must adapt in real-time to changing financial conditions.
Regulatory Compliance:
The lack of standardized guidelines on acceptable levels of explainability poses a challenge for financial institutions navigating diverse regulatory landscapes while implementing explainable AI solutions [137-143].
Future developments in XAI in finance
As AI technologies become increasingly prevalent in finance, the demand for interpretable and explainable AI systems is on the rise. This section delves into the prospective advancements in XAI within the finance sector.
Interpretable Machine Learning Models:
In the future, financial entities are expected to prioritize the development and integration of machine learning models inherently designed for interpretability. While complex models, such as deep neural networks, showcase impressive performance, their lack of interpretability poses a significant hurdle. Innovations in interpretable models, including decision trees, rule-based models, and linear models, will be instrumental in overcoming this challenge. These models not only deliver accurate predictions but also furnish transparent explanations of the factors influencing their decisions.
Hybrid Models and Ensemble Techniques:
A promising direction for XAI in finance involves incorporating interpretable components into intricate AI models. Hybrid models, amalgamating the strengths of interpretable and non-interpretable models, can strike a
Electronic copy available at: https://ssrn.com/abstract=4640316
balance between accuracy and explainability [144-149]. Ensemble techniques, which amalgamate predictions from diverse models, can enhance transparency. By combining outputs from interpretable models with those from more complex models, financial institutions can maintain accuracy while providing comprehensible explanations for decisions.
Regulatory Compliance and Ethical Considerations:
With the increasing use of AI in finance, regulatory bodies are anticipated to impose more stringent requirements for transparency and interpretability. Future developments in XAI will be motivated by the necessity to adhere to regulations like GDPR and address ethical concerns surrounding AI decision-making. Financial institutions will invest in technologies and practices ensuring that their AI systems are not only accurate but also accountable and fair, aligning with regulatory frameworks and ethical standards.
Explainability as a Service (XaaS):
A growing trend in the future of XAI is the emergence of Explainability as a Service (XaaS). This model involves third-party providers offering specialized tools and services dedicated to enhancing the interpretability of AI models. By outsourcing explainability solutions, financial institutions can concentrate on their core competencies while benefiting from state-of-the-art XAI tools. This approach can streamline the integration of interpretable models into existing systems and expedite the deployment of transparent AI in finance.
Natural Language Processing (NLP) for Explanation Generation:
Advancements in Natural Language Processing (NLP) will be pivotal in the future of XAI. Financial institutions will leverage NLP techniques to generate human-readable explanations for AI model decisions. This not only improves transparency but also facilitates communication between AI systems and end-users, making it simpler for stakeholders to understand and trust the decisions made by AI algorithms.
User-Centric Explainability:
Future developments in XAI will prioritize user-centric approaches, tailoring explanations to the specific needs and backgrounds of end-users. Financial institutions will invest in designing interfaces that present interpretable information clearly and accessibly. This may involve interactive visualizations, personalized dashboards, and user-friendly explanations to ensure that individuals, including non-experts, can comprehend and trust the decisions made by AI models.
Continuous Monitoring and Model Maintenance:
To maintain ongoing trust in AI systems, financial institutions will implement continuous monitoring and model maintenance practices. This involves tracking the performance of AI models over time, identifying shifts in data distributions, and updating models accordingly. Transparent reporting on model performance and potential biases will be essential for maintaining trust, and financial institutions will invest in automated tools for real-time monitoring and adjustment of AI systems.
Education and Training Initiatives:
The future of XAI in finance will see a focus on education and training initiatives. Financial professionals and decision-makers will need to develop a deeper understanding of AI and XAI concepts to interpret and leverage AI-driven insights effectively. Training programs and resources will be developed to empower individuals to make informed decisions based on AI recommendations, fostering a culture of trust and collaboration between humans and AI systems.
Conclusions
The financial decision-making has undergone a profound transformation with the emergence of Artificial Intelligence (AI). In recent times, the integration of AI, particularly Explainable Artificial Intelligence (XAI), has played a pivotal role in augmenting transparency and accountability within financial decision-making processes.
Electronic copy available at: https://ssrn.com/abstract=4640316
Our investigation traced the evolutionary trajectory of XAI in the financial sector, from its origins in traditional machine learning models to the more advanced deep learning algorithms. As financial institutions increasingly employ complex AI models to inform decision-making, the demand for interpretability and explainability has become paramount. XAI addresses this need by offering insights into the decision-making process of AI models, ensuring that financial professionals and stakeholders can comprehend and trust the outcomes. A cornerstone of XAI lies in the development of interpretable models that empower stakeholders to grasp the rationale behind AI- driven financial decisions. Our research underscored the significance of utilizing models such as decision trees, rule-based systems, and linear regression, which not only yield accurate predictions but also provide clear explanations for their conclusions. By demystifying the opaque nature of complex AI algorithms, interpretable models pave the way for increased transparency, enabling financial experts to validate and refine decision-making processes.
Understanding the influence of input features on AI model predictions is vital for instilling trust in financial decision-making systems. Feature importance and attribution techniques emerged as powerful tools within the XAI toolkit. Our research showcased how techniques like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) empower stakeholders to identify the most influential factors driving AI predictions. By quantifying the contribution of each feature, financial professionals can assess model behavior and ensure alignment with industry regulations and ethical standards. In the quest for transparency and accountability, our research emphasized the importance of fostering collaboration between humans and AI systems. XAI facilitates a symbiotic relationship, where human experts leverage the strengths of AI models while maintaining oversight and control. Addressing concerns related to bias, fairness, and accountability is imperative to prevent unintended consequences. Embracing XAI in financial decision-making is not just a technological evolution; it is a paradigm shift towards a more responsible and trustworthy financial future.
",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4640316
Artificial intelligence will transform decision-making. Here's how,WEF,"Philip Meissner, Yusuke Narita","ARTICLE effective use of artificial intelligence in strategic decision-making will be one of the biggest determinants for future competitiveness. The interaction between humans and AI, as well as the ability to choose which decisions to delegate to AI, will be among the most important skills for decision-makers. Trust, access and integration will shape the scale and speed of AI adoption in future decision-making processes.
",2023,"	Effective use of artificial intelligence in strategic decision-making will be one of the biggest determinants for future competitiveness.
	The interaction between humans and AI, as well as the ability to choose which decisions to delegate to AI, will be among the most important skills for decision-makers.
	Trust, access and integration will shape the scale and speed of AI adoption in future decision-making processes.
Artificial intelligence already plays a crucial role in the decision-making processes of many companies and is increasingly used to inform policy and public sector decisions around the world. Supervised learning, for example, is already used by some governments to detect potential criminals and terrorists.
Yet, so far, only 7% of companies use AI in big strategic decisions, such as strategy development or financial planning. At the same time, the importance of the technology for the future is enormous: 75% of business leaders believe that what will set companies apart from their competitors in the future will be determined by who has the most advanced generative AI.
Digital inclusion: Our EDISON Alliance is mobilizing leaders from across sectors to accelerate digital inclusion, having positively impacted the lives of 454 million people through the activation of 250 initiatives across 90 countries.
AI in developing economies: Our Centre for the Fourth Industrial Revolution Rwanda is promoting the adoption of new technologies in the country, enabling over 4,000 daily health consultations using AI.
Innovative healthcare: Our Medicine from the Sky initiative is using drones to deliver medicine to remote areas in India, completing over 950 successful drone flights.
AI for agriculture: We are working with the Government of India to scale up agricultural technology in the country, helping more than 7,000 farmers monitor the health of their crops and soil using AI.

Especially in the area of decision-making, we already see AI’s power today. Indeed, it has already arrived in many board rooms. More than 40% of CEOs say they use generative AI to inform their decision-making processes. There are many benefits of such AI tools, from better compliance to less biased and more inclusive strategic decisions. At the same time, as AI becomes more advanced, we will have to develop strategies for how to use these systems in important decisions. We will have to decide how to best interact with AI, and more importantly, which decisions we can completely delegate to AI.
In fact, we think that future competitiveness may not only be about who has the most advanced AI, but also how this technology is used for strategic decision-making. This will ultimately influence the competitiveness of companies and countries in the age of AI.
Two types of AI-based decisions
Some decisions today are already made entirely by AI. For instance, at Ant Financial, AI is approving loans and will now be increasingly used in wealth management. Also, tech companies like Facebook, Microsoft and Netflix allocate digital content by reinforcement learning algorithms.
While fully autonomous AI decisions may still be the exception, most decisions executives and consumers make are already informed by AI. At Amazon, for example, 35% of revenue is driven by AI recommendations. Government services are also increasingly influenced by the technology. In Singapore, citizens can get information about government services 24/7 by using “Ask Jamie” or the new VICA bot.
Have you read?
How to tell if artificial intelligence is working the way we want it to
Does this artificial intelligence think like a human?
This human-AI interaction has important implications for decision-making. A recent study found that decision-makers arrive at completely different decisions using identical AI recommendations based on their individual decision-making styles. This suggests that it may not be enough to develop a perfect AI. If there is a human in the loop, we will still have to think about the psychological elements driving decision-making.
These developments leave us with many questions that are crucial for the future of decision-making. Which decisions will we still make in the future? Could AI automate more and more decisions in our everyday lives? Will it make these decisions better than we do? In any case, we will have to think about which decisions we are comfortable for AI to make and which ones we would rather make ourselves. The answer will likely differ around the world.
Main drivers of AI-based decisions
In our view, there are three main drivers that will shape the speed and scale to which AI will be used in decision-making and, as a result, influence competitiveness.
1. Trust
A recent global study showed that trust levels in AI differ quite substantially around the globe. While 75% of people in India are willing to trust AI, only 15% would do so in Finland and 23% in Japan. We believe that the adoption of AI in decision-making will depend significantly on the level of trust in the system. Will we see a world of multiple speeds in AI-based decision-making? In a competitive setting, such developments could significantly influence the competitiveness of nations in the tech space.
2. Access
Will the world’s leading AI systems be open ones that essentially give everyone access to an individual AI, or closed systems that enable companies and nations to differentiate themselves and control access to the technology? In a world in which closed systems dominate, strategic AI-based decisions might be a luxury of the few.
3. Integration
The third factor we predict will determine the scale and speed of AI adoption in decision-making is the way in which both companies and governments are able to manage the AI-human interface. Being able to effectively utilize AI input in strategic decisions and mitigating the effects of human biases in the process will be crucial.gnificantly on the level of trust in the system. Will we see a world of multiple speeds in AI-based decision-making? In a competitive setting, such developments could significantly influence the competitiveness of nations in the tech space.
2. Access
Will the world’s leading AI systems be open ones that essentially give everyone access to an individual AI, or closed systems that enable companies and nations to differentiate themselves and control access to the technology? In a world in which closed systems dominate, strategic AI-based decisions might be a luxury of the few.
3. Integration
The third factor we predict will determine the scale and speed of AI adoption in decision-making is the way in which both companies and governments are able to manage the AI-human interface. Being able to effectively utilize AI input in strategic decisions and mitigating the effects of human biases in the process will be crucial.",https://www.weforum.org/agenda/2023/09/how-artificial-intelligence-will-transform-decision-making/
"The Impact of AI on Strategic Thinking and Decision-making
",AIM Business School,Elle Taurins,"ARTICLE with the advancement of strong AI development “humanised AI” will not become a substitute to effective leadership any time soon. Moving forward, the role of executive education will transform from knowledge creation to developing AI-human collaboration capability, enabling executives to think critically and use judgement. Strong strategic thinking skills combined with sophisticated analytical tools will become an essential competency for leaders making strategic decisions for their organisations.",2023,"
With the recent launch of generative AI tools like ChatGPT, the discourse about AI in the corporate world ranges from its impact on job displacement and workplace productivity to risks and ethics. The expectations from AI are high worldwide: the market for AI is anticipated to reach 53 billion USD by 2026, with a compound annual growth rate of 35.4 during 2019-2026 (Barnea, 2020). At a C-suite level, the questions are posed on the AI capability to create a competitive advantage by formulating a strategic position or automating strategic decision-making (SDM). This article addresses the potential impact of AI on strategic thinking and strategy formulation. It overviews the benefits of AI tools currently in use and probes the extent to which SDM can be delegated to AI and ML (machine learning).
Strategising in the VUCA World
For leaders and managers, operating in an unpredictable and rapidly changing environment has become the ‘new normal’. The Volatile, Uncertain, Complex and Ambiguous business context, often referred to as VUCA World (Chadha, 2017), continues to intensify. The recent Covid challenge highlighted the need for strategic response capability in unpredictable situations. The unprecedented levels of complexity in both government and business contexts are emphasised by global and Australian researchers investigating the future of work (Ruppanner et al., 2023) and the business intelligence community (Barnea, 2020; Leavy, 2023).
Under VUCA conditions, organisations and corporations must be able to respond to pressures, identify new opportunities and make highly contextualised decisions. VUCA-responsive strategy formulation needs to be agile and less reliant on traditional tools and linear models. Thought leaders have been calling for new thinking and re-inventing strategy creation, suggesting impact analysis (Hines & Bishop, 2013), specific VUCA-factor response actions (Bennett & Lemoine, 2014), strategic foresight (Duijne & Bishop, 2018) and entrepreneurial approaches to strategy (Hoglund & Martensson, 2019).
Cognitive processes in strategy making
The concept of ‘strategy’ (or ‘strategy formulation’) and its adjacent notion of ‘strategic thinking’ are abstracts and are tricky to define. Mintzberg prominently stated (2000, p. 66) that “nowhere was anyone told how to create strategy. How to collect information, yes. How to evaluate strategy, yes. How to implement it, for sure. But not how to create it in the first place.” Strategic thinking underpins strategy formulation and SDM.
The construct of ‘strategic thinking’ is linked to advanced cognitive processes and a leader’s ability to think conceptually, however, it is continuously transforming and adapting to the needs of organisations. In the 1960s, it referred mostly to a capability to synthesise the data, in the 1990s, Mintzberg incorporated a manager’s ability to synthetise the learning in the direction of the business, and later the construct was enhanced into a “conceptual, systems-oriented, directional and opportunistic thinking leading to the discovery of novel, imaginative organizational strategies” (Casey & Goldman cited in Adzeh, 2017, p. 4). Strategic thinking frameworks like Liedtka’s five-factor ‘design approach’ model (1998) or a cognitive Pisapia et al.’s three-factor model (2005) imply that strategic thinking is both analytical and creative. It is meaningful only in a context, requires open minds, being comfortable with ambiguity and the ability to make consequential decisions (Pisapia, 2011).
In a VUCA world, a leader’s role requires a highly developed strategic thinking capability to make strategic decisions based on strategic options. Gartner Inc. has coined the term ‘Decision Intelligence’ (Larson, 2022) and argues, that Decision Intelligence is necessary for three types of inter-dependent decisions: one-off strategic decisions, repeated managerial decisions, and high-volume operational decisions. In essence, these types of decisions align with the three levels of strategy: corporate, business and functional (tactical).
In this turbulent business landscape and a rise of AI/ML, organisations pin their hopes on AI-generated strategic solutions. Many companies have a limited understanding of how the AI-driven change and disruption will impact their strategising and decision-making (Atsmon, 2023). Leaders still need to gain more clarity on the potential AI cognitive capabilities that could either meaningfully contribute to addressing corporate strategic issues, or substitute humans in SDM.
The key pain points in strategising processes
Many corporations are dedicating significant resources to gathering and analyse information on their competitors. Still, they often experience ‘strategic surprises’ when their competitors make moves that were not anticipated. Although the gap between the need for and the availability of the information is almost inevitable, the ‘fast yet reliable capture of emerging changes’ is critically important to senior executives driving strategy formulation (Barnea, 2020).
The traditional macro-environment analysis models like PESTLE require long lead times for data analysis because it relies on historical data and evidence-based reasoning. Considering that even Fortune 500 companies still use simple information-gathering tools, primarily from OSINT (open-source intelligence), the added value of such analysis to senior leaders needs to be improved (Barnea, 2020). This situation pushes executives to decide based on partial information or their ‘gut feeling’. The risk of bias in SDM is highlighted by many researchers and practitioners (Wu et al., 2023; Atsmon, 2023; Sanclemente, 2022; Hesel et al., 2022; Barnea 2020).
The speed of processing high volumes of data into manageable information and viable alternatives adds another pressure to SDM. In uncertainty, executives want to be as informed as possible. The data in organisations is often overwhelming and/or manually curated, and if the monitoring of the inputs is unsystematic or lagging, then important ‘weak signals’ may be overlooked or cause a prediction error. Incorrect predictions may be costly. Lagging data monitoring may lead to businesses not being able to respond in time and potentially losing their competitive advantage.
Strategy creation is a cognitively demanding process underpinned by cognitive insights; it involves reasoning, problem-solving and learning. To inform their decisions, strategists must identify underlying patterns, gain meaningful insights and use probability techniques to support their judgement.
In a nutshell, most organisations are experiencing the need for speed, greater anticipatory and predictive capability, and a synergy of analytical and judgement skills.
Where are we at with AI adoption in strategic applications?
AI is becoming embedded in our daily life, examples include Cortana, Alexa or Siri, or virtual assistants. Our business experience with AI is still rather limited, although task automation is growing in customer service and marketing. Empirical evidence shows that AI can be used to create marketing strategy, especially in manager-AI partnership strategy formulation activities (Eriksson et al., 2020). AI already performs functions such as analysing customer surveys or customer interactions, i.e. automated marketing decisions on an operational level (Barnea, 2020; Korzynski et al., 2023). Examples of AI use in marketing indicate that AI can indeed be used in strategy formulation, however, the strategy is limited to the functional rather than corporate level. Marketing uses a “Weak AI” type, i.e. it can emulate human logic by analysing large amounts of data and can act as a decision-maker if the decision process required is rational. Therefore, it can be automated and provide decision-making predictions or scenarios (options).
The 2022 study of 500 high-level B2C managers conducted by researchers from Nuremberg Institute for Market Decisions shows that 26% of these managers use AI in the role of “collaborator”, where humans interact with AI but control the overall process (Hesel et al., 2022). The preferred model among respondents was augmented decision-making with humans in control. Research suggested that humans are more receptive to AI when they can modify its decisions or forecasts, minimising the so-called ‘algorithm aversion’.
Although Hesel’s research team argues that the boundaries of AI in decision-making are shifting from the operational to the strategic level (Hesel et al., 2022), the definition of ‘strategic’ remains elusive and may not refer to the corporate level of strategy. Higher-order thinking skills would be needed for an organisation-wide strategy and SDM, or a ‘Creative Analytics’ type of AI to substitute a human in making a final decision related to imagination and creativity (Eriksson et al., 2020).
Key advantages of AI
AI undeniably exceeds human capabilities in translating big data, including unstructured inputs like audio, video, and images into manageable information and knowledge. This allows managers to input it into marketing and sales strategies. For example, when Netflix entered the content business, it made use of its data on 27 million US subscribers and 33 million subscribers worldwide. By using subscribers’ viewing history, searches, and ratings, Netflix decided to create a successful US adaptation of the British show House of Cards (Laney 2020 cited in Gudigantala et al., 2022)
Reduction of the long lead time for data analysis outputs may create a competitive advantage due to real-time data tracking (such as purchase data or search traffic) and speed of analysis (Barnea, 2020; Korzynski, et al., 2023).
AI algorithms can help to de-bias important executive decisions. AI can provide executives with competitor data analysis which will help to create a more objective understanding of their rivals by processing or even predicting competitors’ moves (Barnea, 2020).
The types of AI and why it matters
It is important to note that the outputs and benefits of AI may depend on the type of AI selected (Gudigantala et al., 2022; Wu et al., 2023). There is a plethora of AI definitions and classifications, and the discourse on AI/ML terminology is evolving in real-time.
In addition to an already mentioned “Weak” (task-specific) and ”Strong” (human-like problem solver), AI could be based on ability (narrow, general) or functionality (reactive, limited, self-aware), or it can be approached from a cognition (logical) or behavioural perspectives. Prior research organised AI systems into rule-based (decision process automation) or learning-based (predictive models using ML) and included an NLP (natural language-based system) to analyse customer reviews (Martinez-Plumed et al., 2021). AI could specialise in optimisation of solutions, or in physical interactions (robotics). Gartner Analytics Ascendancy Model (GAAM) identified four key types of AI (Descriptive, Diagnostic, Predictive and Prescriptive), while Eriksson et al. (2020) proposed the extension of GAAM to Creative Analytics, i.e., AI capable of innovation. It is generally agreed that there are six key types of AI, three of them are currently in use: Descriptive, Diagnostic and Predictive. The other three types of AI will take more time to develop: AI is expected to be able to advise of value-creating actions, delegate decision authority to AI with supervision and function fully autonomously, and make decision with no human interaction (Atsmon, 2023).
The classification or taxonomy of AI matters, because each type performs a different function and the analyst guiding its process needs to understand what the strategist is aiming to achieve.
Descriptive AI can be used for performance analysis like dashboards (Atsmon, 2023), however, it could be also useful for data collection and analysis, systematically and effectively identifying patterns and signals that may be missed by humans (Eriksson et al., 2020, Barnea, 2020). It facilitates insights that inform strategy formulation or deliver early warning signs of threats. Diagnostic AI can organise a portfolio into segments, while Predictive AI can provide a systemic view for decision makers (Atsmon, 2023). Predictive analytics is still in development today, and it uses ML to understand human behaviour (Gudigantala et al., 2022). ML is a subset of AI, it depends on the analyst for judging and guiding the process and assessing intermediate results (Sanclemente, 2022), and this potentially makes Predictive AI more difficult and riskier.
OpenAI like ChatGPT available to us today may affect decision making at a strategic level if managers use it to obtain information, filter and organise options, and obtain recommendations in specific situations. Generative AI can assist with data analysis that might guide evaluations, and entrepreneurs and managers may be able to make better judgements as a result of using more data and educated reasoning (Korzynski et al., 2023). However, ChatGPT or other AI is currently incapable of reasoning or decision-making.
With a development of Strong AI, ML would enable the ability to think and operate as ‘human-inspired’ or ‘humanised AI’, only, current AI is still a long way from Strong AI capability (Hesel et al., 2022).
AI and humans: a matter of trust
Considering current or future use of AI for strategic analysis or SDM requires a more in-depth understanding of the interconnection between the human and technology.
Analysts can tell a machine what they want it to do, such as creating decision rules or giving it datasets. Alternatively, analysts can define the output or the goal and let the machine find a solution. In other words, analysts ‘feed’ the task to AI. Framing of the task can influence the SDM process profoundly (Wu et al., 2023). AI mechanisms programmed by humans incorporate human bias in ML: “there is an essential connection between the way the machine performs its actions and how the person behind the machine wants the action to occur” (Sanclemente, 2022, p. 1331). Evidence suggests that such bias may have serious implications, including national security. Mishaps of AI use were reported in the US House of Representatives in 2018, which concluded that considerable flaws and biases can exist in the algorithms that support AI systems. Faulty algorithms and skewed data provide blind spots which raise unmanaged security risks (Sanclemente, 2022).
Another challenge is in sourcing technology talent who can build AI tools and be able to translate business problems into AI questions. For this, analysts need to understand what the company is trying to achieve (Atsmon, 2023; Gudigantala et al., 2022).
From the AI user perspective, a previously mentioned ‘algorithm aversion’ problem is evident. The results of the survey of 500 high-level B2C managers from 2000 of the biggest public companies in the world suggested that human decision makers accept algorithm more frequently when they can modify its decisions or forecasts (Hesel et al., 2022). This implies either a lack of trust in AI solutions or a pre-existing bias. It is not clear yet how grounded the fear of AI is, but the new and emerging AI tools have a high potential to improve ‘decision intelligence’ for better outcomes. The role of humans is unlikely to become redundant with the advancement of AI/ML.
Strong strategic thinking skills combined with sophisticated analytical tools will become a critically important competency of CEOs. It is anticipated that the demand and significance of higher-order thinking skills (i.e., analysis, reasoning, problem solving and decision making) will continue to grow at the global level (Joynes et al., 2019).
AI still needs to be closer to a human ability to solve problems that are not well defined. AI can only perform when it knows what to look for, for example, patterns in a large volume of data to identify insights that can help executives inform their SDM. However, a strategic ‘problem’ or goals need to be articulated first. When a strategy is articulated, then a company needs to decide which type of AI should be used. A real-life example of a Swedish waste management company shows that their sustainability-based strategy was defined first, and then their robotic-based AI system automated their operations which enabled the company to increase their efficiency 20 times and saved them 20,000 euros per month in recycling efficiency (Gudigantala et al., 2022).
Currently, AI is more of a tactical rather than a strategic tool, and AI/ML have a potential to support SDM or be used as building blocks of the strategy to create a competitive edge.
What does the future hold for AI and SDM?
Due to the increasingly complex and ambiguous business environment, the shift from certainty to chaos impacts SDM. In contrast to operational decisions, strategic decisions are pivotal and often irreversible. Traditional decision-making tools such as Monte Carlo simulation, NPV, Decision Trees or Portfolio Optimisation were created for a stable environment, while novel tools like Influence Diagrams, Scenario Planning, Real Options Theory, Systems Thinking or Learning Culture are better suited to organisations experiencing ambiguity and chaos (Wu et al., 2023). The future task of AI will be to enhance the learning capability of ML, so that machines find an optimum solution for us.
There is a consensus of the views that AI will develop predictive analytics which will enable AI to anticipate future events, select best options and scenarios, calculate probabilities like decision trees and in this way inform SDM (Eriksson et al., 2020).
The challenge of formulating the strategic direction falls to senior executives and depends on effective leadership. Because of its complexity, strategy would be one of the later domains to be affected by automation (Atsmon, 2023). Providers of executive education could contribute to developing higher-order cognitive skills and the ‘decision intelligence’ capability of current and emergent leaders.
Conclusion
Strategy formulation is a highly conceptual process, underpinned by strategic thinking and decision making. It requires an understanding of the context and exercising judgment in an increasingly complex VUCA world. Currently, AI is already capable of automating operational decisions, identifying patterns, and providing insights with limited prediction. Still, it needs to be capable of solving undefined problems and generating options for corporate strategies that need to be clearly formulated.
Use of AI raises concerns over risks of over-reliance on AI and probes the trust levels of managers delegating full control to AI/ML. The use of AI/ML is claimed to de-bias decision-making, only ironically, it depends on an analyst setting the parameters for AI tasks, creating a spiral loop of machine-human interaction.
Even with the advancement of strong AI development “humanised AI” will not become a substitute to effective leadership any time soon. Moving forward, the role of executive education will transform from knowledge creation to developing AI-human collaboration capability, enabling executives to think critically and use judgement. Strong strategic thinking skills combined with sophisticated analytical tools will become an essential competency for leaders making strategic decisions for their organisations.",https://www.aimbusinessschool.edu.au/why-abs/research/the-impact-of-ai
AI in Decision Making: Transforming Business Strategies,INTERNATIONAL JOURNAL OF RESEARCH AND SCIENTIFIC INNOVATION (IJRSI),"Simon Kaggwa, Tobechukwu Francisa Eleogu, Franciscamary Okonkwo, Oluwatoyin AjokeFarayola, Prisca Ugomma Uwaoma, Abiodun Akinoso","STUDY This study examines the transformative impact of Artificial Intelligence (AI) on strategic business decision-making, revealing that AI not only disrupts traditional models but also significantly enhances business agility and sustainability, with recommendations for aligning AI integration with core business values and strategic objectives.",2023,"This paper delves into the transformative impact of Artificial Intelligence (AI) on strategic business decision-making, offering a nuanced perspective on how AI is reshaping the corporate world. The primary purpose of this study is to explore the emergence and evolution of AI within the realm of business strategy, examining its role in disrupting traditional decision models and enhancing business agility. This study systematically analyzes academic and industry sources through a meticulous literature review, providing a comprehensive understanding of AI’s multifaceted role in business. The methodology adopted is a systematic literature review, which serves as a robust framework for evaluating source credibility and synthesizing insights. This approach enables a thorough examination of AI’s integration into business management, its influence on corporate performance metrics, and its potential in fostering inclusive business practices. The study also addresses the unique challenges and opportunities presented by AI in the business context. Key findings reveal that AI is not merely a technological tool but a strategic asset that significantly redefines business decision-making. The integration of AI into business strategies demonstrates substantial potential in enhancing corporate performance and promoting sustainable business practices. The study concludes that AI is a cornerstone in business evolution, offering unparalleled opportunities for innovation and efficiency. Recommendations advocate for a balanced approach to AI integration, emphasizing the need for businesses to align AI with their core values and strategic objectives. As AI continues to evolve, its role in business decision-making is expected to shape the corporate landscape significantly.
Keywords: Artificial Intelligence, Business Strategy, Decision-Making, Corporate Performance, Sustainable Business Practices, Technological Innovation
1. INTRODUCTION
1.1. Emergence of AI in Strategic Business Decisions
The integration of Artificial Intelligence (AI) into strategic business decisions marks a transformative era in corporate strategy, reshaping traditional models and enhancing business agility. Kitsios and Kamariotou (2021) emphasize the significant role AI and machine learning have played in recent years, particularly in digital transformation initiatives within organizations. Their systematic literature review reveals a convergence of AI with corporate strategy, highlighting the potential of AI in creating business value. This convergence is not without challenges, particularly in the practical implementation and strategic usage of AI tools, which necessitates a deeper understanding and expertise (Kitsios & Kamariotou, 2021).
The evolution of strategic management processes, as discussed by Stipić (2021), further underscores the importance of AI in corporate decision-making. The study indicates that a methodological approach to strategic planning incorporating AI can significantly influence corporate profitability and success. This integration of AI into strategic planning is crucial for companies to navigate complex business environments effectively and make quality, effective decisions that drive successful business outcomes (Stipić, 2021).
Moreover, the influence of digitalization, including AI, on the formation of corporate strategy and new business models is highlighted by Akmaeva et al. (2020). Their research asserts that the current socio-economic and political processes, accelerated by the pandemic, necessitate a reevaluation of corporate management strategies. They advocate for the integration of cutting-edge technologies like AI in all aspects of corporate activities, emphasizing the shift from traditional hierarchical models to more agile, technology-driven approaches. This shift is crucial for developing new business models and strategies responsive to the rapidly changing business landscape (Akmaeva et al., 2020).
The emergence of AI in strategic business decisions is a pivotal development in the corporate world. It offers immense potential for value creation, necessitates rethinking traditional strategic planning processes, and drives the evolution of new business models. As companies navigate this AI-driven landscape, the focus shifts towards harnessing AI’s potential to enhance decision-making processes, improve corporate performance, and ensure sustainable business growth in an increasingly digitalized world.
1.2. Tracing AI’s Evolution in Corporate Strategy
The evolution of Artificial Intelligence (AI) in corporate strategy has been a journey of transformative impact and continuous adaptation. Over the past decade, AI has evolved from a novel technological concept to a core element in strategic business decision-making, reshaping the way corporations approach challenges and opportunities in the digital era.
Kitsios and Kamariotou (2021) provide a comprehensive overview of this evolution, highlighting the significant advances in machine learning techniques and their integration into business strategies. Their research underscores the potential of AI in solving complex business challenges, while also acknowledging the difficulties in its practical implementation. The lack of expertise in strategically utilizing AI for business value creation is a notable challenge. They propose a theoretical model based on their systematic literature review, which discusses the alignment of AI tools with organizational strategy, knowledge management, decision-making processes, and service innovation.
Mithas, Murugesan, and Seetharaman (2020) delve into the strategic considerations that organizations must make in the age of AI. They pose critical questions about how firms should integrate AI into their digital and information technology strategies. Their work suggests that while AI may not entirely steer a firm’s strategic decisions, its role in modifying the architecture of resources and qualifications within firms is undeniable. This new architecture necessitates an internal reorganization for effective deployment in business process strategies. The authors argue that given the nature of decisions automated by AI, it is imperative for firms to establish governance bodies to define the doctrine for using such technology.
Delbufalo, Di Bernardo, and Risso (2022) explore the interaction between human and machine intelligence in the context of competitive business in the digital era. They emphasize the importance of striking a balance between human and artificial intelligence in decision-making processes. Their study highlights the need for businesses to adapt their models and roles to accommodate the intelligent transformation supported by technological development. This adaptation is crucial for maintaining competitiveness and ensuring sustainable strategies and policies.
The journey of AI in corporate strategy is marked by its ability to enhance decision-making, drive innovation, and create new value propositions. From enhancing machine learning capabilities to redefining the roles of human and machine intelligence, AI’s evolution in corporate strategy reflects a broader shift towards more agile, data-driven, and technologically advanced business practices. As AI continues to evolve, it is likely to further reshape the strategic landscape, offering new opportunities and challenges for businesses worldwide.
 1.3. AI’s Disruption of Conventional Decision Models
The advent of Artificial Intelligence (AI) has significantly disrupted traditional decision-making models in business, leading to a paradigm shift in how strategies are formulated and executed. This disruption is not merely a technological advancement but a fundamental change in organisations’ cognitive and strategic processes.
El-Namaki (2016) explores how AI is being applied to business strategy formulation, emphasizing the shift from human-centric to AI-driven strategic processes. AI’s ability to analyze vast datasets and identify patterns unrecognizable to the human eye has revolutionized product and market strategies. The study presents several case studies across different industries, illustrating how AI is penetrating the strategic realm, leading to changes in both the concept and application of business strategies. The paper proposes a conceptual framework for AI application in business, positioning AI as a tool and a core component of strategic formulation.
Tejeda et al. (2022) delve into the cognitive aspects of AI-assisted decision-making. Their research focuses on understanding the human reliance on AI in joint decision-making scenarios. By developing a cognitive model, they infer the latent reliance strategies of humans on AI assistance, providing insights into how AI influences human decision-making processes. The study reveals that while AI assistance is becoming ubiquitous in decision-making applications, there is a need to understand and manage the dynamics of human-AI collaboration. This understanding is crucial for businesses to leverage AI effectively without undermining human judgment and intuition.
Meske and Bunde (2020) address the critical issue of trust and transparency in human-AI interaction, particularly in decision support systems that utilize computer vision. Their research highlights the ‘black box’ problem in AI, where the complexity of algorithms often leads to a lack of transparency and trust. By employing “Explainable Artificial Intelligence” (XAI), they demonstrate how AI’s decision-making processes can be more transparent, enhancing trust in AI systems. The study underscores the importance of explainability in AI, not just for ethical reasons but also for practical applications in business decision-making.
AI’s disruption of conventional decision models in business is multifaceted, encompassing changes in strategic formulation, cognitive processes, and the need for trust and transparency. AI has transformed the landscape of business strategy, challenging traditional models and necessitating a new approach that harmoniously integrates human intelligence with artificial intelligence. As AI continues to evolve, its impact on decision-making models will likely deepen, offering both opportunities and challenges for businesses in the digital age.
1.4. Enhancing Business Agility through AI
In the contemporary business landscape, agility has become a critical factor for success. The integration of Artificial Intelligence (AI) into business processes has emerged as a key driver in enhancing this agility, transforming the way organizations make decisions and adapt to rapidly changing environments.
Rajagopal et al. (2022) investigate the impact of AI systems on business outcomes, focusing on decision-making processes within organizations. Their study reveals that AI-driven digital frameworks significantly contribute to the precision of decision-making, innovation in policy formulation, and the speed of executing decisions. By analyzing various industries, they demonstrate how AI systems facilitate a more dynamic and responsive business culture. The research underscores the role of AI in refining the decision-making process, particularly in terms of speed and accuracy, thereby enhancing the overall agility of businesses.
Pérez-Campuzano et al. (2021) explore the potential of AI in strategic decision-making within the airline industry, especially during the challenging times of COVID-19. Their review highlights how AI algorithms, particularly Machine Learning (ML) methods, can be utilized for market analysis and cost estimation, thereby aiding in strategic decisions during crises. The study provides a roadmap for implementing AI tools in strategic functions, suggesting that AI can significantly contribute to businesses’ agility in managing resources and analyzing competitive environments effectively.
Prange (2020) delves into the concept of strategic agility, emphasizing decision-making beyond mere speed. The study argues that agility in business is not just about the rapidity of responses but also about the flexibility and transformational capabilities of organizations. AI plays a crucial role in this context by enabling businesses to make informed and swift decisions, thus maintaining a competitive edge in turbulent environments. The research suggests that strategic agility, powered by AI, involves a deeper understanding of market dynamics and the ability to adapt strategies quickly and efficiently.
AI has become a pivotal element in enhancing business agility. By providing advanced analytical capabilities and enabling faster, more accurate decision-making, AI is reshaping the way businesses respond to changes and challenges in their environments. As AI continues to evolve, its role in fostering agile business practices is expected to grow, offering new avenues for strategic innovation and competitive advantage.
 1.5. Conceptual Frameworks: AI in Business Context
The integration of Artificial Intelligence (AI) into business contexts necessitates the development of robust conceptual frameworks to guide its implementation and maximize its potential. These frameworks are essential for understanding how AI can be effectively incorporated into various business processes and decision-making strategies.
Najdawi (2020) addresses the challenge of AI adoption in organizations by proposing a framework to assess AI readiness. This framework is grounded in established theories from the Information Systems discipline, such as the Technology-Organizations-Environment (TOE) framework and the Diffusion of Innovation theory. Najdawi’s study is particularly focused on the socio-technical aspects of AI implementation, emphasizing the need to consider local contexts, as demonstrated in the case of the United Arab Emirates. The proposed framework aims to identify critical factors for successful AI strategy implementation, highlighting the importance of aligning AI initiatives with organizational capabilities and environmental factors.
Solberg et al. (2022) contribute to the conceptual understanding of AI in business by developing a model that connects trust in AI with organizational decision-making. Their model is an extension of Mayer, Davis, and Schoorman’s model of organizational trust, adapted to the context of AI decision aids. This conceptual model emphasizes the role of trust, perceived risk, and reliance on AI in organizational settings. It redefines central concepts and introduces new constructs like perceived control over AI decision aids. This framework is significant for businesses as it provides a structured approach to understanding and managing the human-AI interaction in decision-making processes.
Eisenstadt, Althoff, and Langenhan (2020) explore the application of AI in the early conceptual phases of architectural design, providing insights into how AI frameworks can support creative and complex tasks. Their work on the MetisCBR framework demonstrates the use of modern AI techniques, such as explainable AI and generative adversarial nets, in enhancing the capabilities of AI systems in design-related tasks. This example illustrates the broader applicability of AI frameworks in various business contexts, extending beyond traditional data-driven decision-making processes.
The development of conceptual frameworks for AI in business contexts is crucial for harnessing the full potential of AI technologies. These frameworks provide a structured approach to understanding and implementing AI, ensuring that its integration into business processes is aligned with organizational goals and environmental contexts. As AI continues to evolve, these frameworks will play a vital role in guiding businesses in their AI journey, enabling them to leverage AI for strategic advantage and innovation.
 1.6. Review Purpose and Goals
The primary aim of this review is to critically examine the integration and impact of Artificial Intelligence (AI) in strategic business decision-making, highlighting its transformative role in modern business practices.
Objectives:
To explore how AI technologies are reshaping strategic decision-making processes in various business sectors.
To assess the effectiveness of AI in enhancing competitive advantage and operational efficiency in businesses.
To identify the challenges and risks associated with the implementation of AI in business strategies.
To investigate the ethical considerations and societal impacts of deploying AI in business contexts.
To forecast future trends and developments in AI applications within the realm of business strategy and management.
1.6.1. Key Research Queries
Several key research queries emerge in exploring the integration of Artificial Intelligence (AI) in strategic business decision-making. Firstly, we seek to understand how AI is transforming traditional business decision-making models, marking a shift from conventional methods to more AI-driven approaches. This involves examining the extent to which AI influences strategic decisions and alters long-standing business practices.
Additionally, it is crucial to quantify the impact of AI on business performance and competitiveness. This query delves into measuring the tangible benefits and enhancements that AI brings to businesses, including improved efficiency, accuracy, and innovation.
Another significant aspect is identifying the challenges and obstacles businesses face when integrating AI into their strategies. This encompasses both technical and organizational hurdles, as well as addressing potential resistance to change.
Furthermore, the ethical considerations and societal implications of using AI in business contexts cannot be overlooked. This involves scrutinizing the moral responsibilities and potential societal impacts associated with deploying AI technologies in business operations.
Lastly, anticipating future developments in AI applications for business strategy is essential. This query looks forward to emerging trends, potential advancements, and the evolving landscape of AI in the realm of business, preparing organizations for the next wave of technological innovation.
 1.6.2. Exploring Uncharted Territories in AI and Business Strategy
The exploration of uncharted territories in the integration of Artificial Intelligence (AI) and business strategy reveals a landscape rich with potential and challenges. This section delves into how AI is pushing the boundaries in various business sectors, reshaping traditional practices, and creating new paradigms for success.
Esqueda and Melo (2020) provide an intriguing perspective on the impact of AI in the gaming industry, particularly in the context of game localization and translation. Their study highlights how AI-driven strategies are essential in navigating complex sociocultural and linguistic borders, thereby expanding the reach of products in global markets. This example serves as a microcosm of how AI can be leveraged in various business sectors to transcend traditional barriers, offering insights into the broader implications of AI in global business strategies.
Chuang et al. (2020) explore the journey of Creative Design Ltd., a start-up that ventured into uncharted business territories. Their case study illustrates how AI and innovative business models can be synergized to seize opportunities for innovation, even in unfamiliar domains. This narrative underscores the importance of AI in enabling businesses to adapt and thrive in rapidly changing environments, highlighting the transformative power of AI in fostering entrepreneurship and business growth.
Shaheen, Arshad, and Iqbal (2020) examine the role of AI and Machine Learning in the transportation sector, showcasing how these technologies are revolutionizing traditional business models. Their research emphasizes the significance of AI in enhancing decision-making processes, particularly in managing and analyzing vast amounts of data for intelligent transportation systems. This study exemplifies the diverse applications of AI across different business sectors, demonstrating its potential to optimize operations and improve efficiency.
The exploration of uncharted territories in AI and business strategy reveals a dynamic interplay between technology and business. AI is not only transforming existing business models but also enabling the creation of new ones, driving innovation, and enhancing competitiveness across various sectors. As businesses navigate these new frontiers, they are faced with both opportunities and challenges, underscoring the need for strategic planning and adaptation in the age of AI.
 1.6.3. Boundaries and Focus of the Analysis
In this review, the boundaries and focus of the analysis are carefully delineated to provide a coherent and comprehensive understanding of AI’s role in strategic business decision-making. The analysis primarily concentrates on the application of AI within the realm of business strategy, emphasizing its impact on decision-making processes, competitive advantage, and operational efficiency.
The scope is specifically tailored to explore how AI technologies are being integrated into various business models and sectors, highlighting both the transformative potential and the challenges inherent in such integration. This includes an examination of AI’s influence on data-driven decision-making, customer engagement, and market analysis.
Furthermore, the review pays special attention to the ethical implications and societal impacts of AI in business. This involves a critical examination of privacy concerns, data security, and the potential for AI to disrupt traditional employment patterns.
Lastly, the analysis remains focused on current trends and future projections, aiming to provide insights into how AI is likely to evolve within the business context and what this means for future strategic planning and innovation. This forward-looking perspective is crucial for businesses aiming to stay ahead in an increasingly AI-driven world.
2. RESEARCH METHODOLOGY
2.1. Blueprint for AI-Business Literature Exploration
The exploration of AI in business strategy necessitates a structured approach to literature review, ensuring comprehensive coverage and depth of analysis. Gina and Budree (2020) emphasize the importance of identifying critical factors that drive tool selection in business intelligence, which is analogous to selecting literature in AI-business research. This involves categorizing studies based on their relevance to AI’s role in business strategy, focusing on both technical and non-technical factors that influence AI’s integration into business processes.
Yin and Fernandez (2020) advocate for a systematic review approach, which is instrumental in this context. Their methodology, involving well-established criteria for literature selection from recognized databases, ensures the inclusion of relevant and high-quality studies. This approach aids in synthesizing a broad spectrum of research, ranging from AI’s technical applications to its strategic implications in business.
2.2. Criteria for AI-Business Study Selection
The criteria for selecting studies in AI-business research are multifaceted. Stojanović et al. (2016) propose a model for selecting business process improvement methodologies, which can be adapted for selecting AI-business literature. This model suggests focusing on criteria such as the relevance to AI in business strategy, the methodological rigor of the studies, and their practical implications in the business context.
Júnior et al. (2020) demonstrate the importance of a systematic literature review in understanding enterprise architecture in healthcare systems. Their approach, involving a rigorous selection process and assessment of the level of disagreement in evaluations, can be mirrored in AI-business research. This ensures a balanced and comprehensive understanding of AI’s role in business, encompassing various methodologies, tools, best practices, and criteria for selection.
The methodology for AI-business literature exploration and the criteria for study selection are grounded in a systematic, rigorous approach. This ensures a thorough understanding of AI’s multifaceted role in business strategy, encompassing both its technical capabilities and strategic implications.
2.3. Dissecting Themes in AI Business Research
The thematic analysis in AI business research involves identifying and exploring key themes that emerge from the literature. Vergara-Villegas et al. (2021) provide an excellent example of this approach in their thematic issue on AI for Industry 4.0. They demonstrate how thematic analysis can be used to categorize and understand the various applications of AI in industry, from IoT architectures to decision-making processes. This approach is crucial for identifying the diverse ways in which AI impacts business strategies and operations.
Similarly, Anderson, Lees, and Avery’s (2015) work on the Thematic Analysis Grid offers a practical tool for synthesizing key themes from academic literature. This method allows for a structured approach to literature review, enabling researchers to identify both consensus and contradictions within the field. By applying this grid to AI business research, one can effectively map out the landscape of current knowledge, identifying gaps and emerging trends.
2.4. Synthesis Method for AI Business Insights
The synthesis of AI business insights requires a method that can integrate diverse findings into a coherent narrative. Kreines and Kreines (2019) discuss the use of AI tools for objective analysis of scientific texts, which can be adapted for synthesizing business research. Their approach involves using AI to map out the scientific landscape, identifying key topics and trends. This method can be particularly useful in synthesizing AI business research, where the volume of literature is vast and the topics are diverse.
The synthesis process involves not only aggregating findings but also critically evaluating and interpreting them in the context of business strategy and operations. This requires a deep understanding of both the technical aspects of AI and its practical implications in the business world. The end goal is to provide actionable insights that can inform business decisions and strategies in an AI-driven marketplace.
3. FINDINGS
3.1. Strategies for AI-Integrated Business Management
The integration of Artificial Intelligence (AI) into business management strategies represents a significant shift in how corporations approach decision-making and operational efficiency. This section explores the strategies for AI-integrated business management, drawing insights from recent scholarly contributions.
Lin, Lin, and Yang (2017) discuss an innovative approach to estimate corporate risk and profit using AI techniques. Their study introduces a model combining random projection and data envelopment analysis, integrated with AI-based techniques for forecasting firm performance ranking. This approach underscores the potential of AI in enhancing corporate decision-making, particularly in dynamic environments where traditional models may fall short (Lin, Lin, & Yang, 2017).
Amine Belhadi et al. (2022) focus on the application of AI in building resilient supply chains. They propose an integrated Multi-criteria decision-making technique powered by AI algorithms to develop supply-chain resilience strategies. This study highlights the role of AI in optimizing supply chain management, a critical aspect of business operations. The use of fuzzy logic programming, machine learning, and agent-based systems demonstrates the versatility of AI in addressing complex business challenges (Belhadi, Kamble, Wamba, & Queiroz, 2022).
Vagin et al. (2022) explore the role of AI and Big Data in environmental decision-making within business management. Their research reveals the key role of these technologies in optimizing decisions that balance economic and environmental interests. This study provides insights into how AI can be leveraged for sustainable business practices, an increasingly important aspect of corporate strategy (Vagin, Klimenko, Telegina, & Aleksashina, 2022).
The integration of AI into business management strategies is multifaceted, encompassing risk assessment, supply chain resilience, and sustainable decision-making. These studies illustrate the transformative potential of AI in enhancing corporate decision-making processes, offering new avenues for strategic planning and operational efficiency.
 3.2. AI’s Role in Business Skill Enhancement
The advent of Artificial Intelligence (AI) in the business landscape has ushered in a new era of workforce development, where the enhancement of business skills is increasingly intertwined with technological proficiency. This section delves deeper into how AI is reshaping skill development in the business world.
Bai-Ngern and Tubtiang (2020) highlight the critical role of leadership skills in navigating the digital era. They argue that the integration of AI in business processes demands a new breed of leaders who are not only tech-savvy but also possess the ability to foster innovation and collaboration in AI-enhanced environments. This necessitates a shift in leadership training programs to include AI literacy, emphasizing the importance of understanding AI capabilities and limitations. The study suggests that such training should focus on developing strategic thinking skills that leverage AI for competitive advantage.
ElZomor et al. (2020) delve into the transformative potential of robotic automation and artificial intelligence technologies, particularly in the Architecture, Engineering, and Construction sectors. Their study highlights the role of these technologies in reshaping job skills and supporting economic growth, thereby empowering a diverse workforce. This research underscores the significance of AI-driven educational tools in enhancing learning experiences, thereby contributing to inclusive growth and diversity in the workplace.
Smolarek and Scrivener (2021) provide an insightful analysis of the UpSkill Houston initiative, a business-driven education reform program. Their work sheds light on how AI and digital technologies are being utilized to align workforce skills with the evolving needs of the industry. The initiative underscores the importance of developing a curriculum that is responsive to the dynamic demands of the labor market, particularly in sectors heavily influenced by AI and automation. The study highlights the role of AI in providing real-time labor market data, which can be used to inform and adapt educational programs accordingly.
Furthermore, the integration of AI in business skill enhancement is not limited to technical skills alone. There is a growing recognition of the need for ‘soft skills’ such as emotional intelligence, critical thinking, and adaptability in the AI-augmented workplace. AI-driven analytics can help identify skill gaps and tailor training programs to address these needs. This holistic approach to skill development ensures that employees are not only technically proficient but also equipped to handle the complex interpersonal and ethical challenges posed by AI technologies.
AI’s role in business skill enhancement is multifaceted, encompassing the development of both technical and soft skills. The studies reviewed here illustrate the transformative potential of AI in reshaping workforce development, highlighting the need for innovative approaches to training and education in the digital age. As businesses continue to integrate AI into their operations, the focus on skill enhancement will be crucial in maintaining a competitive edge and fostering a resilient, adaptable workforce.
3.3. AI’s Influence on Corporate Performance Metrics
The integration of Artificial Intelligence (AI) into corporate business and financial management has significantly altered the landscape of performance metrics and analysis. This section explores the transformative impact of AI on corporate performance metrics, drawing on recent scholarly research.
Rasheed, Ishaq, and Rehman (2021) conducted a comprehensive analysis of AI’s role in financial and corporate business management in Pakistan. Their study reveals that AI applications have profoundly influenced all aspects of financial management and corporate business activities. The research indicates that 80% of senior business executives believe AI boosts productivity and creates new business avenues. Furthermore, the study categorizes organizations into four maturity clusters based on their AI adoption: Pioneers, Investigators, Experimenters, and Passives. This categorization provides a nuanced understanding of how different organizations leverage AI for performance enhancement. The findings suggest that Pioneer organizations, which fully understand and adopt AI, have seen significant improvements in their performance metrics.
Lin and Chang (2021) explore the relationship between corporate governance and dynamic business performance, particularly in the semiconductor industry. Their work introduces AI as a transformative tool for improving corporate governance, which in turn impacts corporate performance. The study proposes the use of AI for automatic warning systems in corporate governance, suggesting that AI can play a crucial role in identifying and mitigating risks, thereby enhancing overall performance metrics. This approach underscores the potential of AI in enhancing transparency and accountability in corporate governance, leading to improved performance outcomes.
Jiang et al. (2021) present a methodology for benchmarking High-Performance Computing (HPC) AI systems, focusing on their application in business and scientific computing. The study introduces a new metric, Valid FLOPS, which emphasizes both throughput performance and target quality. This metric is particularly relevant in the context of AI’s impact on corporate performance, as it provides a comprehensive measure of AI system efficiency. The research highlights the importance of benchmarking AI systems to understand their impact on corporate performance metrics accurately.
The integration of AI in corporate business and financial management has led to a paradigm shift in how performance metrics are analyzed and interpreted. The studies reviewed here illustrate the diverse ways in which AI is influencing corporate performance, from enhancing productivity and creating new business opportunities to improving corporate governance and developing comprehensive benchmarking methodologies. As AI continues to evolve, its impact on corporate performance metrics is likely to deepen, offering new insights and opportunities for businesses to optimize their operations and strategies.
 3.4. AI as a Catalyst for Inclusive Business Practices
The integration of Artificial Intelligence (AI) into business practices has not only revolutionized operational efficiencies but also opened new avenues for inclusive and socially responsible business models. This section delves into how AI is acting as a catalyst for inclusive business practices, drawing insights from recent scholarly research.
Negro (2019) provides a comprehensive understanding of the intersection between corporate social responsibility (CSR) and inclusive business in the Peruvian business environment. The study emphasizes the role of AI in enhancing these concepts, particularly in developing economies. AI, as Negro suggests, can be instrumental in identifying and addressing social and economic disparities, thereby fostering inclusive business practices. This is particularly relevant in the context of poverty reduction and sustainable development, where AI can analyze vast datasets to identify areas needing urgent attention and aid in the formulation of targeted strategies.
Mai and Nguyen (2021) explore the impact of CSR on competitive advantage and business success, highlighting the role of AI in enhancing these aspects. Their study, conducted in the Vietnamese context, indicates that AI-driven CSR initiatives, particularly in ethical, legal, and philanthropic domains, positively affect competitive advantage. AI’s ability to process and analyze large volumes of data can help businesses in making informed decisions regarding CSR investments, thereby enhancing their overall success and reputation.
Arenas-Torres, Bustamante-Ubilla, and Campos-Troncoso (2021) investigate the incidence of ethics and CSR on diverse business practices in Chilean companies. Their findings reveal an incipient level of standardization in adopting social responsibility practices, with AI playing a significant role in this transformation. AI technologies enable companies to integrate CSR into their core operations more effectively, leading to a positive and significant impact on practices related to diversity, environment, and community.
AI emerges as a powerful tool in driving inclusive business practices and enhancing corporate social responsibility. The studies reviewed here illustrate AI’s potential in identifying social and economic disparities, aiding in CSR decision-making, standardizing social responsibility practices, and aligning business operations with global sustainability goals. As AI continues to evolve, its role in fostering inclusive and responsible business models is expected to become more pronounced, offering new pathways for businesses to contribute positively to society.
3.5. Navigating Business Hurdles with AI Tools
In the rapidly evolving landscape of business and technology, artificial intelligence (AI) has emerged as a pivotal tool for navigating various business hurdles. The integration of AI into business practices is not just about technological advancement but also involves a deep understanding of ethics, corporate culture, and sustainability. Attard-Frost, De los Ríos, and Walters (2023) highlight the importance of considering the ethics of AI business practices, emphasizing that AI ethics guidelines should extend beyond algorithmic decision-making to include the political and economic implications of AI in business. This perspective is crucial in understanding how AI can be used responsibly and ethically in a business context.
The role of leadership and corporate culture in embracing AI for digital transformation is underscored by Frost, Jeske, and Ottersböck (2020). They argue that the successful implementation of AI and hybrid business models requires a change in management and corporate culture. This change is not just technological but also involves the development of skills and competencies among managers and employees. The study suggests that many companies overlook the need to develop these skills, which is essential for thriving in the digital era.
Sustainability is another critical aspect of integrating AI into business practices. Isensee, Griese, and Teuteberg (2021) explore how corporate culture influences the use of AI in terms of sustainable development. They introduce the concept of Sustainable Artificial Intelligence (SAI), which includes a normative element and is influenced by the features of a sustainability-oriented corporate culture. The study offers insights into how specific manifestations of corporate culture can impact the handling of AI in the sense of SAI, indicating that corporate culture is both an indicator and an influencing factor in ensuring the sustainable use of AI.
The integration of AI into business practices presents several challenges and opportunities. One of the primary challenges is the ethical consideration of AI deployment. As AI systems become more complex and influential in decision-making processes, businesses must ensure that these systems are fair, accountable, transparent, and sustainable. This requires a holistic approach to AI ethics that goes beyond technical aspects and considers the broader business practices and political economies in which these systems operate.
Another significant challenge is the transformation of corporate culture to support AI integration. The shift towards AI-driven business models necessitates a change in the mindset and skills of both leaders and employees. Companies must invest in training and development programs to equip their workforce with the necessary skills to leverage AI effectively. This involves fostering a culture of continuous learning and innovation, where employees are encouraged to explore new technologies and apply them in their work.
Sustainability is an increasingly important consideration in business practices, and AI offers both opportunities and risks in this regard. On the one hand, AI can contribute to sustainability goals by optimizing resource use, reducing waste, and improving efficiency. On the other hand, the deployment of AI systems can have negative environmental impacts, such as high energy consumption for computing power. Therefore, businesses must adopt a balanced approach to AI, ensuring that its use aligns with sustainability principles and contributes positively to environmental and social goals.
Navigating business hurdles with AI tools requires a multi-faceted approach that considers ethical, cultural, and sustainability aspects. Businesses must develop comprehensive strategies that address these dimensions to harness the full potential of AI in a responsible and sustainable manner. This involves not only technological innovation but also a transformation in business practices, corporate culture, and ethical frameworks. By doing so, businesses can effectively leverage AI to overcome challenges and drive growth in the digital age.
 3.5.1. Cultivating an AI-Ready Corporate Culture
The advent of artificial intelligence (AI) in the business world has necessitated a transformation in corporate culture, especially in the context of new management technologies. Svistunov, Kuzina, and Lobachev (2021) emphasize the inevitability of this transformation, highlighting the relationship between employee satisfaction and the level of digitalization in a company. They argue that as companies increasingly adopt modern information technology tools, including AI, there is a corresponding need to adapt the corporate culture to maintain and enhance employee satisfaction and creativity.
The role of corporate culture in sustainable development, particularly under the impacts of the Fourth Industrial Revolution, is further explored by Nguyen, M.Q., Nguyen, T.K.C. and Pham (2022). They discuss how corporate culture can act as an instrument for responsible and sustainable business practices in the age of digital transformation. The study highlights that corporate culture is not just a passive element but an active driver that can empower employees and foster a conducive environment for embracing AI and other digital technologies.
In cultivating an AI-ready corporate culture, companies must focus on several key areas:
Employee Empowerment and Satisfaction: As AI and digital technologies transform business processes, it is essential to ensure that employees feel empowered and satisfied with their work. This involves providing them with the necessary training and resources to adapt to new technologies and encouraging a culture of innovation and creativity.
Leadership and Management Practices: Effective leadership is crucial in steering the cultural transformation towards AI readiness. Leaders must be proactive in understanding the implications of AI and digital technologies and in guiding their teams through the transition. This includes fostering open communication, encouraging feedback, and being receptive to new ideas and approaches.
Balancing Automation with Human Elements: While automation and efficiency are key benefits of AI, it is important to balance these with the human aspects of the workplace. This means recognizing the value of human creativity, intuition, and emotional intelligence, and finding ways to integrate these with AI-driven processes.
Sustainability and Ethical Considerations: An AI-ready corporate culture should also be aligned with sustainability and ethical principles. This involves considering the environmental and social impacts of AI and digital technologies and ensuring that their deployment is in line with the company’s broader sustainability goals.
Adaptability and Continuous Learning: The fast-paced nature of technological advancements requires a corporate culture that is adaptable and focused on continuous learning. Employees should be encouraged to continuously update their skills and knowledge, and the organization should remain flexible and open to change.
In conclusion, cultivating an AI-ready corporate culture is a multifaceted process that requires a holistic approach. It involves not only the integration of new technologies but also a transformation in the way companies think about and manage their human resources, leadership practices, and sustainability goals. By focusing on these areas, companies can create a conducive environment for leveraging AI effectively and responsibly, leading to sustainable growth and innovation in the digital era.
3.5.2 AI in Crafting Sustainable Business Futures
The integration of Artificial Intelligence (AI) into business strategies is not just a matter of technological advancement but also a step towards sustainable future practices. The potential of AI in shaping sustainable business models is immense, particularly in sectors like the agri-food industry, environmental policy, and education.
Raji (2022) highlights the transformative role of AI in the agri-food industry, a sector grappling with economic and production crises. AI technologies have revolutionized this field by reducing human intervention and increasing production efficiency. The application of AI in the agri-food sector exemplifies a sustainable model for crisis management, where AI’s capabilities in learning, perception, problem-solving, and reasoning are leveraged to create resilient and efficient business practices. This approach not only addresses immediate production challenges but also sets a precedent for sustainable business models in other sectors facing similar crises.
The work of Perucica and Andjelkovic (2022) takes a broader view, examining the interdependence of AI and environmental sustainability within the European Union (EU). Their study underscores the need for comprehensive sustainable AI policy initiatives, highlighting the EU’s approach as a model that balances technological advancement with environmental stewardship. This perspective is crucial in understanding how AI can be developed and utilized in a manner that is not only technologically innovative but also environmentally responsible. The concept of “sustainable by design” AI, proposed by Perucica and Andjelkovic, emphasizes the importance of creating AI systems that are transparent, responsible, and centered around human values, ensuring that AI contributes positively to the environmental aspect of sustainability.
In the context of education, Paulauskaite-Taraseviciene et al. (2022) discuss the importance of integrating sustainability into engineering and AI study programs. Their research focuses on developing a methodology to assess the sustainability level in engineering studies, using AI as a case study. This approach is vital for preparing future generations to address sustainability challenges using AI. By embedding sustainability goals into AI and IT study programs, educational institutions can ensure that the next generation of AI professionals is equipped with the knowledge and skills to develop AI solutions that are sustainable and beneficial for society.
The synthesis of these studies presents a comprehensive view of AI’s role in crafting sustainable business futures. From enhancing production efficiency in the agri-food industry to shaping environmentally responsible AI policies and educating future generations, AI emerges as a key player in the journey towards sustainability. The challenge lies in ensuring that AI development is aligned with sustainable principles, balancing technological innovation with environmental and social responsibilities.
AI’s potential in driving sustainable business practices is evident across various sectors. The integration of AI into business strategies must be done with a keen awareness of its impact on sustainability, ensuring that AI serves as a tool for positive change and a sustainable future.
4. ANALYSIS AND DISCUSSION
4.1. Decoding AI’s Impact on Business Choices
The integration of Artificial Intelligence (AI) into business decision-making processes has become a pivotal aspect of modern corporate strategy, fundamentally altering the landscape of organizational management and operational efficiency. This transformation is not merely a technological upgrade but a strategic shift that influences the core of business decision-making.
Rajagopal et al. (2022) emphasize the transformative role of AI in shaping the future of business culture. They argue that AI-driven digital frameworks are pivotal in enhancing the decision-making process within organizations. The adoption of AI systems facilitates a more precise and efficient approach to decision-making, leveraging data analytics and machine learning to provide insights that were previously unattainable. This shift is not just about the automation of routine tasks but about enabling a more informed and strategic approach to decision-making, one that is capable of handling complex scenarios with greater agility and accuracy.
Tabesh (2022) delves into the dynamics of managerial decision-making in the era of AI. The study highlights how AI and machine learning technologies are revolutionizing traditional decision-making processes, offering new tools for analysis and intuition. Managers are now equipped with advanced AI algorithms that enhance their analytical capabilities, allowing for a more nuanced understanding of business environments. However, Tabesh also points out the importance of maintaining a balance between human intuition and machine-driven analytics, suggesting that while AI can augment decision-making, it should not replace the human element entirely.
Attard-Frost et al. (2023) bring an ethical perspective to the discussion, reviewing the implications of AI business practices. Their work underscores the need for ethical considerations in the deployment of AI technologies in business. The study suggests that while AI can significantly improve decision-making processes, it is crucial to address the ethical challenges that arise, such as data privacy, algorithmic bias, and transparency. These ethical considerations are integral to ensuring that AI is used responsibly and sustainably in a business context.
The impact of AI on business choices extends beyond operational efficiency and into the realm of strategic planning and long-term organizational development. AI technologies provide businesses with the tools to analyze vast amounts of data, identify trends, and predict future market movements, enabling more strategic and forward-thinking decision-making. This capability is particularly crucial in today’s fast-paced business environment, where the ability to quickly adapt and respond to changing market conditions is a key determinant of success.
Moreover, AI’s role in decision-making is not limited to large corporations. Small and medium-sized enterprises (SMEs) also stand to benefit significantly from AI integration. By leveraging AI tools, SMEs can gain insights and efficiencies that were previously the domain of larger organizations with more substantial resources. This democratization of technology allows for a more level playing field in the business world, where smaller players can compete more effectively with larger counterparts.
the integration of AI into business decision-making processes represents a significant shift in how organizations approach strategy and operations. This shift is characterized by increased efficiency, enhanced analytical capabilities, and the ability to make more informed and strategic decisions. However, it is essential to navigate this transformation with an awareness of the ethical implications and a commitment to maintaining a balance between human intuition and machine-driven analytics. As AI continues to evolve, its role in shaping business strategies and choices will undoubtedly become even more pronounced, offering new opportunities and challenges for organizations across various sectors.
 4.2. AI in Business: A Sector-Wide Comparison
The integration of Artificial Intelligence (AI) into various business sectors has led to significant transformations in how companies operate and compete. This section provides a comparative analysis of AI’s impact across different sectors, drawing insights from recent research.
Dobre et al. (2020) provide a unique perspective by comparing the evolution of the AI sector with the Dotcom era, highlighting the resilience and growth potential of AI despite early-stage risks. This comparison is crucial in understanding how AI, unlike Dotcom, is deeply intertwined with key sectors of the global economy, suggesting a more sustainable growth trajectory. The study also introduces the concept of Schumpeterian creative destruction in the context of AI, where larger firms absorb innovative startups, thereby shaping the competitive landscape.
Papadimitriou et al. (2017) delve into the knowledge-intensive aspects of firms in high-tech sectors and knowledge-intensive business services. Their research underscores the importance of knowledge intensiveness in driving innovation within these sectors. This aspect is particularly relevant in the context of AI, where the ability to leverage and create knowledge is a key competitive advantage.
Makedon et al. (2021) focus on the global robotics market, a critical component of the AI sector. Their findings highlight the transformative impact of AI and robotics on production and consumption patterns, signaling a new technological order. The study provides insights into the growth dynamics of the robotics market and its role in reshaping the global innovation system. This analysis is instrumental in understanding how AI-driven technologies like robotics are creating new market structures and altering the roles of various economic players.
Indrani et al. (2020) explore pricing decisions in different industry sectors, offering a perspective on how AI influences strategic decision-making processes like pricing. Their findings reveal considerable variations in pricing strategies across sectors, influenced by the specific characteristics of products, markets, and business environments. This study is pertinent in illustrating how AI can be leveraged to optimize pricing decisions, a key aspect of business strategy.
The integration of AI in business sectors is not uniform but varies significantly based on the sector’s characteristics and the nature of AI applications. From the resilience and growth potential in the AI sector, knowledge intensiveness in high-tech and service sectors, transformative impact in robotics, to strategic decision-making in pricing, AI’s role is multifaceted and sector-dependent. These insights are crucial for businesses aiming to harness AI effectively, as they navigate the unique challenges and opportunities presented by this transformative technology.
4.3. Unique AI Implementation Challenges in Business
The integration of Artificial Intelligence (AI) into business strategies presents a complex landscape of challenges and opportunities. As Ruokonen and Ritala (2023) elucidate, the adoption of an AI-first strategy is not a straightforward path but requires a nuanced understanding of the firm’s position and capabilities. They categorize firms into three strategic types: digital tycoons, niche carvers, and asset augmenters, each facing unique challenges in leveraging AI for strategic advantage. Digital tycoons, often already entrenched in technology, must focus on maintaining their lead through continuous innovation. Niche carvers, on the other hand, must find unique ways to apply AI to specialized markets, while asset augmenters need to integrate AI into existing assets to enhance their value.
Owoc, Sawicka, and Weichbroth (2019) extend this discussion to the education sector, highlighting the dynamic nature of AI implementation across different business environments. They emphasize that the successful adoption of AI technologies requires not only an understanding of the technology itself but also a keen awareness of the sector-specific challenges and opportunities. This includes recognizing the benefits of AI in enhancing learning experiences and operational efficiency, as well as navigating the challenges in implementation, such as ensuring data privacy and addressing the potential for AI to perpetuate biases.
Fenwick, Vermeulen, and Corrales (2018) delve into the regulatory and ecosystem challenges of AI in business. They argue that dynamic regulation and innovation ecosystems are crucial in navigating the AI landscape. Dynamic regulation, which includes regulatory sandboxes, allows for experimentation and adaptation in a rapidly evolving field. Innovation ecosystems, on the other hand, foster collaborations between established corporations and AI-focused startups, creating synergies that can drive both technological advancement and business growth. These ecosystems are particularly important in sectors like Fintech, where AI’s disruptive potential is significant.
The strategic management of disruptive AI technology, therefore, involves a multifaceted approach. Businesses must not only develop AI capabilities but also create an environment where these technologies can be effectively integrated into existing processes and business models. This requires a balance between technological innovation, regulatory compliance, and ecosystem collaboration.
Moreover, the implementation of AI in business is not just a technical challenge but also a cultural one. Organizations must cultivate a culture that embraces change, encourages innovation, and is agile enough to adapt to the rapid developments in AI technology. This cultural shift is essential for businesses to fully realize the potential of AI and transform it from a mere tool into a strategic asset that can drive long-term growth and sustainability.
The unique challenges of AI implementation in business are as diverse as the sectors in which AI is being applied. From strategic alignment and sector-specific considerations to regulatory compliance and ecosystem development, businesses must navigate a complex landscape. The successful integration of AI into business strategies requires a holistic approach that considers not only the technological aspects but also the cultural, regulatory, and collaborative dimensions of this transformative technology.
 4.3.1. AI as a Strategic Asset in Business Evolution
The integration of Artificial Intelligence (AI) into business strategies has become a pivotal aspect of modern business evolution. Ruokonen and Ritala (2023) explore the concept of an AI-first strategy, identifying three distinct approaches: digital tycoon, niche carver, and asset augmenter. These strategies aim to leverage AI for data, algorithmic, and execution advantages, each with its own strategic bottlenecks and risks. This approach underscores the importance of AI in shaping business strategies and the need for firms to adapt their strategic plans to prioritize AI.
Tariq, Poulin, and Abonamah (2021) delve into the operational excellence achievable through AI, highlighting the driving forces and barriers in this journey. Their work emphasizes AI’s role in enhancing operational management, decision-making, and overall efficiency in the production of goods and services. The study also addresses the challenges businesses face, such as cultural constraints and strategic planning issues, in adopting AI. This perspective is crucial in understanding the operational impact of AI and the necessary steps for successful integration into business strategies.
Ćormarković, Dražeta, and Njeguš (2022) discuss the application levels of AI in human resource systems, moving beyond traditional HRM functions to strategic business partnerships. The paper highlights the evolution of HRM systems facilitated by AI, focusing on the technological features and goals that enhance organizational capabilities. This exploration into AI applications in HRM systems provides insights into how AI can be a strategic asset, not just in operational processes but also in managing and leveraging human capital.
AI’s role as a strategic asset in business evolution is multi-faceted, impacting various aspects of business operations, from strategic planning to human resource management. The studies by Ruokonen and Ritala (2023), Tariq, Poulin, and Abonamah (2021), and Ćormarković, Dražeta, and Njeguš (2022) collectively provide a comprehensive view of how AI can be strategically integrated into business models, highlighting both the potential and the challenges of this integration. As businesses continue to evolve in an increasingly digital world, the strategic incorporation of AI will be a critical determinant of their success and competitiveness.
4.4. Charting the Future of AI in Business Decision Processes
The integration of Artificial Intelligence (AI) into business decision-making processes marks a significant shift in the landscape of organizational strategy and operations. Rajagopal et al. (2022) delve into this transformation, exploring how AI-driven frameworks are reshaping the future of business culture. Their research highlights the pivotal role of AI in enhancing decision-making processes within organizations. By leveraging AI systems, businesses can achieve a higher precision in their decision-making, contributing innovatively to both the process and its outcomes. This shift is not just about the automation of decisions but also about the enhancement of strategic choices through AI’s analytical capabilities.
Ilieva et al. (2021) further expand on this concept by examining the role of AI and machine learning (ML) in business intelligence (BI). They argue that AI is at the core of next-generation analytics, empowering BI with predictive modeling and data-driven insights. This integration of AI into BI platforms is not just a technological upgrade but a strategic move that enables businesses to anticipate market trends, customer behaviors, and operational inefficiencies. The predictive capabilities of AI in e-commerce, for instance, allow for more accurate forecasting and customer segmentation, leading to more effective marketing strategies and improved customer experiences.
Simões et al. (2022) contribute to this discussion by conducting a systematic review of future trends in AI and digital transformation. Their findings categorize the impact of AI across various industry fields, including healthcare, education, and tourism, demonstrating AI’s versatility in driving digital transformation. This comprehensive analysis underscores the importance of AI in shaping future business strategies and operations. By using AI as a tool for value aggregation, businesses can prepare better for future challenges and opportunities.
The integration of AI into business decision processes is not without its challenges. One of the key concerns is the balance between human and AI-driven decision-making. Rajagopal et al. (2022) address this by suggesting a novel paradigm for comparative analysis. They propose a model where both human and AI-based decision-making coexist, complementing each other to enhance overall organizational judgment efficiency. This model emphasizes the importance of participative management, where internal stakeholders are involved in managing the correlation between AI technologies and business decisions.
Another challenge is the ethical and responsible use of AI in decision-making. As AI systems become more sophisticated, there is a growing need for businesses to ensure that these systems are used ethically and responsibly. This includes considerations around data privacy, bias in AI algorithms, and the broader societal impact of AI-driven decisions. Businesses must navigate these challenges carefully to harness the full potential of AI in decision-making processes.
Significant opportunities and challenges mark the future of AI in business decision processes. As AI continues to evolve, it will play an increasingly vital role in shaping business strategies and operations. Businesses that successfully integrate AI into their decision-making processes will be better positioned to adapt to changing market dynamics, anticipate future trends, and make more informed strategic choices. However, this integration must be approached with a keen awareness of the ethical and practical challenges that come with AI. By doing so, businesses can leverage AI not just as a tool for automation but as a strategic asset that drives innovation, efficiency, and growth.
5. CONCLUSION
The exploration of Artificial Intelligence (AI) in strategic business decision-making, as presented in this study, comprehensively addressed its stated aims and objectives through a detailed literature review. The primary aim was to dissect the emergence, evolution, and impact of AI in shaping modern business strategies. This goal was achieved through an exhaustive analysis of various academic and industry sources, providing a nuanced understanding of AI’s role in the business context.
The methodology adopted, a systematic literature review, proved crucial in revealing the multifaceted nature of AI in business. It offered a robust framework for evaluating the credibility of sources and synthesizing insights, ensuring a comprehensive understanding of AI’s role in business strategy and decision-making. This approach facilitated an in-depth examination of themes such as AI’s disruption of traditional decision models, its contribution to enhancing business agility, and its role in business skill enhancement.
Findings from this study indicate that AI transcends its role as a mere technological tool, emerging as a strategic asset that redefines the landscape of business decision-making. The integration of AI into business management strategies has demonstrated significant potential in enhancing corporate performance metrics and fostering inclusive business practices. Moreover, the study underscored the unique challenges and opportunities presented by AI, highlighting the necessity for developing an AI-ready corporate culture and sustainable business futures.
In conclusion, AI is identified as a pivotal element in the evolution of business strategies, offering unparalleled opportunities for innovation and efficiency. The journey of integrating AI into business practices is complex, necessitating a strategic alignment with core values and objectives. Recommendations from this study advocate for a balanced approach, where AI is embraced not only as a technological advancement but as a catalyst for holistic growth and sustainability. Looking ahead, AI’s role in business decision-making is poised to continue shaping the dynamics of the corporate world in profound and enduring ways.",https://www.researchgate.net/profile/Oluwatoyin-Farayola/publication/377345979_AI_in_Decision_Making_Transforming_Business_Strategies/links/6601fe2fa4857c796282c14e/AI-in-Decision-Making-Transforming-Business-Strategies.pdf
AI Based Information Systems,SciSpace,"Peter Buxmann, Thomas Hess, Jason Bennett Thatcher","ARTICLE Artificial intelligence (AI) is about to bring fundamental changes in our society and economy, touching on how organizations make decisions, deliver services, and evaluate opportunities. Given the breadth of their potential reach across companies of different sizes and in different industries, Erik Brynjolfsson and Andrew McAfee of MIT even speak of AI as ‘‘the most important general-purpose technology of our era’’ (Brynjolfsson and McAfee 2017, p. 2). Today, AI applications in most of the cases are based upon machine learning algorithms, whereby supervised learning, in particular, has become established in practice. Consistent with this optimistic view, leaders in practice predict the widespread use of AI technologies. Forbes, for example, conducted a study among more than 300 executives. 95 percent of the Forbes study’s participants believe that AI will play an essential role in their companies in the future (Forbes Insights Team 2018). The McKinsey Global Institute (MGI) study predicts that AI’s application in companies will result in a global value-added contribution of USD 13 trillion by 2030 (Bughin et al. 2018). AI is already being woven into common applications (Buxmann and Schmidt 2021). For example, AI applications that use machine learning algorithms are used to enable essential firm activities, such as analysis of financial credits, to determine the status of production machines, to support essential services, such as law enforcement, and to protect personal data through cybersecurity. Moreover, AI’s application in health care research – e.g., identifying possible treatment plans or use in drug discovery – have assumed even greater importance as scientists search for treatment and vaccinations in COVID-19-times. From an economic perspective, AI holds the potential to help people, businesses, and governments to lower costs of service delivery and speed up the time required to make decisions. In many cases, algorithms also make faster, more systematic, evidence-based decisions than humans. On the other hand, costs and speed are not the only considerations relevant to decisions. There is a need for a broader conversation about the ethical aspects of decisionmaking and using AI to make decisions that affect people’s lives. These concerns about the potential risks posed to fairness, non-discrimination, transparency and privacy merit attention from policymakers, business leaders, and academic scientists. Of course, no one can tell for sure whether these optimistic and pessimistic forecasts will come true. However, such forecasts command attention, suggesting that rather than a passing fashion, AI will be woven into organizations and society in the years to come.
read less",2021,"Artificial intelligence (AI) is about to bring fundamental

changes in our society and economy, touching on how

organizations make decisions, deliver services, and evalu-

ate opportunities. Given the breadth of their potential reach

across
 
companies
 
of
 
different
 
sizes
 
and
 
in
 
different

industries, Erik Brynjolfsson and Andrew McAfee of MIT

even speak of AI as ‘‘the most important general-purpose

technology of our era’’ (Brynjolfsson and McAfee 2017,

p. 2). Today, AI applications in most of the cases are based

upon machine learning algorithms, whereby supervised

learning, in particular, has become established in practice.

Consistent with this optimistic view, leaders in practice

predict the widespread use of AI technologies. Forbes, for

example, conducted a study among more than 300 execu-

tives. 95 percent of the Forbes study’s participants believe

that AI will play an essential role in their companies in the

future (Forbes Insights Team 2018). The McKinsey Global

Institute (MGI) study predicts that AI’s application in

companies will result in a global value-added contribution

of USD 13 trillion by 2030 (Bughin et al. 2018).

AI
 
is
 
already
 
being
 
woven
 
into
 
common
 
applica-

tions (Buxmann and Schmidt 2021). For example, AI appli-

cations that use machine learning algorithms are used to

enable essential firm activities, such as analysis of financial

credits, to determine the status of production machines, to

support essential services, such as law enforcement, and to

protect personal data through cybersecurity. Moreover, AI’s

application in health care research – e.g., identifying possible

treatment plans or use in drug discovery – have assumed even

greater importance as scientists search for treatment and

vaccinations in COVID-19-times.

From an economic perspective, AI holds the potential to

help people, businesses, and governments to lower costs of

service delivery and speed up the time required to make

decisions. In many cases, algorithms also make faster,

more systematic, evidence-based decisions than humans.

On the other hand, costs and speed are not the only con-

siderations relevant to decisions. There is a need for a

broader conversation about the ethical aspects of decision-

making and using AI to make decisions that affect people’s

lives. These concerns about the potential risks posed to

fairness,
 
non-discrimination,
 
transparency
 
and
 
privacy

merit attention from policymakers, business leaders, and

academic scientists.

Of course, no one can tell for sure whether these opti-

mistic and pessimistic forecasts will come true. However,

such forecasts command attention, suggesting that rather

than a passing fashion, AI will be woven into organizations

and society in the years to come.

2 The Special Issue

In this special issue, we set out to stimulate a conversation

about
 
how
 
subfield of computer science, is going to shape organiza-

tions of the future. We explicitly asked authors to consider

if AI and its sub-disciplines such as natural language pro-

cessing,
 
deep
 
learning
 
algorithms,
 
pattern
 
recognition,

knowledge-based
 
systems,
 
or
 
robotics
 
would
 
change

organizations. By directing scholars’ attention to the eco-

nomic and organizational perspective of AI and machine

learning, we hoped to gather insights into topics such as

organizational AI readiness, the influence of virtual assis-

tants, and aspects of fairness and acceptance of AI.

We received 16 manuscripts that provide a compre-

hensive coverage of various theories, methods, and con-

texts. After a developmental peer-review process, four

papers remained. We are delighted to include these four

papers in our issue.

In the first paper of the special issue, Jo
 ̈hnk et al. (2021)

develop a conceptualization of organizational AI readiness

that guides organizations in the AI adoption process. The

authors conduct a qualitative study with a total of 25

interviews with AI experts and compare their findings with

literature and practitioner studies. In total, they identified

18
 
relevant
 
AI
 
readiness
 
factors
 
with
 
58
 
indicators.

Moreover, they structured their results into five categories:

Strategic alignment, resources, knowledge, culture, and

data. These factors and indicators aim to support organi-

zations in determining their AI readiness level, identify

gaps, and make more informed AI adoption decisions.

The paper by Mirbabaie et al. (2021) examines Virtual

Assistants (VAs)’ impact on individuals and teams. They

use social identity theory to describe the identification of

employees with team members and the continued existence

of group identity. Using a laboratory experiment, they

compared two groups in solving a task. One group was

assisted by a VA, while the other was supported by a

person. The main result of this study is that the relationship

between social identification with (virtual) team members

and expanding the self through technology such as VAs is

not contradictory but rather complementary.

Ko
 ̈chling et al. (2021) discuss the issue of bias and

algorithmic fairness in the context of algorithmic decision-

making for recruitment. An exploratory data analysis is

carried out on a data set containing 10.000 15-s video clips

to examine personality traits and invitations for job inter-

views. The findings encompass two main insights: First, the

use of algorithmic decision-making does not eliminate the

threat
 
of
 
implicit
 
biases
 
and
 
discrimination.
 
Second,

underrepresentation in a data set concerning gender and

ethnicity in the training data can cause unpredictable clas-

sifications. For example, the selection of interview repre-

sentatives in the recruitment process is biased towards

specific groups.

Finally, the paper of Berger et al. (2021) deals with

algorithmic aversion in the context of AI. Here, an online

experiment examines how participants react to advice from

a human advisor instead of an algorithm’s advice. The

experiment also investigates the potential effect of partic-

ipants
 
becoming
 
familiar
 
with
 
the
 
algorithmic
 
advisor

during the experiment and that advisor demonstrates its

ability to learn. Overall, the experiment
 
indicates
 
that

familiarity
 
with
 
an
 
erring
 
algorithmic
 
advisor
 
reduces

decision makers’ reliance on this advisor and shows that

demonstrating an algorithm’s ability to learn can offset this

effect over time.

The special issue also includes an interview with Karl-

Heinz Streibich on AI. The president of acatech explains

from a practical point of view what challenges companies

face when adopting AI and discusses AI as the next evo-

lutionary step in using IT and digital systems.

3 What’s Next?

We hope that this special issue provides a starting point for

scholars interested in understanding the implications of AI

for people, organizations, and society. There remain many

interesting topics that our authors’ work suggests merit

attention from economic and organizational scholars. For

example:

•
 
AI, CIOs, and Firm Strategy
 
The work of Jo
 ̈hnk et al.

(2021) points to a need to examine strategic issues tied

to AI’s deployment in organizations. While we know

how CIOs manage strategic and maintenance issues

concerning technology, we know little about how they

go about marshalling resources necessary to develop

and sustain technologies that can fundamentally change

how firms make decisions and deliver services. Key

lingering questions include: How do CIOs align AI

deployment with firm strategy?
 
Will
 
CIOs
 
become

more prominent in firm governance as they acquire

responsibility for supporting strategic AI decisions?

What role will boards of directors play in shaping AI

strategy? While AI readiness constitutes a first step

towards
 
understanding
 
strategic
 
issues
 
because
 
AI

represents a fundamental change in how technology

impacts business, we may need to revisit our present

understanding of how CIOs shape IT strategy as well as

ask if new forms of governance will be required to

effectively deploy and maintain AI in organizations.

•
 
AI,
 
Identity,
 
and
 
Sociotechnical
 
Systems
 
Mirbabaie

et al. (2021) build on the growing stream of work on

individuals and IT identity and point to a need to

progress from considering the individual to considering

the identity of groups and the technology itself. While

we know individuals possess IT identity, will AI learn

to support the quirks and habits of teams? Will it enab

the
 
formation
 
of
 
unique
 
identities
 
relative
 
to
 
the

systems that support them? If so, how will that change

the interaction of not only individuals within groups but

also that of groups vis-a-vis the broader sociotechnical

context in which they work? Further, as AI grows more

sophisticated, autonomous, and capable of double-loop

learning,
 
how
 
will
 
the
 
‘‘identity’’
 
manifest
 
in
 
the

algorithms shape when we interact with them? And

integrate
 
them
 
into
 
the
 
firm
 
processes?
 
This
 
work

on virtual assistants constitutes only a first step towards

a broader understanding of the implications of using AI

to support teams and to integrate them with broader

sociotechnical systems.


that AI is prone to the biases imbued by rules and data

provided by their human designers. While the findings

illustrate how biases in algorithms and data can result in

adverse impacts on different groups of people, their

implications are further reaching for designing and

understanding
 
AI.
 
This
 
work
 
suggests
 
a
 
need
 
for

thoughtful, introspective research that examines how

to collect data
 
that accurately depicts the ‘‘ground

truth’’ of organizational and social life, as we design

but also maintain algorithms targeted at supporting

fairness and equity in organizations and society. How

can we know data represents the full set of factors

relevant to desired outcomes? How can we ensure

people trust the AI and do so sufficiently to continue

sharing information with it? What role will privacy

play? And security? In determining to which extent are

people willing to share data needed to train AI and

sustain them? How can we train AI to detect biases? Or

what about adverse impacts for the people they serve?

•
 
AI, the Uncanny Valley, and the Singularity
 
Berger

et al. (2021) demonstrate that people trust AI that

demonstrates a capacity to improve. While much has

been
 
made
 
of
 
anthropomorphism
 
and
 
the
 
uncanny

valley, there remains a lot to learn about how to design

the manner in which we interact with AI, particularly,

AI that will soon be able to emulate humans. Their

work suggests a need for careful investigation of not

only how we present the AI (e.g., the interface) but also

of how we educate users about the algorithms that drive

the AI, the relationship from data fed into the AI to

outcomes
 
and
 
to
 
users’
 
affective
 
responses
 
to
 
the

support offered by AI. How will users respond to

increasingly human-like systems? Will a greater under-

standing
 
of
 
algorithms
 
result
 
in
 
more
 
trust?
 
And

information sharing? Or will it undercut beliefs about

security and exacerbate fears about privacy? Should

designers avoid the uncanny valley or embrace the

‘‘singularity’’? And should they design systems to be

partners with users?

•
 
AI and Ethics
 
Collectively, these papers suggest a need

to actively question the implications of how we build

the algorithms, gather data to train, and apply AI to

solve problems in organizations and society. These

papers
 
hint
 
at
 
the
 
necessity
 
to
 
investigate
 
ethical

questions, such as: How does the design of AI change

the decisions that we make? Do we fully inform users

of the scope of AI application in the sociotechnical

systems they live in? Or do we apply AI like electricity

– as a utility? And without comment?

In our minds, the papers in this special issue evoke

questions about user data, its sources, and applica-

tion. In how far are designers obliged to explain the

implications of users’ training data contributions?

And what about the potential applications of their

data to solving problems? Or enabling decisions in

AI-enabled information systems? And given appli-

cations of data will certainly evolve, what ethical

obligations do organizations and designers have to

update
 
contributors
 
about
 
new
 
uses
 
of
 
data
 
they

shared? These questions about data are particularly

salient as GDPR becomes infused with the intellec-

tual framework of our society – and is not in the

culture of other societies.

Since we first crafted the call for contributions to

the special issue and when reflecting on these papers,

we found ourselves asking even bigger questions

about ethics and AI that merit attention in future work

in BISE and other rigorous academic outlets – what

limits should we place on AI in society? Just because

we can design systems to make decisions, should we

do so? What kind of problems should unsupervised

AI be applied to? What kind of decisions should

remain at least semi-supervised by humans? As we

allocate decisions between humans and AI, what role

should ethics play in the allocation of responsibilities

between people and machines? How can we ensure

that AI is applied in a way that results in pareto

efficient outcomes for people and society?

These are just a few of the questions and opportunities that

this special issue presents for future research on artificial

intelligence. There are many more that we lacked room to

integrate into this special issue, e.g. about the ethics of

design of designers, and of how governments use AI. We

look forward to seeing how the next wave of AI researchers

address these questions as they seek to build a better

society through the judicious integration of AI into indi-

viduals’ lives and the organizations in which they work.

subfield of computer science, is going to shape organiza-

tions of the future. We explicitly asked authors to consider

if AI and its sub-disciplines such as natural language pro-

cessing,
 
deep
 
learning
 
algorithms,
 
pattern
 
recognition,

knowledge-based
 
systems,
 
or
 
robotics
 
would
 
change

organizations. By directing scholars’ attention to the eco-

nomic and organizational perspective of AI and machine

learning, we hoped to gather insights into topics such as

organizational AI readiness, the influence of virtual assis-

tants, and aspects of fairness and acceptance of AI.

We received 16 manuscripts that provide a compre-

hensive coverage of various theories, methods, and con-

texts. After a developmental peer-review process, four

papers remained. We are delighted to include these four

papers in our issue.

In the first paper of the special issue, Jo
 ̈hnk et al. (2021)

develop a conceptualization of organizational AI readiness

that guides organizations in the AI adoption process. The

authors conduct a qualitative study with a total of 25

interviews with AI experts and compare their findings with

literature and practitioner studies. In total, they identified

18
 
relevant
 
AI
 
readiness
 
factors
 
with
 
58
 
indicators.

Moreover, they structured their results into five categories:

Strategic alignment, resources, knowledge, culture, and

data. These factors and indicators aim to support organi-

zations in determining their AI readiness level, identify

gaps, and make more informed AI adoption decisions.

The paper by Mirbabaie et al. (2021) examines Virtual

Assistants (VAs)’ impact on individuals and teams. They

use social identity theory to describe the identification of

employees with team members and the continued existence

of group identity. Using a laboratory experiment, they

compared two groups in solving a task. One group was

assisted by a VA, while the other was supported by a

person. The main result of this study is that the relationship

between social identification with (virtual) team members

and expanding the self through technology such as VAs is

not contradictory but rather complementary.

Ko
 ̈chling et al. (2021) discuss the issue of bias and

algorithmic fairness in the context of algorithmic decision-

making for recruitment. An exploratory data analysis is

carried out on a data set containing 10.000 15-s video clips

to examine personality traits and invitations for job inter-

views. The findings encompass two main insights: First, the

use of algorithmic decision-making does not eliminate the

threat
 
of
 
implicit
 
biases
 
and
 
discrimination.
 
Second,

underrepresentation in a data set concerning gender and

ethnicity in the training data can cause unpredictable clas-

sifications. For example, the selection of interview repre-

sentatives in the recruitment process is biased towards

specific groups.

Finally, the paper of Berger et al. (2021) deals with

algorithmic aversion in the context of AI. Here, an online

experiment examines how participants react to advice from

a human advisor instead of an algorithm’s advice. The

experiment also investigates the potential effect of partic-

ipants
 
becoming
 
familiar
 
with
 
the
 
algorithmic
 
advisor

during the experiment and that advisor demonstrates its

ability to learn. Overall, the experiment
 
indicates
 
that

familiarity
 
with
 
an
 
erring
 
algorithmic
 
advisor
 
reduces

decision makers’ reliance on this advisor and shows that

demonstrating an algorithm’s ability to learn can offset this

effect over time.

The special issue also includes an interview with Karl-

Heinz Streibich on AI. The president of acatech explains

from a practical point of view what challenges companies

face when adopting AI and discusses AI as the next evo-

lutionary step in using IT and digital systems.

3 What’s Next?

We hope that this special issue provides a starting point for

scholars interested in understanding the implications of AI

for people, organizations, and society. There remain many

interesting topics that our authors’ work suggests merit

attention from economic and organizational scholars. For

example:

•
 
AI, CIOs, and Firm Strategy
 
The work of Jo
 ̈hnk et al.

(2021) points to a need to examine strategic issues tied

to AI’s deployment in organizations. While we know

how CIOs manage strategic and maintenance issues

concerning technology, we know little about how they

go about marshalling resources necessary to develop

and sustain technologies that can fundamentally change

how firms make decisions and deliver services. Key

lingering questions include: How do CIOs align AI

deployment with firm strategy?
 
Will
 
CIOs
 
become

more prominent in firm governance as they acquire

responsibility for supporting strategic AI decisions?

What role will boards of directors play in shaping AI

strategy? While AI readiness constitutes a first step

towards
 
understanding
 
strategic
 
issues
 
because
 
AI

represents a fundamental change in how technology

impacts business, we may need to revisit our present

understanding of how CIOs shape IT strategy as well as

ask if new forms of governance will be required to

effectively deploy and maintain AI in organizations.

•
 
AI,
 
Identity,
 
and
 
Sociotechnical
 
Systems
 
Mirbabaie

et al. (2021) build on the growing stream of work on

individuals and IT identity and point to a need to

progress from considering the individual to considering

the identity of groups and the technology itself. While

we know individuals possess IT identity, will AI learn

to support the quirks and habits of teams? Will it enable

123

2
 
P. Buxmann et al.: AI-Based Information Systems, Bus Inf Syst Eng 63(1):1–4 (2021)





the
 
formation
 
of
 
unique
 
identities
 
relative
 
to
 
the

systems that support them? If so, how will that change

the interaction of not only individuals within groups but

also that of groups vis-a-vis the broader sociotechnical

context in which they work? Further, as AI grows more

sophisticated, autonomous, and capable of double-loop

learning,
 
how
 
will
 
the
 
‘‘identity’’
 
manifest
 
in
 
the

algorithms shape when we interact with them? And

integrate
 
them
 
into
 
the
 
firm
 
processes?
 
This
 
work

on virtual assistants constitutes only a first step towards

a broader understanding of the implications of using AI

to support teams and to integrate them with broader

sociotechnical systems.

•
 
AI, Bias, and Data
 
Ko
 ̈chling et al. (2021) underscore

that AI is prone to the biases imbued by rules and data

provided by their human designers. While the findings

illustrate how biases in algorithms and data can result in

adverse impacts on different groups of people, their

implications are further reaching for designing and

understanding
 
AI.
 
This
 
work
 
suggests
 
a
 
need
 
for

thoughtful, introspective research that examines how

to collect data
 
that accurately depicts the ‘‘ground

truth’’ of organizational and social life, as we design

but also maintain algorithms targeted at supporting

fairness and equity in organizations and society. How

can we know data represents the full set of factors

relevant to desired outcomes? How can we ensure

people trust the AI and do so sufficiently to continue

sharing information with it? What role will privacy

play? And security? In determining to which extent are

people willing to share data needed to train AI and

sustain them? How can we train AI to detect biases? Or

what about adverse impacts for the people they serve?

•
 
AI, the Uncanny Valley, and the Singularity
 
Berger

et al. (2021) demonstrate that people trust AI that

demonstrates a capacity to improve. While much has

been
 
made
 
of
 
anthropomorphism
 
and
 
the
 
uncanny

valley, there remains a lot to learn about how to design

the manner in which we interact with AI, particularly,

AI that will soon be able to emulate humans. Their

work suggests a need for careful investigation of not

only how we present the AI (e.g., the interface) but also

of how we educate users about the algorithms that drive

the AI, the relationship from data fed into the AI to

outcomes
 
and
 
to
 
users’
 
affective
 
responses
 
to
 
the

support offered by AI. How will users respond to

increasingly human-like systems? Will a greater under-

standing
 
of
 
algorithms
 
result
 
in
 
more
 
trust?
 
And

information sharing? Or will it undercut beliefs about

security and exacerbate fears about privacy? Should

designers avoid the uncanny valley or embrace the

‘‘singularity’’? And should they design systems to be

partners with users?

•
 
AI and Ethics
 
Collectively, these papers suggest a need

to actively question the implications of how we build

the algorithms, gather data to train, and apply AI to

solve problems in organizations and society. These

papers
 
hint
 
at
 
the
 
necessity
 
to
 
investigate
 
ethical

questions, such as: How does the design of AI change

the decisions that we make? Do we fully inform users

of the scope of AI application in the sociotechnical

systems they live in? Or do we apply AI like electricity

– as a utility? And without comment?

In our minds, the papers in this special issue evoke

questions about user data, its sources, and applica-

tion. In how far are designers obliged to explain the

implications of users’ training data contributions?

And what about the potential applications of their

data to solving problems? Or enabling decisions in

AI-enabled information systems? And given appli-

cations of data will certainly evolve, what ethical

obligations do organizations and designers have to

update
 
contributors
 
about
 
new
 
uses
 
of
 
data
 
they

shared? These questions about data are particularly

salient as GDPR becomes infused with the intellec-

tual framework of our society – and is not in the

culture of other societies.

Since we first crafted the call for contributions to

the special issue and when reflecting on these papers,

we found ourselves asking even bigger questions

about ethics and AI that merit attention in future work

in BISE and other rigorous academic outlets – what

limits should we place on AI in society? Just because

we can design systems to make decisions, should we

do so? What kind of problems should unsupervised

AI be applied to? What kind of decisions should

remain at least semi-supervised by humans? As we

allocate decisions between humans and AI, what role

should ethics play in the allocation of responsibilities

between people and machines? How can we ensure

that AI is applied in a way that results in pareto

efficient outcomes for people and society?

These are just a few of the questions and opportunities that

this special issue presents for future research on artificial

intelligence. There are many more that we lacked room to

integrate into this special issue, e.g. about the ethics of

design of designers, and of how governments use AI. We

look forward to seeing how the next wave of AI researchers

address these questions as they seek to build a better

society through the judicious integration of AI into indi-

viduals’ lives and the organizations in which they work.

123

P. Buxmann et al.: AI-Based Information Systems, Bus Inf Syst Eng 63(1):1–4 (2021)
 
3















4 Special Thanks

We would like to offer our sincere thanks to all authors and

reviewers who have contributed to the research presented

in this special issue. Special thanks go to the Business &

Information Systems Engineering team, without whom the

special issue would not have been possible, and to Anne

Zo
 ̈ll
 
from
 
Technical
 
University
 
of
 
Darmstadt
 
for
 
her

organizational support during the publication process. We

are very pleased to present the methods, concepts, and

results of this special issue to researchers and practitioners.

We hope that these contributions will enhance the field of

AI-based Information Systems.

Funding
 
Open Access funding enabled and organized by Projekt

DEAL.

Open Access
 
This article is licensed under a Creative Commons

Attribution 4.0 International License, which permits use, sharing,

adaptation, distribution and reproduction in any medium or format, as

long as you give appropriate credit to the original author(s) and the

source, provide a link to the Creative Commons licence, and indicate

if changes were made. The images or other third party material in this

article are included in the article’s Creative Commons licence, unless

indicated otherwise in a credit line to the material. If material is not

included in the article’s Creative Commons licence and your intended

use is not permitted by statutory regulation or exceeds the permitted

use, you will need to obtain permission directly from the copyright

holder. To view a copy of this licence, visit http://creativecommons.

org/licenses/by/4.0/.

References

Berger B, Adam M, Ru
 ̈hr A, Benlian A (2021) Watch me improve –

algorithm aversion and demonstrating the ability to learn. Bus

Inf Syst Eng 63(1) (in print)

Brynjolfsson E, McAfee A (2017) The business of artificial intelli-

gence.
 
Harvard
 
Bus
 
Rev.
 
https://starlab-alliance.com/wp-con

tent/uploads/2017/09/The-Business-of-Artificial-Intelligence.

pdf. Accessed 21 Oct 2020

Bughin J, Seong J, Manyika j, Chui M, Joshi R (2018) Notes from the

AI frontier: modeling the impact of AI on the world economy.

https://www.mckinsey.com/featured-insights/artificial-intelli

gence/notes-from-the-ai-frontier-modeling-the-impact-of-ai-on-

the-world-economy. Accessed 12 Oct 2020

Buxmann
 
P,
 
Schmidt
 
H
 
(2021)
 
Ku
 ̈nstliche
 
Intelligenz
 
–
 
Mit

Algorithmen zum wirtschaftlichen Erfolg, 2nd edn. Springer,

Heidelberg

Forbes Insights Team (2018) On your marks: business leaders prepare

for arms race in artificial intelligence. https://www.forbes.com/

sites/insights-intelai/2018/07/17/on-your-marks-business-lea

ders-prepare-for-arms-race-in-artificial-intelligence/

#17fd0a8a1946. Accessed 12 Oct 2020

Jo
 ̈hnk J, Weißert M, Wyrtki K (2021) Ready or not, AI comes – an

interview study of organizational AI readiness factors. Bus Inf

Syst Eng 63(1) (in print)

Ko
 ̈chling A, Riazy S, Wehner M, Simbeck K (2021) Highly accurate,

but still discriminatory – a fairness evaluation of algorithmic

video analysis in the recruitment context. Bus Inf Syst Eng 63(1)

(in print)

Mirbabaie M, Stieglitz S, Bru
 ̈nker F, Hofeditz L, Ross B, Frick N

(2021) Understanding collaboration with virtual assistants – the

role of social identity and the extended self. Bus Inf Syst Eng

63(1) (in print)

123

4
 
P. Buxmann et al.: AI-Based Information Systems, Bus Inf Syst Eng 63(1):1–4 (2021)



",https://typeset.io/papers/ai-based-information-systems-59pqf4gvjj
"On the current state of combining human and artificial intelligence for strategic organizational decision making
",Springer Business Research ,"Anna Trunk, Hendrik Bickel, Evi Hartmann","ARTICLE Strategic organizational decision making in today’s complex world is a dynamic process characterized by uncertainty. Therefore, diverse groups of responsible employees deal with the large amount and variety of information, which must be acquired and interpreted correctly to deduce adequate alternatives",2020,"Strategic organizational decision making in today’s complex world is a dynamic process characterized by uncertainty. Therefore, diverse groups of responsible employees deal with the large amount and variety of information, which must be acquired and interpreted correctly to deduce adequate alternatives. The technological potential of artificial intelligence (AI) is expected to offer further support, although research in this regard is still developing. However, as the technology is designed to have capabilities beyond those of traditional machines, the effects on the division of tasks and the definition of roles established in the current human–machine relationship are discussed with increasing awareness. Based on a systematic literature review, combined with content analysis, this article provides an overview of the possibilities that current research identifies for integrating AI into organizational decision making under uncertainty. The findings are summarized in a conceptual model that first explains how humans can use AI for decision making under uncertainty and then identifies the challenges, pre-conditions, and consequences that must be considered. While research on organizational structures, the choice of AI application, and the possibilities of knowledge management is extensive, a clear recommendation for ethical frameworks, despite being defined as a crucial foundation, is missing. In addition, AI, other than traditional machines, can amplify problems inherent in the decision-making process rather than help to reduce them. As a result, the human responsibility increases, while the capabilities needed to use the technology differ from other machines, thus making education necessary. These findings make the study valuable for both researchers and practitioners.1 Introduction
Companies exist as a result of and are shaped by decisions (Melnyk et al. 2014; Pereira and Vilà 2016) that constitute and are constituted by their strategy (Mintzberg 1972). Strategic decision making is a dynamic and challenging process (Mintzberg 1973; Liu et al. 2013; Dev et al. 2016; Moreira and Tjahjono 2016) due to organizations operating in complex environments and because of the the direct or indirect effects that decisions can have on stakeholders (Koch et al. 2009; Delen et al. 2013; El Sawy et al. 2017; Carbone et al. 2019).
Traditional decision theory distinguishes between decisions made under risk versus those made under uncertainty (Knight 1921). In the former category, all possible outcomes, including their probabilities of occurrence, are known and statistically or empirically available (Knight 1921; Marquis and Reitz 1969; Sydow 2017). However, for strategic organizational decisions, which belong to the latter category (Knight 1921; Marquis and Reitz 1969), the degree and type of uncertainty are influenced by various aspects (Rousseau 2018). Such decisions must thus to be taken in an adaptive mode to handle complexity (Mintzberg 1973), which organizations support through the introduction of hierarchies and departments to define responsibilities (Simon 1962). While this improves decision speed and efficiency for operational decisions, the quality of strategic decisions has been found to be enhanced by including a multitude of perspectives, experiences, and expertise (Knight 1921; Rousseau 2018). Organizations hence assign the task of handling complexity while ensuring diversity to managers from different departments (Rousseau 2018). Consensus must be achieved among this group to reach a decision, which is why in this study, strategic organizational decision making is defined as group decision making under uncertainty.
Nevertheless, even with more people involved, the human capacity to process information is limited (Lawrence 1991; Fiori 2011). Human decision makers, therefore, consciously construct simplified models, called heuristics or rules of thumb (Simon 1987; Fiori 2011), which deal with complex problems sequentially to make them treatable for the human computation capacity. This is called bounded rationality, a concept that researchers have interpreted differently since Herbert Simon originally defined it in the 1950s (Simon 1955; see overview of Fiori 2011). It is often seen as an unconscious activity that cannot be controlled (e.g., Kahneman 2003), sometimes also known as intuition. For Simon, however, even intuition is based on stored information and experience, which the decision maker decides to rely on when determining alternatives and probabilities, although more unconsciously (Simon 1986; Fiori 2011). Rational behavior is thus assumed to be on the continuum between intended rationality and intuition, depending on the information-processing capabilities of the agent, the complexity of the problem, and various aspects of the environment (Lawrence 1991; Fiori 2011). However, rational behavior is guided by rules, which means that it is always bounded (Fiori 2011). This makes the human brain similar to computers, both being “physical symbol systems” that process information (Simon 1995: 104).
Computers are defined as artificial intelligence (AI), which Simon (1995) sees as mathematical and physical applications that are able to handle complexity, in contrast to traditional mathematical theorems. However, opinions and studies on the extent to which AI can be used for the same tasks as the human brain, especially in connection with decision making have been scarce and differ in focus, technology, and objective (Bouyssou and Pirlot 2008; Munguìa et al. 2010; Nilsson 2010; Glock and Hochrein 2011; Nguyen et al. 2018; Wright and Schultz 2018).
Including technology in business is not a new development, as machines have been part of manufacturing processes to support humans for centuries, but machines are rather a tool, completely governed by humans, and less defined in real social collaboration settings than organizations are (Lawrence 1991; Nguyen et al. 2018; Boone et al. 2019). With AI, machines are assumed to act and react to humans, implying a possible change in the human–machine relationship (Huang and Rust 2018). Opportunities and hazards, however, are neither agreed nor analyzed in more detail, making research necessary (Lawrence 1991; Silva and Kenney 2018; Vaccaro and Waldo 2019).
The goal of this article is thus to offer guidance for groups to successfully apply existing AI to enhance decision quality in complex and uncertain environments. The topic is suitable for study with a literature review, as research on AI in general is manifold, but clear recommendations are lacking. By synthesizing existing frameworks and studies, the following research question (RQ) will be answered:
RQ How can AI support decision-making under uncertainty in organizations?
The assumptions and findings of traditional decision theory, as defined by Knight (1921), Fredrickson (1984) and Resnik (1987), serve as the foundation for the analysis. However, to ensure the success of the whole decision-making process, the “how” of the RQ must also include pre-requisites that are crucial for possible AI integration. Furthermore, AI support can only be evaluated adequately when the potential consequences and challenges of the adapted process are analyzed and, if possible, considered beforehand. To facilitate understanding and derivation of the results, the RQ is thus divided into the following three sub-dimensions, all referring to the general decision-making process under uncertainty (Fredrickson 1984; Rousseau 2018): (1) possibilities of AI integration per step, (2) necessary pre-conditions and crucial preparations, and (3) potential challenges and consequences. The resulting conceptual framework provides an overview of aspects that executives should be aware of, also referring to the potential effects of AI integration on the tasks and responsibilities of human decision makers.
The remainder of the article is organized as follows. First, after a brief overview of the history of AI and its definition, as well as existing categories of applications, the theoretical section provides an introduction to decision theory and group decision making, linking it to AI. The third section briefly describes the method of linking a systematic literature review (SLR) with content analysis (CA) and the executed process. Then, an outline of the findings is presented to answer the RQ, followed by providing a conceptual framework for organizational decision making under uncertainty. The article subsequently offers managerial implications and closes with an overview of limitations, future research possibilities, and a short conclusion.
2 Decision making with the help of AI
2.1 Development and current status of AI research
2.1.1 Definition and history of AI
AI emerged as a concept in the sixth century BC, with Homer’s Iliad mentioning self-propelled chairs (McCorduck 2004; Nilsson 2010). The computing machine was invented in 1937 by Alan Turing, who claimed that as soon as a machine can act as intelligently as a human being, it can be seen as artificially intelligent (McCorduck 2004; Nilsson 2010). Then, in 1955, McCarthy et al. (1955) first introduced the term “Artificial Intelligence” in a proposal for the Dartmouth summer research project to study how intelligence can be exercised by machines. The goal of their project was to describe any feature of intelligence so precisely that a machine could simulate it. Simon supports this view, defining AI as “systems that exhibited intelligence, either as pure explorations into the nature of intelligence, explorations of the theory of human intelligence, or explorations of the systems that could perform practical tasks requiring intelligence” (Simon 1995: 96). More recent definitions include “technologies that mimic human intelligence” (Huang et al. 2019: 44) and “machines that perform tasks that humans would perform” (Bolander 2019: 850), or they focus on the independence of machines from humans, speaking of “artifacts able to carry out tasks in the real world without human intervention” (Piscopo and Birattari 2008: 275). These definitions can be further expanded by similar approaches, all relating machines to intelligence, although this concept is also not defined (for an overview of definitions, see Legg and Hutter 2007: 401).
For this reason, in this article, Nilsson’s (2010: 13) definition is adopted, as it encompasses Simon’s view and all other above-mentioned aspects, while being precise enough to guide the further analysis: “For me, AI is that activity devoted to making machines intelligent, and intelligence is that quality that enables an entity to function appropriately and with foresight in its environment.” The capabilities necessary to “function appropriately and with foresight” range from perception to interpretation and the development of actions to interact with, react to, or even influence the environment to achieve individual goals (Legg and Hutter 2007; Bolander 2019). The specific capability that is needed depends on the environment and the type of problem. Lawrence (1991) established a framework for decisions driven by complexity, leading to several decision types, versus decisions driven by politicality, which describes environmental influences not only from society and politics, but also within organizations. Figure 1 relates these definitions to Simon’s (1986, 1995) continuum of rational behavior, assuming that perception is rather linked to intended rationality, while interpreting and acting require the inclusion of additional experiences and stored information. Following Simon (1995), all steps can be executed by humans and machines alike. This is supported by the definition of an algorithm as “a process or set of rules to be followed in problem-solving operations” (Silva and Kenney 2018: 13). Being the integral part of AI, algorithms thus equate to human heuristics for solving problems in a step-wise manner.
Fig. 1

The continuum of rational behavior (based on Simon 1986, 1995; Lawrence 1991; Nilsson 2010)
Nevertheless, there are caveats regarding this view. One of the earliest stems from Descartes, who in 1637 claimed that it would be “morally impossible (…) to allow it (sic. the machine) to act in all events of life the same way as our reason causes us to act.” This is supported by Bolander (2019), who claims that humans and machines cannot be compared in intelligence, as they have different strengths and weaknesses. Moreover, some researchers find AI to be useful for special areas only, where no abstractions, knowledge transfer, or the analysis of unstructured tasks is needed (Sheil 1987; Surden 2019), and there are differing views on the potential that AI has for creativity, emotions, or empathy (Wamba et al. 2015; Kaplan and Haenlein 2019). To integrate AI beneficially into organizational decision making, current research indicates that one must first understand its capabilities and potential dangers, especially compared to or in interaction with people. This understanding is expected to decrease the human fear of losing power and of change, and it supports building trust. Furthermore, Morozov (2013) highlights the challenge of technological solutionism, assuming technological decisions to be superior and no longer accepting human imperfection and failure. This also includes the risk of consciously or unconsciously creating problems because it is technologically feasible to solve them (Morozov 2013). The following study provides a better understanding of the benefits and limits of AI, starting with an overview of its applications in the following section.
2.1.2 AI applications
A detailed definition of an AI application is not available. For humans, different dimensions of intelligence are said to exist (Legg and Hutter 2007), and following Nilsson’s definition and the continuum of rational behavior (see Fig. 1), types of AI applications range from less to more complex, depending on the environment and the type of decision (McCarthy et al. 1955; Nilsson 2010).
Lawrence (1991) linked these dimensions to possible AI applications, but focused only on two concrete applications: natural language processing and expert systems. Almost 30 years later, the number of AI applications has increased significantly. Therefore, the framework will be linked to the categories of bottom–up and top–down approaches, following the majority of researchers (Nilsson 2010; Bolander 2019; Surden 2019). The former category refers to applications that are created implicitly, meaning that they all statistically learn from experience and are thus not completely predictable, error-free, or explainable. The second cluster includes mathematical and statistical approaches, although researchers sometimes do not agree with or even mention them as being AI (e.g., Simon 1995; Welter et al. 2013; Haruvy et al. 2019). These applications are also called logical rules and knowledge representation, based on rules that human programmers provide to computers, often with the goal of automation (Surden 2019), leading to systems that are predictable and explainable with strict and known abilities (Bolander 2019). Figure 2 offers a framework relating the categories to the continuum of rational behavior (see Fig. 1), with top–down applications assumed to be used for perception and interpretation, and bottom–up applications for actions, as this step requires the highest level of intelligence. Specifying clear applications for the categories is unfortunately not possible, as researchers do not even agree on how to categorize traditional mathematical applications, while new applications for bottom–up AI are also not stipulated or agreed. The reason for this may be that most systems today, especially when it comes to decision making, are located in the middle, “having a human in the loop” (Bolander 2019; Surden 2019).
Fig. 2

This section established a better understanding of current AI research. In the following, an introduction to decision theory and the characteristics of group decision making, as an equivalent of the organizational approach for strategic decisions, are provided.
2.2 Organizational decision making
2.2.1 Decision theory and resulting challenges
As already explained, strategic decision making belongs to the category of decisions under uncertainty. To make the best decision, each alternative is assigned a probability and utility level, and the alternative with the highest weighted value is chosen (Knight 1921; Fredrickson 1984; Resnik 1987). Probability levels are estimates, characterized by coherence, conditionalization, and convergence (Resnik 1987). Coherence relates to the influence of frequency. With a high frequency of similar decisions in similar situations, expertise increases, which conditions the estimate into a specific direction. Convergence refers to the number of people included. As this number increases, the processing capacity is assumed to increase as well (Resnik 1987).
Utility levels represent an individual or group’s subjective preference for each of the alternative outcomes (Thompson 1967). Especially when decisions affect and involve many stakeholders, values cannot be defined to equally include all utility levels (Liu et al. 2013; Melnyk et al. 2014; Wright and Schultz 2018). Objectivity has been found to be possible only to a limited extent, as decision makers need to rely further on heuristics due to uncertainties inherent in information processing and group discussions in complex environments. In addition, the type and amount of rationality can differ within one decision (Metzger and Spengler 2019), as some aspects of the decision might be influenced more intuitively than others. This entails the risk of bias, which can lead to incorrect problem definitions or the wrong evaluation of alternatives, as some impacts are valued higher than others or guided by assumptions, such as the sunk cost effect (Roth et al. 2015; Danks and London 2017; Cheng and Foley 2018; Boone et al. 2019; Julmi 2019; Kourouxous and Bauer 2019; Metzger and Spengler 2019). Bias can either be conscious, an active introduction of incorrect information by one decision group member at any stage of the process, or unconscious, due to the individual or group being unaware of subjectivity, which in some cases even increases with experience (Roth et al. 2015; Cheng and Foley 2018). Although the process of decision theory refers to one rational individual, research on decisions under uncertainty has found that groups make decisions more in line with the theory than individuals do, and they also compensate for some of these challenges through discussion (Charness and Sutter 2012; Kugler et al. 2012; Carbone et al. 2019). As groups are also the focus of this study, the next section provides an overview of current research (for an overview, see Kugler et al. 2012).
2.2.2 Decision making in groups
As stated in the introduction, for the purpose of this article, strategic organizational decision making is defined as group decision making under uncertainty, as groups are the established type of such decisions in organizations (Rousseau 2018). Heterogeneous groups have been found to make better decisions than homogeneous ones, as information diversity, discussion, and experience lead to improved interpretation, thereby decreasing bounded rationality (Beckmann and Haunschild 2002; Charness and Sutter 2012; Kouchaki et al. 2015; Rousseau 2018; Herden 2019). However, whether groups help to reduce bias (Kouchaki et al. 2015; Rousseau 2018) or can also introduce it into a decision is not agreed (Marquis and Reitz 1969; Charness and Sutter 2012). In addition, for designating alternatives and probabilities, groups have been found to engage in negotiation (Marquis and Reitz 1969; Kugler et al. 2012), but a research gap exists about how they define joint utilities (Samson et al. 2018).
According to Rousseau (2018), to enhance decision quality, it is crucial to search for different types and forms of information and not only the most easily available. At the same time, the reliability, validity, consistency, and relevance of information sources must be analyzed. While this can be facilitated when more people are involved in the decision-making process, researchers have also found that using technology that is able to process large amounts of data can have a supportive effect (Long 2017; Herden 2019). Several researchers on group decision making thus call for more exploration of the use of group communication and information systems (Charness and Sutter 2012; Kugler et al. 2012), including the effect that computer programs can have to help with structuring decisions (Schwenk and Valacich 1994).
Combining humans and technology is expected to improve decision making even further than only including more people. The following section provides the framework of the organizational decision-making process as guidance for this study.
2.3 The basic process for organizational decision making under uncertainty
The proposed process in Fig. 3 is based on decision theory (Fredrickson 1984) and several studies on decision making under uncertainty with the involvement of many people (Beckmann and Haunschild 2002; El Sawy et al. 2017; Long 2017; Rousseau 2018). It provides guidance for analyzing the results of the SLR along the sub-dimensions of the RQ and serves as the foundation for the conceptual framework.
Fig. 3

The basic organizational decision-making process as the framework for the analysis (based on studies from Fredrickson 1984, Beckmann and Haunschild 2002; El Sawy et al. 2017; Long 2017; Rousseau 2018)
Full size image

The process begins with the definition of the decision goal as the guideline for all subsequent steps. The information that must be collected in step two can be categorized as external (i.e., societal, political, legal, or industrial sources) or internal (El Sawy et al. 2017). Scholars deem internal information to be either explicit (e.g., facts and figures on the organization, as well as its products, traffic flows, inventories, and prices) or implicit (Beckmann and Haunschild 2002; Rousseau 2018). Implicit internal information is more difficult to glean, as it often entails highly individual aspects, such as emotions or experience, and is influenced by the amount of trust or the reasons for hidden agendas that each group member has (Fu et al. 2017; Boone et al. 2018, 2019). Since decision makers can only interpret information that is available, the quality and completeness resulting from step two influences the rest of the process (Meissner 2014; Julmi 2019). In addition, the amount of information has an impact on the process, as especially in large organizations, most collected information is not needed, while the processing capacity remains limited (Feldman and March 1981; Fiori 2011; Roetzel 2018). Steps two and three, defined in this framework as knowledge management, continuously influence all further steps, as the flow of information never stops, implying that there can be an impact during a later step as well (Long 2017).
Based on the interpretation of the available information, shaped by the decision goal and the heuristics of the group, alternatives are determined in step four, for which probability and utility values are then assigned in step five. Finally, in step six, the group weighs the alternatives and makes the decision. In an ideal world, the resulting outcome matches the desired goal.
For the purpose of this article, the decision-making process consists of three stages, namely, input–process–output, which are linked to perception, interpretation, and actions, respectively. The framework hence connects to the continuum presented in Sect. 2.1.1.
Research to date has neither stipulated the steps for which the use of AI is suitable and in what way, nor is there an agreement on its benefits. The possibilities for bias, for example, have been found to even increase when using AI for decisions (for an overview, see Silva and Kenney 2018), as an AI application is executing a small decision-making process for itself each time it is used, based on the goal it is used for and the data it has available. As there is no dialogue possibility with the technology, scholars argue that it is often not clear how the system arrives at a certain output (Bolander 2019). On the one hand, each algorithm is only as good as the data input and the programmed process mining, which are usually both done by humans and thus might be biased (Barocas and Selbst 2016). This is dangerous, as humans are not able to compensate for failed algorithms (Vaccaro and Waldo 2019). On the other hand, some AI applications have been found to support the challenge of including ambiguous utility values (Metzger and Spengler 2019). The following literature review provides an analysis of the antecedents and consequences of applying AI in strategic organizational decision making and how to best combine it with human capabilities.
3 Research methodology
Following Meredith (1992), conceptual models that build on descriptions and explanations provide the best foundation for theory testing afterwards. For the purpose of this article, an SLR was used as the descriptive basis, as it is defined as a systematic approach that “informs regarding the status of present knowledge on a given question” (Rousseau et al. 2008: 500). It follows specific criteria and is re-executable (Tranfield et al. 2003), implying that it is reliable and combines all literature of a delineated research area. The structured summary also provides an in-depth understanding of results (Briner and Denyer 2012). This is expected to offer the necessary explanations to understand the phenomenon, resulting in a conceptual model for empirical testing. For a qualitative analysis of selected articles, this approach is further amplified by CA (Mayring 2008), which is a more iterative approach that integrates as much material on a topic as possible, while inductively building categories afterwards. It is a useful methodology for analyzing various influences on the correct design of processes, especially when linked to new technologies such as AI. The CA methodology has been employed by Glock and Hochrein (2011) to analyze purchasing organization design, Rebs et al. (2018) to study stakeholder influences and risk in sustainable supply chains, Nguyen et al. (2018) to analyze big data analytics in supply chain management (SCM), and Roetzel (2018) to study information overload. Combining SLR and CA methods ensures that all relevant literature, analyzed in a structured process, is included (Denyer and Tranfield 2009; see Table 1), thereby offering a detailed description and explanation of theory building (Meredith 1992).
Table 1 Systematic review process
Full size table

3.1 Search strategy
After a preliminary scope searching (Booth et al. 2016), the databases Business Source Complete (via EBSCO Host), Sciencedirect, ABI/inform (via ProQuest), and Web of Science were selected. These electronic databases are also acknowledged in the current literature and were chosen to provide fast and reliable access to appropriate articles. Furthermore, as AI is a rather technological topic, information was assumed to be primarily included in electronic databases.
The databases were searched using three search strings, each consisting of a combination of three groups of keywords resulting from the preliminary scope search. In the first group, to avoid inadvertently excluding results, fairly general terms relating to AI were used, namely AI and machine learning. A decision was made to not search for abbreviations, because in this research field, not only is the acronym AI common, but it is also used in different fields, resulting in potentially irrelevant results. For the same reason, the second group also included broad search terms, namely decision making and decision support, and the third group only included human machine. The preliminary search revealed that this search term captures all existing combinations that researchers use to define the relationship, although it led to remarkably fewer results. The search terms were coupled with Boolean operators AND/OR to search strings (Booth et al. 2016), which were entered into the databases from 2016 onwards in peer-reviewed journals. The start was set to 2016, as the search frequencies of AI on Google Trends (AI 2020) show an initially large increase after 2016, supported by Nguyen et al.’s (2018) literature review findings. The search strategy was adapted for all databases (see Table 4 in Appendix 1), as there were differences in user interface and functionalities (Booth et al. 2016).
3.2 Selection process
The database search resulted in 3458 articles, and 2524 after duplicates were removed, making a selection process necessary (see Fig. 5 in Appendix 1). Each study was evaluated according to established inclusion and exclusion criteria of quality and relevance regarding the RQ and its three sub-dimensions as mentioned in the introduction (Briner and Denyer 2012; see Table 5 in Appendix 1), resulting in a final total of 55 articles (see Appendix 3). The majority of articles were eliminated, as they were considered to be either too specifically tied to certain industries or not generalizable for answering the RQ or focused on operational decisions only. The only exception was the broad literature on SCM, which often focused on multi-stakeholder decisions, similar to the definition of organizational decisions due to the heterogeneous groups of people involved, and that literature was thus considered to offer relevant input to the analysis.
3.3 Classification of content
A deeper insight into the content of the articles is provided by classification, which aids in categorizing data to enable a more structured description (Mayring 2015) and to create new knowledge that would not be possible by reading the articles in isolation (Denyer and Tranfield 2009). The following classification categories relate to the topics addressed in the sample and the three previously stated sub-dimensions of the RQ:
Knowledge management with the help of AI.
Categorization of AI applications.
Impact of AI on organizational structures.
Challenges of using AI in strategic organizational decision making.
Ethical perspectives on using AI in strategic organizational decision making.
Impact of AI usage in strategic organizational decision making on the division of tasks between humans and machines.
The first two categories address the first sub-dimension (i.e., the possibilities of AI integration into the previously introduced decision-making process). The subsequent two categories, organizational structures and challenges, provide insight into the second and third sub-dimensions (i.e., possible pre-conditions and preparations necessary for a successful AI integration, and the challenges and consequences thereof), while ethical perspectives contribute to answering all three sub-dimensions of the RQ. The last category closes the analysis by addressing the first sub-dimension of the RQ directly with a proposed division of tasks between humans and AI, and it also includes findings on the possible consequences for the designation and development of the human role. The difficulty in defining categories that can be assigned perfectly to the sub-dimensions demonstrates the variety of aspects that researchers and practitioners relate to this topic. Moreover, it is important to mention that clustering for CA had to be done in several rounds, with much discussion among the research team, as many articles offer input and content for more than one category. However, as Table 2 indicates, each article was attributed to one category only. Frameworks or models are marked in bold and are also described in the following analysis.
Table 2 Overview of articles and assigned categories
Full size table

The final step of the methodological process, namely, the results and discussion based on the interpretation of data, is provided in the next chapter, followed by the conceptual integration of theory.
4 Results and discussion
4.1 Distribution of articles per year, journal, and research methodology
Assessing the distribution of articles per year, an increase can be seen over the entire period of observation (see Fig. 6 in Appendix 2). The highest rise occurred between 2018 (10 articles) and 2019 (28 articles). This might be due to a higher focus on the topic in the business area worldwide, starting in the last quarter of 2019 (Artificial Intelligence in Business and Industrial Worldwide 2020) and leading to an increase in scientific interest to analyze this topic from a business perspective. This is also evident in the increasing number of published articles in high-quality journals in 2019.
The 55 selected articles have been published in 42 different journals, while only 9 have more than 1 article included in the review (see Fig. 7 in Appendix 2). This illustrates the relevance of the topic for different disciplines, the high interest in the research field, and the various focus topics. This is also demonstrated by the respective focus of the magazines, from technology and computer systems to society and ethics, and journals focusing on business and management. The type and methodology of analysis (see Fig. 8 in Appendix 2), however, is rather theoretically oriented. Empirical approaches increase in 2019, where even conceptual articles try to relate their findings to practical observations and data.
Regarding the distribution of articles among the categories defined for the content analysis (see Fig. 9 in Appendix 2), a major focus is on the human–machine relationship (12) and ethical perspectives (12). In both categories, theoretical approaches are still dominant. Therefore, real-life examples of an implementation of AI into organizational decision making are assumed to be rare, making empirical analysis difficult. This seems to be different for the smallest category of knowledge management (6), which is the most practically analyzed one.
The following sections provide an overview of the articles per category of the CA, each of which deals with one or more sub-dimensions of the RQ, as explained in Sect. 3.3. Thereby, more detailed insights into the content of the articles are offered (for an overview see Table 2) to provide managerial implications that are only possible by synthesizing the findings (Denyer and Tranfield 2009). The basis for these implications will be the conceptual framework presented in Sect. 4.3. It offers an answer to the RQ based on a combination of the findings resulting from addressing the sub-dimensions.
4.2 Using AI as support for strategic organizational decision making
4.2.1 Knowledge management with the help of AI
Studies of the sample highlight that through the interaction between individuals and technological systems, new meanings and influences are expected to be created (Shollo and Galliers 2016). Researchers agree that AI can be used for the collection, interpretation, evaluation, and sharing of information, thereby providing support in speed, amount, diversity, and availability of data (Acharya and Choudhury 2016; Shollo and Galliers 2016; Bohanec et al. 2017a). In addition, Acharya and Choudhury (2016) highlight the opportunity to increase data quality, as too much, too little, or incorrect information can negatively affect decision outcomes, which is often the case in large organizations with complex structures.
However, Metcalf et al. (2019) raise the concern that the training of AI will be difficult, as data are constantly changing and complex in nature. They thus deem humans to be necessary to ensure the quality of information and interpretation, which is also supported by other researchers’ findings (Shollo and Galliers 2016; Bohanec et al. 2017a, b; Terziyan et al. 2018). In addition, especially for highly strategic decisions, implicit information has been found to be more important than pure analysis of facts (Acharya and Choudhury 2016; Bohanec et al. 2017a). Therefore, “while humans have access to both explicit and tacit knowledge, lack of access to tacit knowledge and the reliance on historical data from which patterns can be identified are major limiting factors of AI (…)” (Metcalf et al. 2019: 2). Some researchers even provide evidence that groups are capable of including some of these aspects through discussion (Shollo and Galliers 2016; Bohanec et al. 2017a; Metcalf et al. 2019).
Nevertheless, several researchers offer potential tools to make implicit knowledge available, as marked in bold in Table 2. The most holistic method for integrating all types of information into decision making is proposed by Terziyan et al. (2018): by cloning human decision makers, the patented intelligence (Pi-Mind) methodology attempts to capture soft facts and potential utility levels, although the quality of the clone always depends on the input data provided by humans. Acharya and Choudhury (2016: 54) call for an inter-organizational knowledge-sharing model to address the challenge that “an overemphasis on technology might force an organization to concentrate on knowledge storage, rather than knowledge flow.” As information quantity influences all steps of the decision-making process, these authors also state that resources within an organization should be allocated to enable efficient knowledge management (Acharya and Choudhury 2016).
The six articles in this category do not propose clear strategies on how to organize knowledge management, neither in general, nor with the help of AI. However, an agreement can be observed on AI supporting the amount and speed of information collection and interpretation. Nevertheless, the authors in this category argue that the resulting quality depends on human capabilities and willingness to disclose implicit information.
4.2.2 Categorization of AI applications
Almost all articles in the sample propose a set of AI applications to a certain extent. Table 3 clusters all the applications mentioned according to their use case and possible integration into the decision-making process defined in Sect. 2.3.
Table 3 Overview of mentioned AI applications related to process step and use case
Full size table

Researchers of this category agree on the stages of input–process–output, with related definitions on data being at rest, in collection, in transition, in motion, or in use. Parallel to this, respective applications increase in ability from purely statistical AI, which some researchers do not even classify as AI (Baryannis et al. 2019a), to human–machine AI (Blasch et al. 2019), supporting the framework in Sect. 2.1.2.
Scholars argue that determining which application to use depends on the type, quantity, and quality of data available, resulting in various necessities to handle the data, such as classification, clustering, or detection of connections (see Table 3). Moreover, as several applications can be used for both top–down and bottom–up approaches (Flath and Stein 2018; Mühlroth and Grottke 2018; Baryannis et al. 2019a, b; Blasch et al. 2019), the purpose for which the specific application should be used is identified as an additional influence (Blasch et al. 2019).
Articles use and recommend a hybrid approach, as mathematical models are, on the one hand, found to be less capable of handling a large amount of data, which, on the other hand, is needed to train machine learning applications, often based on mathematical ones (Baryannis et al. 2019b; Blasch et al. 2019). Therefore, Simon’s (1995) definition of AI applications being more than mathematical theorems is supported.
The articles discuss potential and hypothetical use cases for AI, mainly with the goal of data interpretation, alternative creation, or probability and preference definition, possibly even related to an evaluation of consequences (Pigozzi et al. 2016; Baryannis et al. 2019a, b). Information collection is seen as a task to be completely fulfilled by AI. It is related to the generation of information from various and numerous sources with differing techniques, such as natural language processing, text mining, or other data mining possibilities (Baryannis et al. 2019b; Blasch et al. 2019). Nevertheless, executing feature engineering afterwards to reduce input bias as far as possible is said to be necessarily done by humans, assisted by top–down applications (Flath and Stein 2018).
There is disagreement on how useful AI applications are in general for organizational decision making. As Baryannis et al. (2019b) found in their literature review, the majority of studies analyzed do not see any decision-making capability, although some articles provide bottom–up applications as decision support systems. Practical experiments in the sample instead refer to information gathering and status tracking within production or logistics [e.g., the data science toolbox of Flath and Stein (2018), supply chain risk management tools by Baryannis et al. (2019a, b), and the self-thinking supply chain of Calatayud et al. (2019)] with the exception of Colombo (2019), who introduced holistic risk analysis and modeling (HoRAM) as an already tested application to be used for almost the whole decision-making process in dynamic environments.
In summary, although scholars do not agree on what to classify as an AI application and whether it is useful for decision making, the consensus is that the choice of application is influenced by various dimensions of data and the basic reason for which the technology is intended to be used. Relating to Fig. 2, most applications that the articles refer to can be clustered as top–down, as they are not able to act self-consciously (i.e., human-like; Blasch et al. 2019). With increasing research and development efforts, however, the literature expects the capabilities of AI applications to increase and to shift to the right (Mühlroth and Grottke 2018; Colombo 2019).
4.2.3 Impact of AI on organizational structures
Von Krogh (2018: 405) supports Herbert Simon’s findings, stating that organizational structures are closely linked to decision making, as they result from the limited human processing capacity: “To mitigate this problem, information-processing and decision-making authority can be delegated across roles and units that display various degrees of interdependence.” The organizational strategy and resulting goals have been found to be an important influence, not only for this definition of roles and relationships to make information manageable, but also on all steps of the strategic decision-making process relating to Fig. 3 (von Krogh 2018).
Organizational strategy and goals are further said to determine the reasons for which AI is used (Bienhaus and Abubaker 2018; von Krogh 2018; Butner and Ho 2019; Paschen et al. 2019). They are also discussed as the basis for an adaptation or creation of structures, which is expected to be necessary to make AI integration possible (von Krogh 2018; Paschen et al. 2019; Udell et al. 2019). However, von Krogh (2018) also argues that structures change as soon as AI applications are actively used, thus influencing processes and responsibilities. In their surveys, Bienhaus and Abubaker (2018) and Butner and Ho (2019) recommend completely re-building and re-thinking processes rather than placing new ones on top of old structures. To support organizations with establishing these new processes, Paschen et al. (2019) developed a framework with four dimensions to assess whether the introduction of AI leads to an innovation in products or processes, as well as whether it is competency-enhancing or -destroying, thereby referring to humans as well. Depending on the combination of these four dimensions, firms can “generate different value-creating innovations” (Paschen et al. 2019: 151).
Lismont et al. (2017) offer another perspective, categorizing companies according to their readiness for technology implementation. They conclude that the more mature a company is in using AI, the higher the variety of applications, the number of affected processes, and related goals are. Due to interdependencies, Tabesh et al. (2019), therefore, argue that the complex construct of organizations should only be changed in steps and always while carefully referring to the defined strategy.
In summary, organizational structures are the foundation for successful AI integration, and vice versa, the use of AI in decision making also influences those structures. The strategic reasons for implementing AI inform the type and location of AI used. However, the available applications are also expected to influence existing decision-making processes that are to be adapted to make usage possible.
4.2.4 Challenges of using AI in strategic organizational decision making
To determine whether, how, and why to integrate AI into existing business processes, AI literacy has been found to be crucial (Kolbjørnsrud et al. 2017; Lepri et al. 2018; Canhoto and Clear 2019), as “not every decision problem needs to be solved by technology” (Migliore and Chinta 2017: 51). Researchers define AI literacy as a profound understanding of the technology and its possibilities and limitations, which according to Whittle et al. (2019) is often missing. To increase AI literacy, scholars have argued that the involvement of the employees who will be affected by AI integration, rather than only top management, is crucial, as acceptance differs across levels (Kolbjørnsrud et al. 2017; Bader et al. 2019). Stakeholders have been found to necessarily gain a sense of ownership, and by familiarizing themselves with the technology and actively being part of the integration, they are able to define their role. Therefore, according to the literature, education and training constitute a highly important task (Kolbjørnsrud et al. 2017; Watson 2017). The authors such as Migliore and Chinta (2017), Bader et al. (2019) and Whittle et al. (2019) recommend analyzing which capabilities are needed by which employee to leverage the technology’s potential, thereafter enabling each individual to successfully work with AI for assigned tasks. This also implies that executives need to guide employees through this process, based on their own literacy and understanding of the technology (Kolbjørnsrud et al. 2017; Watson 2017; Whittle et al. 2019).
Soft skills in general have been argued to become increasingly important with the introduction of AI in organizational decision making (Kolbjørnsrud et al. 2017), including a focus on training employees in capabilities for collaboration, creativity, and sound judgement. AI is recommended to be introduced step wise (Kolbjørnsrud et al. 2017; Watson 2017) as trust into the technology increases with experience and understanding. Employees become accustomed to using it for tasks for which machines have not been used before (Kolbjørnsrud et al. 2017; Lepri et al. 2018). Transparency, referring to “information about the nature and flow of data and the contexts in which it is processed” (Singh et al. 2019: 6563) to reach a certain decision (Canhoto and Clear 2019), is crucial for a successful introduction and usage as well. The articles in this category suggest a heterogeneous introduction team consisting of new and established organizational executives (Kolbjørnsrud et al. 2017; Lepri et al. 2018) and people with sufficient training (Watson 2017). Scholars again claim that finding the right introduction team and providing support over the process are the responsibility of leadership. Kolbjørnsrud et al. (2017) found that top executives possess a higher awareness and understanding of their responsibility to invest time and guide employees through this process, than middle managers do.
Further challenges that the majority of authors in the category have addressed are data security and data privacy issues, as well as the danger of data manipulation, which must be evaluated before implementing new technologies (Kolbjørnsrud et al. 2017; L’Heureux et al. 2017; Lepri et al. 2018; Canhoto and Clear 2019; Singh et al. 2019; Whittle et al. 2019). The articles assume that the resulting transparency and literacy help to decrease bias. Migliore and Chinta (2017) also found that having more available data is helpful. These authors define bias as bounded rationality, which contrasts the definition in this study (see Introduction). Therefore, this assumption is questioned, as additionally, the right quantity and quality of data has been found to be a challenge in itself (Lepri et al. 2018; Canhoto and Clear 2019). Bellamy et al. (2019: 78) suggest that “machine learning is always full of statistical discrimination,” meaning that even machines are biased. Some frameworks have thus been proposed to offer solutions for fair pre-processing, in-processing, and out-processing, for example the AI Fairness 360 (Bellamy et al. 2019) and the Open Algorithms (Lepri et al. 2018), but they are also said to only reduce bias. Furthermore, Canhoto and Clear (2019) demonstrate that decision quality always depends on the application used, the resources available, the input provided, and the interpretation ability of the humans using it.
The literature thus suggests that education and training, combined with an awareness of data security issues, lead to literacy and transparency, thereby decreasing caveats. Furthermore, focusing on the active involvement of affected employees and a step-wise introduction has been found to result in a successful implementation. Through these factors, even though the danger of active or implicit bias might not decrease, at least awareness is supported. However, the majority of authors have also claimed that in processual and structural implementation, the important aspects of ethics and morality should not be forgotten.
4.2.5 Ethical perspectives on using AI in strategic organizational decision making
Although all researchers of this category state that an ethical framework is needed to use AI in organizational decision making, there is no agreement on the design. Some recommend an implementation of decision rules into AI systems (Webb et al. 2019; Wong 2019), while others concentrate on making the machine learn moral guidelines by itself (Bogosian 2017), relating to top–down and bottom–up approaches of AI.
Morally or socially correct behavior and the resulting implicitly learned societal rules are claimed to be rather subjective (Cervantes et al. 2016; Etzioni and Etzioni 2016; Bogosian 2017; Giubilini and Savulescu 2018). Some researchers, therefore, propose a combination of legal frameworks (Vamplew et al. 2018), although these alone cannot include the complex and often conflicting factors that humans incorporate into decisions: “What may be right for one person may be completely inaccurate for the other.” (Cervantes et al. 2016: 281).
Parisi (2019: 26) states that “the question of automated cognition today concerns not only the capture of the social (and collective) qualities of thinking, but points to a general re-structuring of reasoning as a new sociality of thinking,” requiring a new understanding and definition of aspects such as fairness, responsibility, moral fault, or guilt. In an attempt to offer a new definition, several researchers have analyzed the behavior humans exhibit when working with artificial agents, especially in terms of attributing human values and shortcomings to the machines. The UnBias project by Webb et al. (2019) demonstrates that fairness is the guiding principle in decisions, though the understanding of fairness differs among participants. Wong (2019) lists conditions to ensure fairness. Among them, the transparency of the decision process and the inclusion of all affected stakeholders’ perspectives are as important as a regulatory framework. Other researchers have analyzed the differences in the definitions of ethical aspects for human-only, AI-only, or combined decision making and found that moral fault was always attributed to humans (Shank et al. 2019). Kirchkamp and Strobel (2019) discovered that the feeling of guilt also does not change, while responsibility in human-AI teams is perceived as being higher than in human-only teams, and selfish-acting decreases. According to their findings, any higher form of moral responsibility is so far not attributed to machines. In addition, Hertz and Wiese (2019) found that people choose machines for analytical questions, while human advisors are preferred for social and personal topics.
In summary, articles on ethics are as divided as the topic of AI itself. “Legal and safety-based frameworks (…) are perhaps best suited to the more narrow AI which is likely to be developed in the near to mid-term” (Vamplew et al. 2018: 31), and they, therefore, seem to be the only frameworks agreed on as a guiding principle (Etzioni and Etzioni 2016; Wong 2019). Researchers thus assume that including ethical guidelines into algorithms is only possible to a limited extent and is always influenced by the people designing them, although several researchers have proposed tools to support this inclusion (Cervantes et al. 2016; Etzioni and Etzioni 2016; Giubilini and Savulescu 2018; Vamplew et al. 2018). A new definition of social and moral norms and aspects in relation to AI is argued to be necessary. As no clear recommendation can be derived on how to solve this challenge, Vamplew et al. (2018) assume that a step-wise procedure is required to agree on ethical guidelines and the extent to which case-based judgement remains.
4.2.6 Impact of AI usage in strategic organizational decision making on the division of tasks between humans and machines
Most articles in this category claim that the “unique strengths of humans and AI can act synergistically” (Jarrahi 2018: 579), implying that through the combination of human and AI capabilities, efficiency and profitability in decision making are expected to increase (Smith 2016; Anderson 2019; Shrestha et al. 2019). Furthermore, it is widely agreed that humans and machines can augment each other, implying that AI systems learn from human inputs and vice versa (Jarrahi 2018; Schneider and Leyer 2019). This assumption is also supported by the authors from other categories, such as Kolbjørnsrud et al. (2017), Terziyan et al. (2018), von Krogh (2018) or Blasch et al. (2019), demonstrating the relevance this topic has on other aspects as well.
Researchers offer several frameworks for dividing tasks between AI and humans, usually ranging from full delegation to AI or hybrid forms, through to human-only decision making (Shrestha et al. 2019; Yablonsky 2019). Parry et al. (2016) and Agrawal et al. (2019), however, are the only authors who consider the potential of allowing AI to make decisions completely independently as being realistic. Nevertheless, they also claim that this is not suitable for all types of decisions, making “the retention of a veto power when the decisions can have far-reaching consequences for human beings (…)” necessary (Parry et al. 2016: 17). Bolton et al. (2018: 55) identify AI as being able to “automate tasks,” which “allows humans to focus on work that will add value”, while Klumpp and Zijm (2019) speak of the artificial divide, meaning that humans become supervisors more than executors. Thus, using AI to automate some tasks of the decision-making process gives people time to invest in those skills that AI cannot adequately perform, but which are critical to strategic decisions. The other authors further argue that humans are better at judgment, the analysis of political situations, psychological influences, flexibility, creativity, visionary thinking, and equivocality (Parry et al. 2016; Smith 2016; Rezaei et al. 2017; Jarrahi 2018; Agrawal et al. 2019; Shrestha et al. 2019). In addition, “even if machines can determine the optimal decision, they are less likely to be able to sell it to a diverse set of stakeholders.” (Jarrahi 2018: 582).
To summarize this category, the authors claim that AI offers the potential for machines to augment human capabilities, and vice versa, while it also changes the human role to become more of a supervisor. The authors hence expect a rather limited integration possibility of this technology into a process such as strategic organizational decision making, where capabilities are needed that only humans are argued to possess.
Lyons et al. (2017), therefore, claim that for the relationship between humans and machines to work, all involved parties must understand the tasks, responsibilities, and duties, and a high level of transparency is required, which is similar to the organization of human-only relationships. A possible concept of how this can be defined for the purpose of strategic organizational decision making is provided in the following.
4.3 Conceptual framework for AI integration into the organizational process for decision making under uncertainty
The majority of researchers in the sample support the decision-making process in Sect. 2.3 (Bohanec et al. 2017a; von Krogh 2018; Shrestha et al. 2019) and its usage as guidance for addressing the sub-dimensions of the RQ. Derived from the analysis of research in the previous sections, Fig. 4 thus presents the conceptual framework as an elaboration on Fig. 3. As the arrows indicate, the majority of categories are expected to not only influence the process itself, but also be influenced by it. In addition, some categories even impact one another. Therefore, not all categories can be attributed exclusively to one sub-dimension of the RQ. Next, the parts of the conceptual framework are explained in more detail.
The majority of researchers claim that strategic organizational decision making is a people-driven and -dependent task in which technology can only be used as support, although most of the researchers in Sect. 4.2.6 expect humans and AI to augment each other. Regarding the first sub-dimension of the RQ, the conceptual framework presents the possible division of tasks between human decision makers and technology, with the task of knowledge management as its own category. This task combines several aspects and must be considered thoroughly by itself, as the results of Sect. 4.2.1 have demonstrated. Researchers agree that AI has the potential to collect large amounts of information from numerous sources, leverage sharing, and facilitate interpretation, implying that using it for the knowledge management task can increase speed and efficiency (Acharya and Choudhury 2016; Shollo and Galliers 2016; Blasch et al. 2019; Butner and Ho 2019). However, AI is said to be unable to solve the inherent challenge of making implicit data available that stakeholders and decision makers are not willing or able to provide, although initial possibilities for overcoming this challenge have been proposed (Terziyan et al. 2018; Colombo 2019; Metcalf et al. 2019). The quality of implicit information is thus further believed to depend on humans and can only be evaluated and framed through human discussion (Rousseau 2018).
With the overview of the task division, the framework summarizes the current academic discussion on tasks for which AI is expected to be useful. The technology’s successful integration and usage, however, has been found to depend on the respective AI application, and vice versa, as Fig. 4 also highlights. Furthermore, while utility calculations are said to depend on humans (Pigozzi et al. 2016), researchers argue that AI can provide a forecast of how each decision alternative might affect the organization or partners (Agrawal et al. 2019; Baryannis et al. 2019a, b; Colombo 2019). This might influence the weighing of alternatives, for which the pure mathematical calculation can be carried out by AI as well. The final decision, however, must be taken by the human decision group only. With the current state of technology available, AI can thus leverage the stages of input and process (Bohanec et al. 2017a; von Krogh 2018), with the most significant impact on knowledge management (Mühlroth and Grottke 2018; Blasch et al. 2019). This indicates that most existing applications cannot be defined as AI at all based on Nilsson’s (2010) definition of intelligence.
The choice of application is also influenced by organizational structures and the related allocation of resources. However, evidence from research also suggests that this impact is reciprocal, implying an influence of AI applications on the definition of organizational structures (Lismont et al. 2017; Tabesh et al. 2019). According to the literature, this category is both a pre-requisite for and a consequence of introducing AI into the process of decision making under uncertainty (von Krogh 2018; Paschen et al. 2019; Tabesh et al. 2019). For this introduction to be successful, an important foundation is the organizational strategy and the resulting reasons for which AI is used and integrated into the decision-making process, such as knowledge management (Bienhaus and Abubaker 2018; von Krogh 2018; Butner and Ho 2019).
AI literacy and data transparency are further pre-requisites of AI integration, as highlighted in the middle of the framework. Scholars agree on the importance of enabling employees to use the technology beneficially (Lepri et al. 2018; Canhoto and Clear 2019). They must learn which application to choose for which task, which data need to be provided for the application to work correctly, and how results should be interpreted. In addition, training and continuous experience of working with the technology has been found to increase trust and thus effectiveness (Kolbjørnsrud et al. 2017; Lepri et al. 2018).
As the analysis in Sect. 4.2.5 highlighted, ethical perspectives influence all other categories. The question as to who is morally responsible or what framework the machines should act in accordance with, has not yet been solved, nor is it possible to include moral guidelines in algorithms (Cervantes et al. 2016; Vamplew et al. 2018). Evidence suggests that, to date, machines are not related to any moral responsibility, implying the necessity to adapt the definition of moral constructs such as guilt or fairness (Parisi 2019). Any successfully proven approach of how to realize this, however, is missing.
Based on the analysis of the previous sections, and as Fig. 4 highlights, the answer to the RQ is influenced by a variety of aspects. This makes it difficult to provide a clear definition or guideline of how to best integrate AI into the organizational process of decision making under uncertainty. Researchers have found that AI always depends on a clear goal, as it cannot handle uncertainty or input complexity (Smith 2016; Jarrahi 2018; von Krogh 2018). Relating the findings to Nilsson (2010), a current AI application can thus only “interact with foresight in its environment” when used by humans. This contrasts with Simon’s (1986, 1995) theory of computers and humans being alike. Nevertheless, some researchers have also proposed developing AI further as a net-based and learning algorithm, which would yield more capabilities of intelligence than it currently has (Parry et al. 2016; Watson 2017; Agrawal et al. 2019; Bolander 2019), although there is no agreement on whether AI will ever be able to exercise implicit human capabilities (Parisi 2019; Shrestha et al. 2019). In addition, research suggests that AI is unable to serve as a substitute for all benefits of human group decision making (von Krogh 2018), and using it can also amplify the dangers and challenges that human decision making entails (Flath and Stein 2018; L’Heureux et al. 2017). Moreover, especially when it comes to individual decision making, AI is assumed to have a less beneficial effect. The diversity of experience and other soft skills can only be provided through human negotiation and discussion, as “it is easier to recognize biases in other people than in ourselves” (Rousseau 2018: 137).
Therefore, utilizing AI as support in this important organizational process implies a role change for human decision makers. As the literature states, they become supervisors (Bolton et al. 2018; Klumpp and Zijm 2019), a role that has to be interpreted differently than it is defined in traditional production processes. However, supervising AI has manifold dimensions, and a deep understanding of AI’s functioning and the ability to translate and interpret its results correctly are crucial for a successful and responsible use of this technology (Lyons et al. 2017; Canhoto and Clear 2019; Whittle et al. 2019). This leads to several managerial implications and research possibilities, which are presented in the following chapter.
Fig. 4

Conceptual framework for AI integration into the organizational process for decision making under uncertainty
Full size image

5 Concluding remarks
5.1 Managerial implications
The analysis of the AI applications revealed that researchers disagree on whether current applications are useful for strategic decisions (Shollo and Galliers 2016; Baryannis et al. 2019b). Therefore, proposals for implementation strategies are rare.
Deduced from the organizational strategy, managers are recommended to first specify the reason for integrating AI and the resulting decision tasks to be supported. In line with this follows the adjustment of organizational structures to make AI integration possible. Third, the applications to be used must be stipulated. However, as the results have shown, each of these steps can also influence the other, so it is not a stringent but very individual implementation process. Scholars argue that the AI literacy of managers is crucial to become aware of the possibilities and challenges of AI, which in turn enables managers to make the most efficient use of the technology (Kolbjørnsrud et al. 2017; Whittle et al. 2019).
Scholars, however, emphasize the importance of being aware that with an integration of AI into the strategic decision-making process, the human role is expected to change. This means a shift in responsibility, which simultaneously requires a focus on other skills (Kolbjørnsrud et al. 2017; Bolton et al. 2018; Bader et al. 2019; Klumpp and Zijm 2019). Therefore, researchers suggest that employees and managers alike should engage in training those capabilities that AI does not possess, such as empathy, creativity, and emotions (Parry et al. 2016; Jarrahi 2018; Terziyan et al. 2018; von Krogh 2018; Schneider and Leyer 2019).
It can consequently be stated that human groups remain important, although AI offers some benefits, such as information amount and diversity, which can usually only be gained with the inclusion of more people in the decision-making process. Smaller teams, thus, are expected to increase efficiency and speed, as less negotiation is needed. Here, it is important to ensure that diverse group members are chosen with the necessary skills for strategic decision making and AI usage. This, however, also increases the risk of few people possessing too much power and managers must always be aware that the use of AI can bring additional dangers and challenges, such as bias in several dimensions (Flath and Stein 2018; L’Heureux et al. 2017).
Some studies have provided frameworks for analyzing the readiness of an organization or the necessary steps to become more AI-based (see Table 2; Watson 2017; Canhoto and Clear 2019; Yablonsky 2019). Nonetheless, ethical frameworks must still be developed, although this perspective is discussed with increasing awareness (Bellamy et al. 2019; Parisi 2019; Shank et al. 2019; Webb et al. 2019). Managers are hence required to actively engage in this when developing and extending the use of AI.
5.2 Limitations and further research possibilities
This study has some limitations. The first relates to the methodology that was employed. Although the steps of Tranfield et al. (2003) and Mayring (2008) were followed, bias might have been introduced through the definition of keywords, which would have influenced the search and interpretation. As categories were defined rather broadly, articles on some specific topics might be missing. Nevertheless, the decision to carry out broad research and use broad interpretation categories was made to include as much data as possible and to obtain a general understanding of a rather undefined topic with many research objectives. Moreover, including further keywords relating to statistical or mathematical applications might have expanded the findings, as articles that do not mention AI when using these applications would also have been included. However, since there is so far no clear definition of which applications to include when speaking of AI, a decision was made not to enlarge the number of keywords. This allowed for an understanding of the current state of AI in decision making rather than biased results.
Searching in only four databases is another bias-related limitation, but searches in other databases would not have been possible in the same manner due to technical constraints in the search fields. As AI is a fairly practice-oriented topic, it might also have been interesting to include more practical views, however, this was not possible due to the peer-reviewed criterion. As the literature review of Calatayud et al. (2019: 26) demonstrates, non-scientific articles currently dominate the topic. For this reason, the suggestion would be to enhance this literature review by adding practical literature.
The fact that the small number of articles in this literature review leads to a myriad of different topics highlights the uncertainty that might only be solved by testing several designs. The potential for leveraging the best of both worlds could be seen in the analysis, and further, especially empirical, research is thus needed to analyze the possibilities of AI and the potential results of its integration into human-centered processes, such as decision making (von Krogh 2018). As most companies are still in the piloting and planning phase (Butner and Ho 2019), opportunities increase to uncover other interesting results. In this regard, the following would be helpful: a clear definition of AI and related applications, as well as initial process concepts demonstrating how to integrate the technology into decision-making structures and how to establish a partnership with the humans involved. The framework provided in Fig. 4 could be a useful starting point, while theory wise, the actor-network theory might serve as a basis, as it might help to explain when and how the responsibility changes from human to non-human actors.
5.3 Conclusion
This article is the first to focus on the current status of research about AI’s potential to become a support in strategic organizational decision making, that is, group decision making under uncertainty. It set out to answer the following question: How can AI support decision making under uncertainty in organizations?
The conceptual framework of Fig. 4 (see Sect. 4.3) provides the synthesis of findings from the analysis of current literature on this question. It takes into account the necessary pre-conditions for and potential consequences of combining human decision makers and AI, as well as a potential task division.
This study revealed that the established understanding of machines as tools is not suitable for AI. Successfully using this technology requires human decision makers to change their role and become translators and interpreters of the results rather than only supervising the machine with the execution of a predefined process. This also implies an increase in responsibility and change in the skills needed. Therefore, the way in which to view AI will heavily depend on how humans view themselves (Mueller 2012), while its benefits also greatly depend on the context and goal. While Lawrence’s (1991) framework of complexity and politicality is expected to remain, the resulting applications might further change with the development of the technology as a learning algorithm. Assuming that computing machines and humans are equal, however, is neither to be expected, based on current research, nor ethically supported (von Krogh 2018).",https://link.springer.com/article/10.1007/s40685-020-00133-x
The Impact of Artificial Intelligence on Business Strategy and Decision-Making Processes,ResearchGate,"Gonesh Chandra Saha, Reshmi Menon, Sudha Paulin","ARTICLE This research paper examines the integration of artificial intelligence (AI) into business strategies and its effects on decision-making processes, highlighting AI's potential to enhance efficiency, effectiveness, and innovation in businesses while addressing challenges such as ethical concerns and data privacy, and concludes with strategic recommendations for organizations.",2023," Artificial  intelligence  refers  to the  development  of  computer systems  that  can perform  tasks  that 
typically  require  human intelligence,  such  as speech  recognition,  problem-solving,  and  learning. As AI  technologies 
continue  to  advance  rapidly,  organizations  are  increasingly  integrating  AI  into  their  business  strategies  to  gain  a 
competitive  advantage.  The  study examines the  integration  of AI  into  business  strategies  and its  effects  on decision-
making processes. Additionally, it  investigates the potential  benefits and challenges associated with the adoption of AI 
in business contexts. Through a comprehensive literature review and analysis, this research provides  insights  into  the 
current  state  of  AI  implementation  in  businesses.  The  findings  indicate  that AI  has  the  potential  to  revolutionize 
business  strategies  and  decision-making  processes  by  enhancing  efficiency, effectiveness,  and  innovation.  However, 
challenges related to ethical considerations, data privacy, and the need for human oversight must be addressed to ensure 
responsible  and  effective  AI  adoption.  The  study  concludes  with  recommendations  for  organizations  to  effectively 
leverage AI  in  their  business strategies  and  decision-making processes.  Understanding  the  impact of AI  on business 
strategy  and  decision-making empowers  organizations  to harness  the  transformative potential  of  AI  and  navigate the 
dynamic and complex business landscape. 
 
Keywords:  - Artificial  intelligence,  Business  strategy,  Decision  making  process,  AI  integration,  Benefits  and 
Challenges, AI implementation, Ethical considerations. 

1. Introduction: - In a variety of industries, artificial intelligence (AI) has emerged as a transformative force that 
has revolutionized business operations and decision-making. AI has the potential to significantly alter business strategy 
and decision-making procedures due to its ability to imitate human intelligence and perform intricate tasks. The purpose 
of this research paper is to  investigate the significant impact  that AI has had on these essential  aspects of the operation 
of  an organization.  Organizations'  plans  and  actions  to  achieve their  goals  and  objectives  are  included  in  business 
strategy.  Organizations  are  constantly  looking  for  novel  strategies  to  gain  a  competitive  advantage  in  the  highly 
competitive and rapidly changing business environment of today. The incorporation  of artificial intelligence (AI) into 
business plans has  emerged as a potent  means by which businesses can  boost  their capabilities, streamline operations, 
and adjust to shifting market dynamics. Additionally, decision-making is a crucial process that influences organizations' 
direction  and  outcomes. Organizations  have  unique  opportunities  to boost  accuracy,  effectiveness,  and  efficiency by 
incorporating AI technologies  into  decision-making  processes.  Managers can  make  decisions  that are  more  based  on 
data  thanks  to AI  systems'  ability  to  process  massive  amounts  of  data,  spot  patterns,  generate  insights,  and  offer 
recommendations  in  real time.  The effect  of simulated  intelligence on  business  procedure and  independent direction 
stretches out past functional productivity. It has the potential to open up new growth opportunities, make it possible for 
innovative business models, and make customer experiences better. However, the use of AI has a number of drawbacks 
that  must  be  carefully  addressed  in  addition  to  its  potential  advantages.  When  relying  on  AI  systems  for  crucial 
decision-making,  ethical  considerations  like  transparency,  fairness,  and  accountability  become  crucial.  Additionally, 
organizations must strike a balance  between using data  for AI-driven insights and protecting sensitive information due 
to concerns about data privacy and security. This research paper will examine the current state of AI implementation in 
business settings, including the integration of AI into business strategies and its  impact on decision-making processes, 
through a comprehensive literature review and analysis. It will look at the obstacles that need to be overcome as well as 
the  benefits  that  AI  adoption  could  bring  to  businesses.  Organizations  can  effectively  harness  the  transformative 
potential  of  AI  while  navigating  the  ethical  and  practical  considerations  associated  with  its  implementation  by 
comprehending the implications of AI on business strategy and decision-making. It has the potential to change the way 
businesses work because it can process and analyze a lot of data, learn from patterns, and mimic human intelligence. It 
enables  businesses  to make decisions  that  are better  informed,  driven  by data,  and  predictive,  resulting  in  improved 
outcomes and competitive advantages. 
 
The  incorporation  of  AI  into  organizations'  overall plans  and  actions  is part  of  the  impact  on  business  strategy.  By 
facilitating  innovation,  identifying market  trends,  optimizing resource  allocation,  and  providing  valuable insights,  AI 
can assist in strategic decision-making. It enables businesses to optimize their competitive position, identify new growth 
opportunities, and adapt to shifting market conditions. 
AI  has  the  potential  to  speed  up,  improve  accuracy,  and  speed  up decision-making  processes.  It  has  the  ability to 
automate  routine  and  repetitive  tasks,  allowing  managers  to  concentrate  on  more  intricate  and  strategic  decisions. 
Decision-makers  can  access  real-time  insights  and  make  more  precise  judgments  thanks  to  AI  systems'  ability  to 
analyze  data,  identify  patterns,  and  generate  recommendations.  By  reducing  biases  and  human  errors,  AI  can  also 
enhance decision-making quality and  objectivity. It  is able to find  hidden patterns and correlations  in data that human 
decision-makers might not see, allowing for better-informed choices and predictions. 
 
2. Literature Review and Analysis: - Artificial intelligence (AI) has gained significant attention in recent years 
as organizations across various industries  seek to  harness  its  potential  to  drive  business  strategy  and  decision-making 
processes.  This  literature  review  aims  to  explore  the  existing  body  of  knowledge  on  the  impact  of  AI on  business 
strategy  and  decision-making,  providing  insights  into  the  key  themes,  findings,  and  gaps  in  the  current  research 
landscape. 
 
2.1 AI Integration  in  Business  Strategies: - Several  studies  have examined  the  integration  of AI  into  business 
strategies  and  its implications.  Chui  et  al.  (2018) emphasize  the  transformative  role  of AI  in  enabling new  business 
models  and  enhancing operational  efficiency.  They  highlight  the  importance  of  aligning AI  initiatives  with  strategic 
goals and  organizational capabilities to drive  competitive advantage. Porter and Heppelmann  (2017) argue t hat AI can 
enable  organizations  to  create  new  value  propositions,  enhance  customer  experiences,  and  optimize  supply  chains, 

leading to superior  business  strategies.  Organizations  need  to identify opportunities to leverage AI  to  disrupt  existing 
business models or create entirely new ones, thereby unlocking new avenues for growth and competitive advantage. 
 
2.2 Impact of AI on Decision-Making Processes: - The influence of AI on decision-making processes has been a 
focal point  in  the  literature.  Brynjolfsson  and  McAfee  (2017)  assert that AI  can augment  human  decision-making  by 
processing  large  volumes  of  data, identifying  patterns,  and  generating  insights.  They  suggest that  combining  human 
expertise with AI capabilities can lead to improved decision outcomes. Srinivasan et al. (2019) emphasize the role of AI 
in  supporting  real-time  decision-making  through  predictive  analytics,  enabling  organizations  to  respond  swiftly  to 
market changes and optimize resource allocation. 
 
2.3 Benefits and Challenges of AI Adoption:  - Studies have highlighted  the potential benefits  of AI adoption in 
business  settings.  Many  researchers  point  to enhanced  efficiency  and  productivity  as key  advantages.  Li  and Liang 
(2018) discuss the role of AI in automating routine tasks, reducing operational costs, and increasing productivity. They 
also  highlight  the  potential  for AI  to  improve  accuracy and  speed  in  decision-making  processes.  Kiron et  al. (2018) 
emphasize the  value of AI in enabling  data-driven  insights, facilitating innovation, and driving competitive advantage. 
While the potential benefits of AI adoption are evident, researchers have also  identified  challenges  organizations  face 
during implementation. Data privacy and security concerns are also highlighted, with researchers emphasizing the need 
for robust safeguards and compliance measures. 
 
2.4 Current State of AI Implementation in Businesses: - Various studies  have explored the current state of AI 
implementation  in  business  contexts.  A  survey  conducted  by  Accenture  (2017)  revealed  that  organizations  are 
increasingly  investing  in AI  to  gain  a  competitive  edge.  The  study  highlighted  the  adoption  of  AI  across  diverse 
functions, including  customer service,  operations,  and  marketing. Additionally, Kagermann  et  al.  (2019)  conducted  a 
study  on AI  implementation  in German  companies, highlighting  the  challenges  faced, such as  organizational  change 
management and skill gaps. 
 
3. Integration of AI with Business Strategy: - The integration of artificial intelligence (AI) in business strategy 
has  become  increasingly  important  as  organizations  recognize  the  transformative  potential  of  AI  technologies.  This 
section provides an analysis of the integration of AI in business strategy, highlighting key findings and implications. 
 
3.1 Strategic  Alignment:  - The  analysis  reveals  that  successful  integration  of AI  in  business  strategy  requires 
strategic  alignment.  Organizations  need  to  align  AI  initiatives  with  their  overall  strategic  objectives  and  core 
competencies. This ensures that AI is utilized to address specific business challenges and opportunities effectively. The 
findings suggest that organizations that align AI initiatives with their strategic goals are more likely to achieve positive 
outcomes and competitive advantage. 
 
3.2 Creating  New  Business  Models: -The  analysis  shows  that AI enables  organizations  to  create new  business 
models and disrupt existing ones. By leveraging AI technologies, organizations can identify innovative ways to deliver 
products and services, optimize supply chains, and engage with customers. AI-powered platforms, predictive analytics, 
and automation capabilities open up new avenues for revenue generation and value creation. The findings highlight the 
need for organizations to explore  and  embrace AI-driven business model innovations to  stay competitive  in  a  rapidly 
changing business landscape. 
 
3.3 Enhancing  Operational  Efficiency:  -AI integration  in  business  strategy significantly  enhances  operational 
efficiency.  Through  automation  and  intelligent  process  optimization,  organizations  can  streamline  operations,  reduce 
costs, and improve  productivity. The  analysis  indicates that AI  technologies, such as machine learning  algorit hms and 
robotic process automation, enable  organizations to automate routine and  repetitive tasks, freeing up human  resources 
for higher-value activities. This leads to improved efficiency, faster decision-making, and resource optimization. 
 
3.4  Fostering  Innovation:  -  The  analysis reveals that AI  integration in business strategy  fosters  innovation. AI 
technologies provide  organizations with the ability to extract insights from vast amounts  of data, identify patterns, and 

uncover  hidden  opportunities. The  findings suggest  that  organizations  that  leverage AI  to  drive  innovation  are more 
likely to develop new products and services, enhance customer experiences, and gain a  competitive edge. AI facilitates 
data-driven  decision-making,  supports experimentation,  and  enables  agile responses  to  market dynamics,  fostering  a 
culture of innovation within organizations. 
 
3.5 Overcoming Challenges: - The  analysis identifies several  challenges  associated with the integration of AI in 
business strategy. Ethical considerations, such as bias, transparency, and accountability, are key challenges that need to 
be  addressed.  Organizations  must  develop  ethical  frameworks  and  guidelines  to  ensure  responsible  AI  adoption. 
Additionally, the analysis highlights the need for organizations to invest in building AI capabilities, acquiring talent, and 
addressing organizational change management to successfully integrate AI in their business strategies. 
 
Benefits of AI 
Challenges of AI 
Improved decision accuracy and quality 
Ethical considerations and algorithmic biases 
Enhanced  data  analysis  and  insights  generation
   
Resistance to change and organizational adoption 
Increased operational efficiency and productivity 
Lack  of  transparency  and  interpretability  of  AI 
algorithms 
Identification  of  growth  opportunities  and  risk 
mitigation 
Data privacy and security concerns 
Innovation and new business models  
Skills gap and workforce readiness 
Strategic  planning  based  on  real-time  market 
insights 
Integration and interoperability with existing systems 
Automation  of  routine  tasks  and  process 
optimization 
Cost of implementation and maintenance 
Resource  allocation  optimization  for  better 
performance   
Dependence  on  AI  systems  and  potential  technical 
failures 
Personalization of products and services 
Potential job displacement and societal impacts 
Table 1 Benefits and Challenges of Integration of AI on Business Strategy and Decision making 
 
4.Impact of AI on Decision-making process: - The impact of artificial intelligence (AI) on decision-making processes 
has  been  profound,  revolutionizing  the  way  organizations  make  strategic  and  operational  decisions.  This  section 
provides an analysis of the impact of AI on decision-making processes, highlighting key findings and implications. 
 
4.1 Speed and Efficiency: - One of the significant impacts of AI on  decision-making processes is the increased speed 
and efficiency of decision-making. AI algorithms can  process  vast  amounts  of data  and extract  valuable  insights at a 
much  faster  rate  than  humans.  This  enables  organizations  to  make  real-time  decisions,  respond  swiftly  to  market 
changes, and  capitalize  on emerging opportunities.  The  analysis  indicates  that AI-enabled  decision-making  processes 
significantly reduce the time required to gather information, analyze data, and generate actionable insights. 
 
4.2 Data-Driven  Decision Making: -  AI has  transformed decision-making processes by enabling data-driven decision 
making.  With  AI's  ability  to  analyze  large  and  complex  datasets,  organizations  can  make  decisions  based  on 
comprehensive and accurate information. The analysis reveals that AI algorithms can uncover patterns, correlations, and 
trends that humans may overlook, thereby enhancing decision accuracy. By  leveraging AI's data  analysis capabilities, 
organizations can make informed decisions and reduce reliance on intuition or subjective judgments.  
 
4.3  Risk Management  and  Prediction:  -  AI  has  a  profound  impact  on  risk  management and prediction in decision-
making processes. AI  algorithms  can assess risks, analyze  historical  data,  and  identify  potential  risks  or anomalies in 
real-time.  This  enables  organizations  to  proactively  manage  risks,  mitigate  potential  threats,  and  optimize  decision 
outcomes.  The  analysis indicates  that  AI-powered  predictive  models enhance  decision-making  by providing  reliable 
forecasts and scenario analyses, allowing organizations to make proactive decisions and avoid costly mistakes. 

4.4Augmented Decision-Making: - AI serves  as a valuable tool for  augmenting human decision-making capabilities. 
The analysis reveals that AI can provide decision support by presenting relevant  information, suggesting  options,  and 
evaluating  potential  outcomes.  This  augmentation  of  decision-making  processes  allows  humans  to  leverage  AI's 
computational power, cognitive capabilities, and pattern recognition to make more informed and effective decisions. AI 
acts as a collaborator, enhancing human judgment rather than replacing it. 
 
 
 
4.5  Decision  Optimization:  -  AI  enables  decision  optimization  by  evaluating  multiple  variables,  constraints,  and 
objectives  simultaneously.  Through  optimization  algorithms, AI  can  identify  the  best  course  of  action,  considering 
complex  trade-offs  and  constraints.  The  analysis  suggests  that  AI-driven  decision  optimization  helps  organizations 
maximize outcomes, allocate resources efficiently, and achieve strategic objectives. This has significant implications for 
resource allocation, supply chain management, and strategic planning. 
 
4.6  Ethical  Considerations:  -The impact  of  AI  on  decision-making processes also  raises  ethical  considerations. The 
analysis highlights the  importance  of addressing ethical issues such as bias,  fairness,  transparency, and  accountability. 
AI  algorithms  can  inadvertently  perpetuate  biases  present  in  the  data  used  for  training,  leading  to  unfair  or 
discriminatory  decision  outcomes.  Organizations  must  implement  ethical  frameworks  and  guidelines  to  ensure 
responsible and unbiased AI-driven decision-making processes. 
 
5.Implementation  process  of  integration of AI  in Business  Strategy: - Implementing  artificial intelligence  (AI) in 
business  strategy  requires  a  systematic  approach  that  considers  various  factors  to  ensure  successful  integration  and 
maximize its benefits. This section outlines the process of  implementing AI in business strategy, highlighting key steps 
and considerations. 
 
Define  Strategic  Objectives:  -  The  first step  in  implementing AI  in  business  strategy  is to  clearly  define  strategic 
objectives. Organizations need to identify the specific areas where AI can create value and align AI initiatives with their 
overall  strategic  goals.  This  involves  assessing  the  organization's  current  challenges,  competitive  landscape,  and 
opportunities where AI can make a significant impact. 
 
Assess Readiness and Resources: - Before implementing AI, organizations must assess their  readiness  and  available 
resources. This includes evaluating the organization's data infrastructure, technological capabilities, and expertise in AI. 
Organizations need to determine whether they have the  necessary data quality, quantity, and accessibility to support AI 
initiatives. Additionally, assessing  the  availability  of AI  talent and  identifying any skill  gaps  is  crucial for  successful 
implementation. 
 
Identify Use Cases: - Next, organizations should identify specific use cases where AI can be applied to solve business 
challenges or  optimize processes. This  involves  conducting  a  thorough  analysis  of  various  functions  and  departments 
within the organization, such as customer service, supply chain  management,  marketing,  or  product  development.  By 
identifying use cases that align with strategic objectives, organizations can prioritize AI implementation efforts. 

     
Data  Preparation  and Integration:  -  Implementing AI  requires  high-quality and  relevant data.  Organizations  must 
ensure  that  their  data  is accurate,  clean,  and  accessible  for AI  applications. This  involves  data collection,  cleansing, 
integration, and  storage in a centralized repository. Organizations may need  to invest in data managem ent systems and 
infrastructure to facilitate seamless integration with AI algorithms and models. 

 
AI  Model  Development  and  Training: -  Once  the  data  is  prepared,  organizations  can  develop  AI  models  and 
algorithms  tailored  to  their  specific  use  cases.  This  involves  selecting  appropriate  AI  techniques,  such  as  machine 
learning, natural  language processing, or  computer vision, based  on the desired outcomes. Organizations must allocate 
resources for model development, training, and fine-tuning to ensure accurate and reliable results. 
 
Pilot Testing and Evaluation: -  Before full-scale implementation, organizations should conduct pilot tests to evaluate 
the effectiveness and feasibility of the AI solution. This involves deploying the AI models in a  controlled environment 
and collecting  feedback  from users and  stakeholders.  Pilot testing allows  organizations to identify any potential issues 
or limitations and refine the AI solution based on real-world feedback. 
 
Deployment and Integration:  - Once the AI solution has  been successfully piloted, it can be  deployed and integrated 
into the organization's existing systems and processes. This may involve collaboration between IT teams, data scientists, 
and  business  stakeholders to  ensure  a  smooth  integration.  Integration  may  include  creating APIs  for data  exchange, 
integrating AI into existing software platforms, or developing custom applications for AI-enabled decision support. 
 
Continuous  Monitoring  and  Improvement:  -  AI  implementation  is  an  ongoing  process  that  requires  continuous 
monitoring and improvement. Organizations should establish mechanisms to monitor the performance and impact of the 
AI solution.  This  involves  tracking  key performance  indicators (KPIs),  analyzing  user feedback, and  refining  the  AI 
models based on evolving business needs and changing market dynamics. 
 
6. Human AI Collaboration: - The integration of artificial  intelligence (AI) in  business strategy and  decision-making 
processes has transformed the way organizations operate and make critical choices. Central to this transformation is the 
concept of human-AI collaboration, which leverages the unique strengths of both humans and AI systems. This section 
explores  the  role  of  human-AI  collaboration  in  the  context  of  the  impact  of AI  on  business  strategy  and  decision 
making, highlighting its benefits and implications. 
AI and Business 
strategy 
Identify 
Objectives 
and 
resources 
Use case 
Preparation 
and 
Integration 
Prepare AI 
model and 
testing 
Deploment 
Continous 
improveent 

6.1Augmenting  Decision-Making  Capabilities:  -Human-AI collaboration  enhances  decision-making capabilities  by 
combining  human  expertise,  intuition,  and  contextual  understanding  with AI's  data  analysis  and  pattern  recognition 
capabilities. AI systems can process large volumes of  data, identify trends, and generate insights, empowering humans 
to make well-informed decisions. By leveraging AI as a decision support tool, organizations can enhance the accuracy, 
speed, and efficiency of their decision-making processes. 
Data-Driven  Insights:  - AI  systems  excel  at  analyzing  vast  amounts  of  data,  uncovering  hidden  patterns  and 
correlations. Human-AI collaboration enables  decision  makers to access AI-generated insights that may not  be readily 
apparent  to  humans alone.  By  integrating AI-generated  insights  into the  decision-making  process,  organizations can 
make data-driven decisions, mitigating biases and relying on evidence-based analysis. 
 
Enhanced  Strategic  Planning: -  AI's analytical  capabilities  combined  with  human  judgment  and  strategic  thinking 
have  a  profound impact  on  business strategy  formulation.  Human-AI  collaboration  enables  organizations  to  analyze 
market trends, competitor behavior, and customer preferences in real-time, facilitating more accurate strategic planning 
and  adaptation  to dynamic  business  environments.  AI-powered  predictive  models  can  forecast  future  scenarios  and 
assist in identifying growth opportunities, potential risks, and competitive advantages. 
 
Efficient  Resource  Allocation: -Optimizing  resource  allocation  is  a  critical  aspect  of  effective  business  strategy. 
Human-AI  collaboration  aids organizations in  allocating  resources  more  efficiently  by  leveraging AI's computational 
power  and  optimization  algorithms. AI  systems  can  analyze  data,  consider  multiple  variables  and  constraints,  and 
suggest  optimal  resource  allocation  strategies.  This  ensures  that  resources  are  allocated  where  they  will  have  the 
greatest impact, maximizing organizational performance. 
 
Ethical Considerations and Bias Mitigation: -Human-AI collaboration necessitates addressing ethical considerations 
and biases. Organizations must ensure that AI systems are trained on unbiased data and implement measures to identify 
and  rectify  potential  biases.  Collaboration  between  humans  and  AI  can  help  in  detecting  and  mitigating  biases, 
enhancing fairness, and ensuring that AI-driven decisions align with ethical standards. 
 
Continuous  Learning  and Adaptation:  -Human-AI  collaboration  promotes  continuous  learning  and adaptation. As 
humans  interact  with AI  systems,  they  provide  feedback,  validate AI-generated  insights, and  refine  decision-making 
processes. AI  systems,  in  turn,  learn from human feedback, adapt  their  models, and improve over time. This iterative 
learning loop enables organizations to stay agile, adapt to changing business dynamics, and continuously improve their 
decision-making capabilities. 
 
6.2Implications  and  Future  Directions: - The impact  of  human-AI collaboration  on  business  strategy and decision 
making is  far-reaching. By embracing AI as a collaborative  partner, organizations can harness its  analytical power and 
combine it with human expertise to make informed decisions, drive innovation, and gain a competitive edge. However, 
organizations must also  address  challenges  related  to  trust,  transparency, ethics,  and  skills development  to  ensure  the  
effective  integration  of  AI  in  decision-making  processes.  In  the  future,  advancements  in AI  technologies,  such  as 
explainable AI and interactive AI interfaces, will further enhance human-AI collaboration. Exploring the potential of AI 
in facilitating group decision-making, optimizing collaboration among human teams, and supporting strategic  foresight 
will  be  critical  areas of research.  Organizations  must also  continue to  invest in AI  literacy and  training programs to 
enable employees to effectively collaborate with AI systems and unlock the full potential of human-AI collaboration for 
strategic decision making. 
 
7. Conclusion: - The paper examined the impact of artificial intelligence (AI) on business strategy and decision-making 
processes.  The  findings  unequivocally  demonstrate  that  AI  has  brought  about  transformative  changes  in  how 
organizations formulate strategies and make critical decisions. The integration of AI technologies in  various aspects of 
business  operations  has  led  to  significant  improvements  in  decision-making  outcomes,  operational  efficiency,  and 
strategic  planning.  AI-powered  decision-making  processes  have  proven  to  enhance  the  accuracy  and  quality  of 
decisions by leveraging  vast  amounts  of  data,  uncovering  patterns,  and  generating  valuable  insights.  This  data-driven 

approach  enables  decision-makers  to  make  more  informed  choices  aligned  with  organizational  goals,  leading  to 
improved  performance  and  competitive  advantage.  Moreover,  the  integration  of  AI  in  operational  processes  has 
streamlined  workflows,  reduced errors,  and  increased  overall efficiency,  allowing  organizations  to optimize resource 
allocation and achieve  higher levels of  productivity. In terms  of strategic planning, AI has emerged as a  powerful  tool 
for analyzing  market trends, customer behavior, and competitive dynamics. This capability empowers  organizations to 
develop more agile and adaptive business strategies, identify growth opportunities, and proactively manage risks. AI -
powered  predictive  models  contribute  to  proactive  decision-making  and  enable  organizations  to  allocate  resources 
strategically to  drive business growth. The impact  of AI extends beyond operational and  strategic aspects, as  it fosters 
innovation  and  the  emergence  of  new  business  models.  Organizations  can  leverage  AI  technologies  to  personalize 
products  and  services, enhance  customer  experiences,  and identify  new  revenue streams.  The  ability  of  AI-powered 
recommendation  systems,  predictive analytics,  and  natural  language  processing to  deliver  personalized offerings  and 
optimize  customer  interactions  has  significant  implications  for  market  differentiation  and  customer  satisfaction. 
However, the  integration of AI  in business strategy and decision-making processes comes with ethical considerations. 
Addressing  algorithmic  bias,  ensuring  privacy,  transparency,  and  accountability  are  critical  to  maintaining  ethical 
standards and building trust  with  stakeholders.  Organizations must establish robust ethical frameworks and  guidelines 
to ensure  responsible AI adoption and mitigate potential  risks.  Additionally, human-AI collaboration has emerged as a 
vital aspect of AI's impact on business strategy and decision making. By leveraging the strengths of both humans and AI 
systems,  organizations  can achieve  superior  outcomes. The  collaboration  enables  decision-makers to  harness  human 
expertise, intuition, and creativity, along with AI's analytical capabilities. This collaboration leads to enhanced decision 
making, efficient resource allocation, and augmented innovation. 
 
In  conclusion,  the  integration  of  AI  technologies  has  profoundly  impacted  business  strategy  and  decision  making. 
Organizations  that  effectively  leverage  AI  can  gain  a  competitive  edge,  improve  operational  efficiency,  drive 
innovation,  and  achieve  better  strategic  outcomes.  However,  addressing  ethical  considerations,  building  trust,  and 
investing in AI literacy and training are imperative for successful AI adoption. The dynamic interplay between humans 
and AI systems holds great potential for shaping the future of business strategy and decision making, and organizations 
that navigate this landscape thoughtfully and responsibly will be well-positioned for success in the digital age",https://www.researchgate.net/publication/376307798_The_Impact_of_Artificial_Intelligence_on_Business_Strategy_and_Decision-Making_Processes
"Artificial intelligence - driven sustainable development: Examining organizational, technical, and processing approaches to achieving global goals",Wiley online library,"Ignat Kulkov, Julia Kulkova, Rene Rohrbeck, Joick Menvielle, Veltteri Kaartemo, Hannu Makkonen",META ANALYSIS presents a comprehensive literature review using a systematic approach to explore the role of artificial intelligence (AI) in promoting sustainable development in line with the United Nations Sustainable Development Goals (SDGs),2023,"This study presents a comprehensive literature review using a systematic approach to explore the role of artificial intelligence (AI) in promoting sustainable development in line with the United Nations Sustainable Development Goals (SDGs). The systematic review approach was applied to collect and analyze topics, and the literature search was conducted in two stages, encompassing 57 articles that met the research requirements. Our analysis reveals that AI's contribution to sustainability is concentrated within three key areas: organizational, technical, and processing aspects. The organizational aspect focuses on the integration of AI in companies and industries, addressing barriers to implementation and the relationship between companies, partners, and customers. The technical aspect highlights the development of AI algorithms that can address global challenges and contribute to the growth of stability and development in society. The processing aspect emphasizes the internal transformation of companies, their business models, and strategies in response to AI integration. Our proposed conceptual model outlines the essential elements organizations must consider when incorporating AI into their sustainability efforts, such as strategic alignment, infrastructure development, change management, and continuous improvement. By addressing these critical aspects, organizations can harness the potential of AI to drive positive social, environmental, and economic outcomes, ultimately contributing to the achievement of the SDGs. The model serves as a comprehensive framework for organizations seeking to leverage AI for sustainable development, but it should be adapted to individual contexts to ensure its relevance and effectiveness.
Addressing pressing global challenges such as climate change, extreme weather events, hunger, and inequality calls for urgent, concerted action from countries and organizations worldwide. In response, the United Nations Sustainable Development Goals (SDGs) aim to improve health, education, and economic growth while tackling climate change and promoting equality. Achieving these goals necessitates a holistic understanding of the complex, interrelated systems that underpin sustainability challenges. Harnessing the potential of artificial intelligence (AI) technologies offers a promising avenue to facilitate system-level changes and promote sustainable development (Jarrahi, 2018; Jeste et al., 2020).
Despite optimism surrounding AI's potential contribution to sustainability, academic literature on the subject remains scattered across various disciplines, with limited cross-pollination between different fields of inquiry. This article addresses this gap by conducting a systematic literature review of business and management studies investigating AI's role in meeting the SDGs. In doing so, it pursues four objectives:
	Identify and compile empirical business and management studies examining AI's role in achieving the SDGs, consolidating disparate research and facilitating interdisciplinary knowledge exchange.
	Categorize identified studies to reveal predominant research themes and foci, providing a comprehensive overview of the field, particularly in the business-to-business (B2B) context.
	Develop a conceptual model based on organizational, technical, and processing aspects, offering a comprehensive understanding of AI's role in promoting sustainable development in a B2B context.
	Uncover research gaps and propose a future research agenda to stimulate interest and catalyze the development of a new research field.
The article is structured as follows: The methodology employed in conducting the systematic literature review, including search strategies, selection criteria, and data extraction procedures, is detailed. Next, findings are presented, discussing emerging research categories and introducing a conceptual model based on organizational, technical, and processing aspects. This model aims to provide a comprehensive understanding of AI's role in promoting sustainable development in a B2B context. Finally, suggestions for future research are offered, highlighting potential avenues for advancing AI's role in promoting sustainable development and the applicability of the proposed conceptual model.
In this study, we conducted a systematic literature review to explore AI's role in sustainable development within a B2B context, resulting in a final sample of 57 articles. By synthesizing the existing literature, we illuminated AI technologies' potential in addressing complex sustainability challenges and proposed a conceptual model derived from organizational, technical, and processing aspects. This model serves as a comprehensive framework for understanding the interplay between these dimensions and provides a foundation for future interdisciplinary collaboration, innovation, and practice. Our findings reveal that AI's contribution to sustainability is concentrated within these three main areas, and the insights form the basis for guiding organizations in incorporating AI into their sustainability efforts, holding practical relevance for businesses, policymakers, and stakeholders.
2 METHOD
To conduct a literature review, we followed the basic parameters outlined by Schlegelmilch and Öberseder (2010), including defining the scope of the review, selecting data sources, identifying keywords for search, and establishing criteria for including and excluding data from the selection. The systematic review approach was applied to collect and analyze topics that intersect the study areas (Lim et al., 2022; Tranfield et al., 2003). We conducted our literature search in two stages. In the first stage (October 2021), we searched the Web of Science (WoS) database using the query “sustainability OR sustainable” and “artificial intelligence OR machine learning OR deep learning OR AI” in the title, abstract, and keywords sections (Bocconcelli et al., 2018). The total number of WoS articles was 1317. These broad terms were employed to identify the entire set of studies potentially relevant to our research. We did not impose publication time limits, as our goal was to encompass all empirical insights produced over time. Subsequently, we narrowed down our selection by applying the following filters: Articles Only, Use of English, “Business” and “Management” areas (Calabrò et al., 2019).
In addition to searching the WoS database, we conducted a parallel search in the Scopus database using the same search criteria, including keywords, date range, and other filtering options (Caputo & Kargina, 2022; Kumpulainen & Seppänen, 2022). This approach allowed us to ensure a comprehensive search across two major academic databases. The total results found in the Scopus database amounted to 1298 articles. We made a deliberate decision to exclude the articles found only in the Scopus database based on consistency with previous studies. This decision aligns with our research objectives and the scope of our review. However, we acknowledge that this exclusion may limit the comprehensiveness of our study, and readers should consider this aspect when interpreting our findings.
In selecting the WoS database for our systematic review, we considered several key factors that guided our decision. Our initial comparison with the Scopus database revealed that the results were almost identical, but we recognize the need to elaborate on our rationale for focusing solely on WoS.
	Relevance to Research Objectives: The WoS database offered a collection of journals that were highly pertinent to our specific research objectives. We evaluated the unique articles found in the Scopus database and determined that they were less relevant to the focus of our study (most of them were not from B2B area).
	Quality Control and Consistency: WoS is renowned for its rigorous selection criteria for indexing journals, ensuring a high standard of quality. By concentrating on WoS, we aimed to maintain consistent quality across the articles included in our review.
	Alignment with Previous Research: Our study builds upon previous research that utilized the WoS database. By maintaining consistency in our database selection, we aimed to create a coherent and comparable body of work.
	Resource Efficiency: Conducting a systematic review is a resource-intensive process. By focusing on the WoS database, where most relevant articles were found, we were able to allocate our resources more efficiently without sacrificing the comprehensiveness of our review.
	Avoidance of Redundancy: Our initial comparison with the Scopus database revealed a high degree of overlap with the WoS database. Including both would have introduced redundancy without significantly enhancing the breadth or depth of our review.
In the second stage, we manually reviewed the abstracts, research methods, and discussions in each article to verify their relevance to our research. At this stage, we excluded all non-empirical studies as well as articles that did not focus on the B2B context. In order to clarify the process of inter-rater agreement when selecting the final sample of articles, we employed a systematic approach (Gioia, 2021; Papaioannou et al., 2016). The three researchers independently assessed each article for its relevance to the research topic and whether it met the inclusion criteria (B2B, empirical research, AI, sustainable development). Discrepancies in the selection process emerged when researchers disagreed on the inclusion of certain articles. To address these discrepancies, we implemented a consensus-based approach to resolve any disagreements (Potapchuk & Crocker, 2017). When a discrepancy arose between researchers, we convened a discussion session where each researcher presented their rationale for either including or excluding the article in question. The article was included in the final sample if at least two out of the three researchers agreed that it met the inclusion criteria (Cole, 2023). This approach to addressing discrepancies and ensuring inter-rater agreement in the selection process enhanced the transparency and reliability of our study (Armstrong et al., 1997). By fostering open communication and seeking consensus, we were able to maintain consistency in our selection process and produce a robust final sample of articles for analysis.
Our final sample comprised 57 articles included in this review spans a time range from 2008 to 2022. These dates represent the earliest and latest publication years within the selected studies, encompassing a diverse and comprehensive view of the research landscape on the subject. We analyzed the final sample by examining the abstracts, results, and discussions, identifying information about sustainability and the role of AI in addressing applied problems facing companies, industries, regions, countries, etc. Relevant quotations were extracted and retained for further discussion among the study participants. We also noted if the article focused on developing and testing an algorithm or method on a specific case. Key information from the articles was entered into a separate Excel spreadsheet to track progress and quickly identify and remove duplicate information.
Following the formation of our final sample, we applied a thematic analysis to categorize studies into SDG categories based on their focus (Starbird et al., 2016; Tremblay et al., 2020). Recent studies (Carlsen & Bruggemann, 2022; Mensah, 2019; Tremblay et al., 2020) have identified five key SDG topics: People (no poverty, zero hunger, good health and well-being, quality education, gender equality), Planet (clean water and sanitation, responsible consumption and production, climate action, life below water, life on land), Prosperity (affordable and clean energy, decent work and economic growth, industry innovation and infrastructure, reduced inequalities, sustainable cities and communities), Peace (peace, justice, and strong institutions), and Partnership (partnerships for the goals). The study authors independently examined notes and findings to minimize exploratory bias (Creevey et al., 2022). This stage was also conducted in parallel and independently among the authors, with regular consultations and verification of a unified approach to research.
Our study identified nine journals that accounted for fifty percent of the publications in our sample, contributing 28 out of the 57 publications: Business Strategy and the Environment (3 publications), European Journal of Operational Research (2 publications), Information Technology & Management (2 publications), International Journal of Human Resource Management (2 publications), Journal of Business Research (4 publications), Journal of Enterprise Information Management (2 publications), Journal of Intellectual Capital (2 publications), Technological Forecasting and Social Change (9 publications), and Tourism Management (2 publications). All other journals in our sample were represented by a single publication.
The selection of the journals and the analysis of publication trends in this study are not arbitrary but rather strategically aligned with the emerging interest in the intersection of AI and sustainable development. The identified journals, such as Business Strategy and the Environment, Technological Forecasting and Social Change, and Journal of Business Research, are renowned for their focus on innovation, technology, and sustainability, making them highly relevant to our research theme.
Similar to many other research areas, the number of published papers focusing on a specific topic increased gradually over time. However, a significant growth in the number of publications occurred in 2020 and 2021. Our study found one article published in 2008 (1.7%), two articles in 2017 (3.5%), three articles in 2018 (5.2%), eight articles in 2019 (14%), 13 articles in 2020 (22.8%), 29 articles in 2021 (50.8%), and one article dated 2022 (1.7%).
The significant growth in publications in 2020 and 2021 underscores the burgeoning interest in AI's role in sustainability, reflecting the global urgency to address SDGs. By concentrating on these specific journals and trends, we have captured the pulse of academic and industrial advancements in AI-driven sustainability, providing a comprehensive and timely overview of the field. This focus ensures that our analysis is rooted in the most relevant and influential sources, offering valuable insights into the current state of research and future directions.
Qualitative research constitutes approximately 70% of the articles in our sample. The majority of researchers in our sample focus on studying the utilization of AI capabilities for the sustainable development of companies, industries, countries, regions, and other entities, using one or several implemented cases as examples. In contrast, quantitative studies make up about 30% of the sample and tend to concentrate on identifying implicit trends and characteristics of specific industries (Creswell, 2013; Mayring, 2004). The use of case studies enables a more comprehensive understanding of a multifaceted research field, particularly at the intersection of several domains, compared to a superficial examination (Beverland, 2005). Moreover, case studies facilitate a deeper understanding of the subject area, especially within the context of an emerging phenomenon and a scarcity of knowledge (Essamri et al., 2019).
Studies that have employed interviews as a primary information source have used them to investigate human and organizational perspectives. Human-centered research primarily aimed to explore the future of jobs under the influence of AI, the adaptation of different generations to new opportunities and challenges (Grønsund & Aanestad, 2020; Nam et al., 2020), and the role of humans in the deployment of AI (Ogbeibu et al., 2022; Prem, 2019). On the organizational side, interviews provided extensive data on changes at the company level (business model, business processes, strategy), industry level (trends, opportunities, development specifics), and country level (competitive advantages, required resources, and changes). The majority of the research was conducted as qualitative studies using questionnaires, online or offline meetings, and semi-structured interviews.
Cases from the manufacturing sector (automotive, additive manufacturing, and various types of digital and smart production) were represented in 45.6% or 26 articles. The service sector (mainly finance, education, health, and logistics) was represented in 40.3% or 23 articles. A mix of industries was represented in 8.7% or five articles, and studies without a specific industry were represented in 4.6% or four articles.
5 FINDINGS
In this chapter, we present the results of our sample analysis. Based on the previous literature (Carlsen & Bruggemann, 2022; Mensah, 2019; Tremblay et al., 2020), we have categorized the articles into five key SDG topics: People, Planet, Prosperity, Peace, and Partnership (see Table 1).
The People theme is divided into three key parts: 1. The use of AI in global hunger eradication (Flores & Villalobos, 2020; Lachman & López, 2019; Stehel et al., 2019); 2. Changes in employee roles within the new organizational structure of companies (Froehlich, 2017; Grønsund & Aanestad, 2020); 3. Transformation of mature industries under the influence of new technologies (Grønsund & Aanestad, 2020; Vinichenko et al., 2020).
5.1.1 AI and hunger eradication
This subsection covers topics related to SDG2, “zero hunger,” including sustainable industry development, institutional barriers in agriculture, global population growth, and related societal challenges. Articles addressing food production, quantity and quality, and global food safety are also included. Researchers approach hunger eradication from different angles, such as policymaking (Stehel et al., 2019), overcoming institutional and industrial barriers (Lachman & López, 2019), and addressing population growth and food insecurity (Flores & Villalobos, 2020). Kazancoglu et al. (2021) investigate food safety and quality improvements through new technologies.
5.1.2 Employee roles and organizational structure
This subsection addresses gender equality (SDG5) and good health and well-being (SDG3), with a focus on sustainable human development. Froehlich (2017) explores the aging population and the integration of older workers in knowledge-intensive organizations using AI. Nam et al. (2020) investigate perceptions of job insecurity related to technology usage, while Grønsund and Aanestad (2020) examine human-algorithm interaction in the maritime industry. Adnan et al. (2019) study the influence of religiosity on the adoption and effective use of new technologies.
5.1.3 Mature industries and new technologies
Mature industries, such as education and automotive, require significant resources and innovations for sustainable development from a human perspective (SDG3, SDG4, and SDG5). Vinichenko et al. (2020) analyze the education industry's sustainable development, highlighting the role of AI in enhancing competitiveness, collaboration, and university performance. Ding (2021) investigates the intersection of education and hospitality industries, focusing on how machine learning technologies can improve staff training and optimize working hours in the restaurant business.
5.1.4 Summary
The People theme explores AI's role in enhancing well-being and equality. It emphasizes efforts to eradicate poverty and hunger, promote health and education, and achieve gender equality. These findings reflect a comprehensive approach to leveraging AI for human-centric sustainable development.
5.2 Planet
The Planet theme primarily targets two areas: 1. Changes in supply chain management with respect to planetary resources (Dubey et al., 2021; Hill & Böse, 2016); 2. Industry-specific aspects related to consumption (Chiarini, 2021; Hamida et al., 2021). Some papers address both topics concurrently.
5.2.1 Supply chain management and planetary resources
This subsection highlights responsible consumption and production (SDG12) as the key theme. The role of AI in sustainable supply chain management is explored from different perspectives, such as the impact of the COVID-19 pandemic on supply chain transformation (Dubey et al., 2021), AI's influence on resource consumption in the manufacturing industry (Chiarini, 2021), and the growth of export potential in small and medium-sized enterprises through environmental AI strategies (Denicolai et al., 2021). Hill and Böse (2016) investigate AI-assisted resource optimization for reducing air pollution. Goralski and Tan (2020) examine the benefits and limitations of AI in addressing sustainability challenges at the organizational and process levels.
5.2.2 Industry-specific aspects of consumption
This subsection focuses on the application of AI in specific industries for responsible consumption and production. Hamida et al. (2021) demonstrate how the construction industry can reduce emissions and energy use to minimize environmental impact. Li et al. (2022) explore long-term trends in the tourism industry at the organizational and technical levels. Prem (2019) investigates the possibilities of AI for regional development, emphasizing the need for policymaker involvement and the formation of necessary rules for successful technology implementation. Benzidia et al. (2021) examine the application of AI in the supply chain integration process and its impact on environmental performance within French hospitals.
Several studies intersect multiple themes, such as deep learning by individuals, groups, professional communities, and other institutions (Brown & Vergragt, 2008), or the development and testing of green solutions for the needs of humanity, industry, and the planet (Flores & Villalobos, 2020). Ogbeibu et al. (2022) explore the benefits of new technologies, including AI, in addressing global warming challenges and fostering environmental sustainability.
5.2.3 Summary
In the Planet subchapter, AI's impact on ecological sustainability is examined. The findings highlight AI's potential in optimizing resource consumption, reducing pollution, and fostering responsible production. This theme underscores the importance of AI in maintaining a sustainable ecological balance.
5.3 Prosperity
The Prosperity subsection encompasses an extensive range of studies, revealing the fragmented nature of the research. Nevertheless, the primary focus lies in AI innovations within specific industries (e.g., Araujo & Kollat, 2018; Krieger et al., 2021) and the outcomes for businesses and society stemming from the sustainable implementation of these technologies (e.g., Dominique-Ferreira et al., 2021; Sjodin et al., 2018). Organizational factors prove crucial in achieving sustainable development, with various articles at the intersection of organizational-technological, organizational-technological-environmental, and other factors.
5.3.1 Industry-specific AI innovations and applications
The development of AI-driven innovation in particular sectors (SDG8) is a recurring subject. The food, auditing, transportation, and other industries are represented in the majority of the reviewed papers. Businesses in the food industry engage customers in responsible brand development (Araujo & Kollat, 2018). Machine learning methods facilitate dialog, content dissemination, and evaluating the effectiveness of social media strategies.
Krieger et al. (2021) explore AI usage for audit practices, emphasizing the impact of technological, organizational, and environmental contextual factors on the adoption process. AI techniques contribute to a better understanding of client and audit risks. Additionally, AI social robots in hospitality services, AI and virtual reality in the fashion industry, and AI-driven digital twins in infrastructure industries are transforming these sectors (Broo & Schooling, 2023; de Kervenoael et al., 2020; Silva & Bonetti, 2021).
5.3.2 Sustainable changes through business adaptation
The second significant aspect of the Prosperity theme is the research focus on the sustainable changes brought about by AI through business adaptation, such as shifts in employee roles, the inclusion of all population groups in value creation, and regional development. These investigations also concentrate mainly on SDG8 (decent work and economic growth).
Haftor et al. (2021) present a conceptual framework for applying machine learning to generate industrial value, forming a virtuous cycle of learning and value creation. Case studies by Klumpp and Zijm (2019) help classify employee roles in the innovation process and discuss measures at various company levels. Sousa and Wilks (2018) identify essential skills for sustainable business development in the technological future, including problem-solving, critical thinking, creativity, and emotional intelligence, among others.
Several studies explore the integration of older individuals into active societal roles (Hidalgo et al., 2020), the transformation of logistics processes influenced by AI (Allal-Chérif et al., 2021), and the positive effects of AI on the environmental, social, and financial aspects of a company's strategy (Bag et al., 2022).
5.3.3 Summary
The Prosperity theme delves into AI's contribution to economic growth and innovation. It emphasizes AI's role in enhancing industry performance, reducing inequalities, and promoting sustainable communities. These insights underscore AI's potential to drive sustainable economic prosperity.
5.4 Peace
The Peace theme, while not featuring as many articles as other sections, can be tentatively divided into two subgroups: Promoting Institutions and Industry-Specific Contributions to Global Sustainable Development.
5.4.1 Promoting institutions
This subgroup discusses how technological advancements can foster the development of peace, justice, and strong institutions. The focus is on the transfer of technology from more developed regions to developing ones, helping to establish fair and robust institutions. For example, Sly and María (2021) showcases the role Chinese policymakers play in shaping and developing regions based on technological progress, providing opportunities to export new approaches that spread influence and support sustainability.
5.4.2 Industry-specific contributions to global sustainable development
This subgroup highlights industry-specific research that addresses peace, justice, and strong institutions (SDG16). It encompasses a variety of industries, with each study offering unique insights. Canhoto (2021) discusses the importance of detecting and preventing money laundering using new technological capabilities. The author suggests a new approach to training AI for this industry, given the lack of available data. Butticè et al. (2019) examine the country-specific effects of green campaigns, demonstrating that such campaigns are more widespread in countries with limited environmental sustainability orientation. Lee and Kim (2020) argue that companies can consider sustainability as a new growth engine by developing new businesses that differentiate the customer experience, create effective organizational structures, generate business impact, and leverage the latest ICT technologies, including AI.
5.4.3 Summary
Focusing on peace and justice, this theme explores AI's potential in supporting legal systems and governance. The findings reveal the importance of AI-driven solutions in conflict resolution and fostering strong institutions. This section emphasizes the role of AI in building a peaceful and just society.
5.5 Partnership
The Partnership theme emphasizes the importance of sustainable collaboration among various institutional stakeholders for joint development, with new technologies serving as a key factor for success. In our literature review of Partnership for the Goals (SDG17), only one article is featured. Yu (2021) explore the timely subject of government-industry-university relations in the age of AI. They highlight the ways in which AI capabilities influence business, knowledge, and regulatory subsystems as the foundation for sustainable development at the national level. The shift of primary development resources to the digital realm drives a change in established paradigms and alters the rules of engagement within the partnerships among stakeholders.
6 DISCUSSION
Our analysis of the role of AI in sustainability reveals a diverse approach to the development of SDGs, with most research concentrated in the Prosperity section, focusing on organizational and industrial changes due to AI implementation. We propose to group our studies according to the degree of sustainable AI impact on: (1) organizational or company level; (2) industrial or regional level; (3) technical articles aimed at developing and testing algorithms on specific examples.
6.1 Organizational aspect
This section highlights researchers' contributions to the prosperity pillar, with most articles related to SDG9 (Industry, Innovation, and Infrastructure). These studies discuss industrial changes resulting from AI implementation in companies (Braganza et al., 2021; Jain et al., 2022; Klumpp & Zijm, 2019), industrial barriers hindering effective technology application (Kazancoglu et al., 2021;Lachman & López, 2019; Stehel et al., 2019), and relationships between companies, partners, and customers (Silva & Bonetti, 2021; Vinichenko et al., 2020). Authors note the competitive advantage of applying AI-based programs in practice, often related to environmental, financial, and social factors (Bag et al., 2022).
An essential aspect in this subsection is a company's transition to a more stable position and how this process forms additional competitive advantages compared to other industry players. Companies consider UN SDGs as targets for efficiency, cost reduction, quality improvement, and more (Kazancoglu et al., 2021; Sjodin et al., 2018). Manufacturing is the most prominent industry in this context (Chiarini, 2021; Dubey et al., 2021; Pan et al., 2022), followed by agriculture (Lachman & López, 2019; Stehel et al., 2019) and food production (Flores & Villalobos, 2020; Kazancoglu et al., 2021), focusing on innovation, testing new solutions, and addressing hunger (SDG2) or health and well-being (SDG 3) goals. These articles may also relate to the people pillar. Researchers target policymakers as one of the AI application beneficiaries (Li et al., 2022; Stehel et al., 2019), emphasizing more accurate supply and demand forecasting methods for a stable industry and society position.
AI implementation in a company requires infrastructure preparation and a change in business approach. Employees should focus more on problem-solving, critical thinking, creativity, and people management (Jain et al., 2022; Sousa & Wilks, 2018). Consequently, changes in the company's business model entail significant shifts in personnel training. The staff becomes crucial in integrating solutions at planning, implementation, and operation stages (Ding, 2021; Hill & Böse, 2016). Productive employment and decent work (SDG8) are also often discussed in this section, with AI-based innovations allowing for rethinking employee roles in the company, changing work process perceptions (Jain et al., 2022).
Regional development is considered in combination with new growth and development mechanisms (Sly & María, 2021, Prem, 2019). AI algorithms enable drawing non-obvious conclusions from complex tasks for industry development, such as defining participant relationships, identifying key development points, and long-term planning amid uncertainty (González-Cancelas et al., 2019).
Population growth studies under climate change are relevant for many researchers and practitioners (Flores & Villalobos, 2020; Hamida et al., 2021; Polemis & Spais, 2020). Forming a more optimistic vision for the industry and predicting results can help attract investors and other participants. Simultaneously, the industry's sustainable development concerning reducing emissions and resource use is supported by AI-based application implementation.
We anticipated a more significant response to combating the COVID-19 pandemic; however, such works are relatively rare. These papers generally address issues common to all sectors, such as supply chain management (Dubey et al., 2021).
Researchers also compare the application of various new technologies to achieve sustainability. In the literature we studied, in addition to AI, authors refer to technologies like virtual and augmented realities, blockchain, and 5G. Consequently, the proposed conclusions are often general regarding new IT solutions' advantages. However, when ranking capabilities and potential, AI often holds an edge (Brown & Vergragt, 2008; Silva & Bonetti, 2021).
In the case of the planet pillar, authors identify key areas where AI could provide advantages, such as reducing emissions, conserving natural resources, and decreasing energy costs (Benzidia et al., 2021; Denicolai et al., 2021; Goralski & Tan, 2020). However, some papers in this section express doubts about using AI to achieve sustainability, indicating an existing discrepancy in understanding the results and requiring additional attention from researchers and practitioners.
Having explored the organizational considerations for integrating AI into sustainable development, we now turn our attention to the technical aspects. This shift is essential as the technical foundation plays a crucial role in enabling the organizational strategies and must be carefully aligned with the broader sustainability objectives.
6.2 Technical aspect
The planet pillar in this section explores algorithms that address global challenges like healthcare, knowledge, and humanity (Benzidia et al., 2021; Brown & Vergragt, 2008). Forecasting and targeting humanity's global problems have been accomplished with the help of AI-based programs (Canhoto, 2021; Flores & Villalobos, 2020). Addressing such challenges contributes to society's stability and growth (Hidalgo et al., 2020; Penagos-Londoño et al., 2021).
6.3 Processing aspect
The processing section presents works focused on a company's internal transformation, including its business model, strategy, resource reallocation, and changes in business processes. The proposed conclusions in such articles highlight improvements at the company level in relation to AI implementation or usage. Most of the papers in this subchapter also belong to the organizational section and have been discussed previously. The findings in these papers offer recommendations at the company and industry level. However, unlike those in the organizational chapter, the remaining research does not provide conclusions concerning changes at the industry or regional level.
Discussions about the role of AI software continue at the company level. Despite the complexity and uncertainty, authors tend to lean towards an optimistic outlook for such integrations (Ogbeibu et al., 2022). However, the workforce will require new skills; significant changes in functionality are possible depending on the industry and specialization within the company (Adnan et al., 2019; Grønsund & Aanestad, 2020). Moreover, authors suggest not expecting rapid changes within the company. New technologies involve high costs and changes that unprepared organizations may not overcome (Adnan et al., 2019).
Additionally, integration efficiency and sustainable development can be influenced by human factors that are difficult to account for in advance. In our review, one such factor is religiosity, which cannot be unequivocally attributed to either the positive or negative side. Therefore, researchers should pay extra attention to non-obvious human factors that can potentially impact a company and industry's sustainable development when working with AI.
Supply chain transformations continue at the processing level (Allal-Chérif et al., 2021; Bag et al., 2022). Such research is more concentrated around purchasing departments and/or logistics, with a focus on determining optimal procurement proposals and delivery optimization. On one hand, such functionality is easier to automate, freeing up more working time for staff. On the other hand, researchers emphasize the importance of working with talent and developing leadership competencies in new or renewed employees.
Researchers explore compelling cases of using AI to change established companies. However, they note that drawing unambiguous conclusions about changes based on isolated successful IT solution integration cases is not possible. More research and comparative analysis of successful cases in similar circumstances are needed.
7 CONCEPTUAL MODEL FOR INTEGRATING AI INTO SUSTAINABLE DEVELOPMENT
The increasing role of AI in driving sustainable development has generated significant interest among researchers, practitioners, and policymakers. Based on our literature review, we propose a conceptual model that highlights the key organizational, technical, and processing aspects for integrating AI into sustainable development efforts. This model aims to provide a comprehensive framework for organizations and industries to navigate the challenges and opportunities presented by AI and contribute positively to the achievement of SDGs, see Figure 2. This figure provides a clear visual guide to the relationships and flow between different aspects of integrating AI into sustainable development.

FIGURE 2
Open in figure viewer
PowerPoint
Conceptual model diagram.
7.1 Organizational aspects of the conceptual model
7.1.1 Strategic alignment and commitment
To successfully integrate AI into sustainable development initiatives, organizations must align their strategies, goals, and objectives with the principles of sustainability and the broader SDGs. This requires strong commitment and support from top management, as well as effective communication and collaboration among various organizational stakeholders. Organizations should also develop clear performance metrics and targets related to sustainability and AI, monitor progress regularly, and adjust their strategies as needed to ensure continuous improvement.
7.1.2 Human capital development and talent management
The effective implementation of AI technologies for sustainable development depends heavily on the availability and quality of human capital. Organizations must invest in the development of their workforce to ensure they possess the necessary skills, knowledge, and expertise to work with AI systems effectively. This includes providing training and development opportunities in areas such as data science, machine learning, and ethical AI, as well as fostering a culture of innovation, adaptability, and continuous learning. Furthermore, organizations should adopt talent management strategies that attract, retain, and develop employees with the right combination of technical and non-technical skills to support their AI-driven sustainability initiatives.
7.1.3 Stakeholder engagement and partnership building
Collaboration and partnerships play a critical role in driving sustainable development through AI. Organizations should actively engage with various stakeholders, including customers, suppliers, regulators, academia, and non-governmental organizations, to identify opportunities for collaboration, share knowledge and best practices, and co-create innovative solutions. Building strong partnerships across sectors and industries can help organizations access valuable resources, expertise, and networks, as well as accelerate the development and adoption of AI technologies for sustainable development.
7.2 Technical aspects of the conceptual model
7.2.1 AI technology selection and implementation
AI technology selection and implementation is a nuanced process that requires careful consideration of both the potential benefits and associated risks. For example, machine learning algorithms can significantly enhance predictive accuracy in areas such as energy consumption forecasting, leading to more efficient resource utilization. However, these algorithms may also inadvertently introduce biases or errors if not properly validated, posing ethical and operational challenges.
Recent developments in the field of AI ethics have led to the creation of frameworks and guidelines that emphasize transparency, fairness, and accountability. Organizations must be mindful of these principles when implementing AI technologies to ensure responsible innovation. Practical examples of potential social impacts include the creation of new job opportunities in AI-driven industries but also the risk of job displacement in traditional sectors. Environmental impacts may encompass the use of AI in optimizing energy consumption, reducing waste, and supporting sustainable agriculture practices. Economic impacts could involve leveraging AI to drive innovation, increase productivity, and contribute to equitable economic growth, while also considering potential market disruptions and inequalities.
7.2.2 Data management and analytics
Data is a critical enabler of AI-driven sustainability initiatives. Organizations must establish robust data management practices to ensure the quality, accuracy, and availability of data for AI systems. This includes developing data governance policies and procedures, as well as implementing data storage, processing, and analytics infrastructure that supports the needs of AI applications. Moreover, organizations should prioritize the responsible and ethical use of data, addressing issues such as data privacy, security, and bias.
7.2.3 AI ethics and responsible innovation
The development of AI technologies for sustainable development must be guided by ethical considerations and principles of responsible innovation. Organizations should develop and adhere to AI ethics guidelines and frameworks that address issues such as transparency, fairness, accountability, and human rights. Furthermore, organizations should consider the potential social, environmental, and economic impacts of their AI-driven sustainability initiatives, and strive to minimize any negative consequences while maximizing positive outcomes.
7.3 Processing aspects of the conceptual model
7.3.1 Business process transformation and integration
AI-driven sustainability initiatives often require significant changes to existing business processes and operations. Organizations should conduct a thorough assessment of their current processes to identify areas where AI can be effectively integrated to enhance efficiency, reduce costs, and improve sustainability outcomes. This may involve redesigning workflows, automating repetitive tasks, and streamlining decision-making processes to leverage the full potential of AI technologies. Organizations must also ensure that AI systems are seamlessly integrated with existing systems and infrastructure to facilitate smooth and efficient operations.
7.3.2 Change management and organizational adaptation
The successful implementation of AI technologies for sustainable development requires organizations to adapt to new ways of working and thinking. Change management plays a critical role in facilitating this transition, helping organizations manage the complexities and uncertainties associated with AI-driven transformation. Organizations should develop a comprehensive change management strategy that addresses key aspects such as communication, employee engagement, and cultural adaptation. This strategy should also include measures to mitigate potential resistance to change and ensure that employees feel supported and empowered throughout the transformation process.
In the context of AI integration for sustainable development, the application of specific change management models or strategies can provide a structured approach to guide organizations through the transformation process. For example, Kotter's 8-Step Change Model (Kotter, 2012) can be particularly relevant:
	Creating a Sense of Urgency: Highlighting the importance of AI in achieving sustainability goals and creating a compelling vision for change.
	Building a Guiding Coalition: Assembling a cross-functional team with the influence and expertise to lead the AI integration effort.
	Forming a Strategic Vision and Initiatives: Developing a clear vision and strategy for how AI will contribute to sustainable development within the organization.
	Enlisting a Volunteer Army: Engaging employees at all levels to support and participate in the AI-driven transformation.
	Enabling Action by Removing Barriers: Identifying and addressing organizational barriers that may hinder the successful implementation of AI technologies.
	Generating Short-Term Wins: Creating early successes in AI integration to build momentum and demonstrate tangible progress.
	Sustaining Acceleration: Continuously driving the change effort, adapting to challenges, and seizing new opportunities for AI-driven sustainability.
	Instituting Change: Embedding AI-driven sustainability practices into the organizational culture, ensuring long-term commitment and success.
By applying such a model, organizations can systematically navigate the complexities of AI integration, aligning technological advancements with sustainability objectives, engaging stakeholders, and fostering a culture of continuous innovation and adaptation.
7.3.3 Continuous improvement and innovation
Sustainable development is an ongoing journey, and organizations must continually innovate and evolve to meet the changing needs of their stakeholders and the environment. AI-driven sustainability initiatives should be designed with flexibility and adaptability in mind, allowing organizations to respond effectively to emerging challenges and opportunities. This requires a commitment to continuous improvement and a culture of experimentation, learning, and innovation. Organizations should regularly evaluate the performance and impact of their AI-driven sustainability initiatives, incorporating feedback and lessons learned to refine their strategies and enhance their outcomes over time.
In the pursuit of continuous improvement and innovation for AI-driven sustainability initiatives, organizations can benefit from adopting specific methodologies that promote adaptability, efficiency, and iterative progress. Two such methodologies are Agile and Lean:
	Agile Methodology: Agile emphasizes flexibility, collaboration, and iterative development. In the context of AI-driven sustainability, Agile can enable organizations to rapidly adapt to changing requirements, foster cross-functional collaboration, and deliver incremental improvements. Regular review and adaptation cycles ensure that AI solutions are aligned with sustainability goals and can be quickly adjusted to respond to new challenges or opportunities.
	Lean Methodology: Lean focuses on maximizing value while minimizing waste. By applying Lean principles to AI-driven sustainability initiatives, organizations can streamline processes, eliminate inefficiencies, and focus on activities that directly contribute to sustainable development outcomes. Techniques such as Value Stream Mapping can be used to identify and eliminate bottlenecks, ensuring that AI technologies are implemented in the most effective and resource-efficient manner.
Both Agile and Lean methodologies promote a culture of continuous learning, experimentation, and improvement. By integrating these methodologies into AI-driven sustainability initiatives, organizations can create a dynamic and responsive approach that continually evolves to meet the changing needs of stakeholders and the environment, thereby enhancing the effectiveness and impact of their efforts.
In conclusion, our conceptual model highlights the key organizational, technical, and processing aspects that organizations should consider when integrating AI into their sustainable development efforts. By addressing these critical aspects, organizations can harness the power of AI technologies to drive positive social, environmental, and economic outcomes and contribute to the achievement of the UN SDGs. While the model provides a comprehensive framework for organizations, it is essential to recognize that each organization's context and needs will be unique. Therefore, the model should be adapted and customized to reflect the specific challenges and opportunities faced by individual organizations and industries.
8 FUTURE RESEARCH AREAS
8.1 Human factors and AI adoption
The findings chapter highlighted various organizational, technical, and processing aspects of AI implementation for sustainable development. However, non-obvious human factors, such as religiosity, culture, and cognitive biases, have been underexplored in the literature. These factors can significantly influence the successful adoption and integration of AI in organizations and industries, as they shape employee attitudes, decision-making processes, and overall organizational culture.
Future research in this area could examine how these human factors interact with AI adoption and their implications for sustainability. For instance, researchers could explore how cultural differences influence AI implementation strategies and outcomes across different countries and industries. This could involve investigating the role of national culture in shaping organizational receptiveness to AI-driven sustainability initiatives, as well as the ways in which cultural dimensions, such as power distance or uncertainty avoidance, affect AI adoption processes and outcomes.
Additionally, research could delve into the impact of cognitive biases on AI integration and decision-making related to sustainability. This might involve studying how biases, such as confirmation bias or the sunk cost fallacy, influence decision-makers' perceptions of AI's potential for sustainable development and their subsequent actions.
Understanding these human factors and their impact on AI adoption for sustainability will provide valuable insights for organizations, policymakers, and practitioners, helping them develop more effective strategies for overcoming potential barriers and fostering successful AI integration.
8.2 AI in combating global challenges
While the findings chapter discussed various applications of AI in promoting sustainable development, the potential of AI in addressing global challenges like climate change, pandemics, and inequality remains underexplored. Given the urgency of these issues and their interdependence with the SDGs, future research should focus on developing new algorithms, tools, or strategies that leverage AI capabilities to tackle these challenges across different industries and sectors.
For example, research could investigate how AI can be used to enhance climate change modeling and prediction, support the development of clean energy technologies, or optimize resource allocation for disaster response and mitigation. Similarly, studies could explore the application of AI in public health to improve disease surveillance, vaccine development, and health system management during pandemics, or analyze how AI-driven solutions can help reduce inequality by promoting inclusive economic growth and access to essential services.
In our review of AI in combating global challenges, we recognize that our findings are shaped by the specific search strategy and criteria we employed. While we have identified certain trends and gaps within the scope of our review, we acknowledge that there are indeed studies that have adopted AI in addressing critical global issues such as climate change and public health. For example, Morgan et al. (2023) have used an ensemble machine learning model to delineate groundwater potential zones in desert fringes, contributing to our understanding of climate change impacts in specific regions. Similarly, Nguyen et al. (2022) have adopted AI in vaccine development, specifically in designing a multi-epitope candidate vaccine to control African swine fever spread. Given these examples, we emphasize that our suggestions for future research are based on the specific scope and limitations of our review. We encourage researchers to consider the broader landscape of AI applications in global challenges and to build upon both the findings of our review and the existing body of work in these critical areas.
By examining these topics, future research can contribute to a better understanding of AI's potential in addressing global challenges, fostering innovative solutions, and promoting sustainable development across various domains.
8.3 Case analyses and comparative studies
The findings chapter emphasized the need for more case analyses and comparative studies to better understand the impact of AI on sustainable development in different industries, countries, and contexts. Conducting such research will provide valuable insights into best practices, common challenges, and the factors that contribute to successful AI implementation.
Future research could involve in-depth case studies of organizations that have successfully integrated AI technologies for sustainable development, examining the strategies employed, the challenges encountered, and the outcomes achieved. These case studies could span a range of industries and countries, allowing for cross-case comparisons and the identification of common patterns and themes.
Moreover, researchers could conduct comparative studies that examine AI implementation across multiple organizations or industries, focusing on factors such as organizational structure, culture, and industry dynamics. Such research could reveal insights into the conditions that facilitate or hinder AI adoption for sustainability, as well as the potential benefits and risks associated with various implementation strategies.
By conducting case analyses and comparative studies, future research can help organizations, policymakers, and practitioners develop more effective approaches to AI-driven sustainable development, tailored to specific industries, contexts, and challenges.
8.4 AI's role in transforming mature companies
The findings chapter highlighted the potential of AI in transforming mature companies but also noted the need for further research to better understand the challenges and opportunities associated with implementing AI technologies in such organizations. Mature companies often face unique barriers, such as entrenched organizational structures, legacy systems, and resistant corporate cultures, which can hinder the adoption and integration of AI-driven sustainable initiatives.
Future research in this area could focus on examining the specific challenges mature companies face when implementing AI for sustainability, as well as the strategies and interventions that can help overcome these barriers. This might involve studying how mature companies can effectively navigate the process of digital transformation, restructure their operations, and develop new business models that leverage AI capabilities for sustainable growth.
Additionally, research could explore the role of leadership and change management in driving AI adoption within mature companies, including the development of training programs, communication strategies, and incentives that foster a culture of innovation and agility. Researchers could also investigate the factors that contribute to the successful integration of AI technologies in mature organizations, such as the alignment of AI-driven initiatives with corporate sustainability goals, the availability of necessary resources, and the presence of supportive internal and external stakeholders.
By delving into these topics, future research can provide valuable insights and guidance for mature companies seeking to harness AI for sustainable development, as well as contribute to a deeper understanding of the factors that enable successful AI implementation in various organizational contexts.
8.5 Balancing AI's sustainability benefits and risks
In our exploration of AI's role in sustainable development, we have focused on the opportunities and applications that contribute to the achievement of the SDGs. While we recognize that there may be potential risks and unintended consequences associated with AI technologies, our review did not specifically delve into these aspects. Future research may wish to explore these dimensions in greater detail to provide a more comprehensive understanding of the balance between AI's sustainability benefits and potential risks.
For example, research could investigate the development of AI solutions that minimize energy and resource consumption, as well as the design of policy frameworks and industry standards that promote the responsible and sustainable use of AI technologies. Studies could also delve into the ethical dimensions of AI implementation, examining issues such as fairness, accountability, and transparency in AI-driven decision-making, as well as the implications of AI for privacy, data security, and human rights.
Furthermore, researchers could explore strategies for addressing the potential labor market impacts of AI adoption, such as reskilling and upskilling programs, social safety nets, and policies that promote inclusive and equitable access to AI-driven opportunities. By focusing on these topics, future research can help ensure that AI's contributions to sustainable development are maximized while its potential risks are carefully managed and mitigated.
9 CONCLUSION
The integration of AI into sustainable development initiatives represents a transformative opportunity for organizations, industries, and policymakers. This study has provided a comprehensive exploration of the organizational, technical, and processing aspects that underpin the successful incorporation of AI into sustainability efforts. Through a systematic literature review, we have identified key themes and insights that form the basis of a conceptual model, offering a holistic framework for navigating the complex landscape of AI-driven sustainable development.
Organizational Aspects: Strategic alignment, human capital development, and stakeholder engagement emerged as vital components for aligning AI initiatives with sustainability goals. The importance of top management commitment, workforce training, and cross-sector collaboration was underscored as essential for success.
Technical Aspects: The selection and implementation of AI technologies, data management, and ethical considerations were highlighted as critical factors. The study emphasized the need for careful evaluation of AI techniques, robust data governance, and adherence to ethical principles to ensure responsible innovation.
Processing Aspects: Business process transformation, change management, and continuous improvement were identified as key elements in leveraging AI for sustainability. The application of specific methodologies, such as Agile or Lean, and the integration of change management models like Kotter's 8-Step Change Model, were suggested to enhance efficiency and adaptability.
Our research also pointed to areas for future exploration, including the investigation of non-obvious human factors, the role of AI in combating global challenges, and the need for more case analyses and comparative studies. The potential risks and unintended consequences associated with AI technologies were acknowledged, with recommendations for future research to strike a balance between AI's sustainability benefits and potential risks.
The limitations of this study, such as the focus on specific databases and the exclusion of non-technical articles detailing AI techniques, should be considered when interpreting the findings. However, the insights and framework provided in this research contribute valuable knowledge to the field and offer practical guidance for organizations seeking to harness AI for sustainable development.
In conclusion, this study underscores the transformative potential of AI in driving sustainable development and provides a roadmap for organizations, practitioners, and researchers to navigate the challenges and opportunities in this rapidly evolving field. By addressing the critical aspects outlined in our conceptual model, we can harness the power of AI to drive positive social, environmental, and economic outcomes, contributing to the achievement of SDGs. The journey towards AI-driven sustainability is complex and ongoing, and this research serves as a foundational step towards a more sustainable and innovative future.",https://onlinelibrary.wiley.com/doi/full/10.1002/sd.2773
Artificial Intelligence and its Impact in International Relations,Journal of Politics and International Affairs,"Meleouni, C., & Efthymiou, I.-P.",ARTICLE AI is reshaping international relations through automation and predictive analytics.,2023,"n an era  marked by  remarkable technological advancements, Artificial Intelligence (AI) has emerged as  a 
powerful  force  in  the  realm  of  International Relations.  AI's  adaptability,  which  includes  automating the 
processes  of  foreign  policy
-
making  and  improvi
ng  predictive  analytics, is  swiftly  reshaping  the ways  in 
which countries participate in international affairs. The impact of  AI  is  notably observed  in activities like 
data  analysis,  policy  assessment,  conflict  resolution,  cybersecurity,  language  translati
on,  and  disaster 
response, all  of  which streamline diplomatic activities and enhance communication. This article  discusses 
how AI technology will shape the field of International Relations.
Keywords:
Artificial  Intelligence,  Politics,  International  Security,  International  Relations,  Diplomacy, 
Democracy, Conflict Resolution, Global Governance, Policy Makers



Journal of Politics and Ethics in New Technologies and AI 
Volume 2, Issue 1 (2023)                                                                                                     
2
to execute activities typically associated with human intellect. These tasks encompass activities such 
as  pattern  recognition,  experiential learning,  drawing  inferences,  making  forecasts,  and  initiating 
actions. This can occur either in digital form or as 
the intelligent software that operates autonomous 
physical systems (Department of Defense, 2019).
The  field of Machine Learning and AI  is already seen to have a  profound impact on  how  countries 
manage international relations. The evolution of computers, the availability of countless data and new 
algorithms have enabled the rapid development of AI (Euro
pean Parliament, 2020). AI  is designed to 
improve human life. Thus, many nations such as the USA  and China are racing to achieve a global 
innovation advantage in AI as they believe that it will be able to protect national security among other 
fields  (Castr
o,  McLaughlin,  &  Chivot,  2019).  Nevertheless,  as  far  as  International  Relations  is 
concerned, it will have implications for military, economic and information superiority (Allen & Chan, 
2017). International Relations and International Security seem to be a
ffected by AI  innovations. This 
paper aims to present the way AI technology will shape the field of International Relations. 
Modern Artificial  Intelligence
It's  important to  note that modern AI  is a  dynamic and  evolving field, and  it continues to advance 
rapidly. Researchers and practitioners are constantly working on  improving AI  models, algorithms, 
and  applications, making  AI  an  integral part  of  our  contemp
orary technology  landscape.  Modern 
Artificial Intelligence (AI) refers to the current state of AI  technology and research, characterized by 
significant  advancements  and  innovations  in  machine  learning,  deep  learning,  natural  language 
processing, computer v
ision, and related fields. It encompasses the latest approaches, techniques, and 
applications that have emerged in the field of AI.
AI  is a field of Computer Science where machines can appear to be intelligent by running programs. 
It is a very broad topic and covers our daily lives and is found from smart phones and social media to 
smart cars.  Also,  Al  machines  are  able  to  'understand'
their  environment, solve  problems  and  act 
towards  a  specific goal.  AI  systems are  capable  of  adapting their  behavior  to  a  certain extent  by 
analyzing  the  consequences  of  previous  actions  and  solving  problems  autonomously  (European 
Parliament, 2020). 
Modern AI  relies on deep learning, leveraging neural networks for tasks like image recognition and 
NLP.  It  harnesses  big  data  for  model  training  and  employs  sophisticated techniques like  transfer 
learning. Reinforcement learning enables AI to learn from in
teractions, while computer vision allows 


-
time processing on  devices, reducing reliance on the 
cloud.
Throughout the years, there has been a rapid rise in the use of  AI.  Nowadays,  AI  is found in every 
aspect of  life such as self
-
driving cars or in media, Healthcare,
Finance, Education,
Manufacturing,
Energy,  Agriculture, Environmental Conservation, Marketing  and  Advertising,  Human  Resources, 
even Space Exploration.
Artificial Intelligence  in International Relations
Artificial Intelligence (AI) has emerged as a pivotal technological force in the 21st century, poised to 
profoundly impact international relations (Amaresh,  2020).  Notably,  nations  such  as  the  USA  and 
China are fervently racing to secure a global innovatio
n advantage in AI, driven by their belief in the 
technology's  capacity  to  bolster  national  security,  among  other  domains  (Castro,  McLaughlin,  & 
Chivot, 2019). This fervor has triggered a race often likened to an arms race, although it's essential to 
recogn
ize the limited scope of such AI development within the Department of Defense (DOD).
A comprehensive framework for AI's potential roles in policy formulation and international relations 
has been suggested by a Chatham House report. It classifies these roles into three categories: analytical, 
predictive, and operational. In the analytical r
ole, AI stands to automate substantial portions of foreign 
policymaking.  Predictive  AI   influences  long
-
term  perspectives,  empowering  policymakers  to 
comprehend the ramifications of their actions. Operational AI, a more distant prospect, involves fully 
auto
nomous systems and weaponry (Cummings, Roff, Cukier, Parakilas, & Bryce, 2018). 
Furthermore, a 2020 report from the US Congress underscores the burgeoning impact of AI on national 
security.  AI   research  spans  diverse  realms,  encompassing  information  collection  and  analysis, 
logistics, cyber operations, information operations, administr
ation, and control. It finds application in 
a  spectrum  of  semi
-
autonomous and  autonomous  vehicles.  The  United  States,  while  feeling  the 
competitive  pressure  in  the  AI   market,  is  compelled  to  innovate  in  military  AI  applications. 
Concurrently, China strives
to establish itself as an AI pioneer by 2030, while Russia concentrates its 
efforts on robotics (Congressional Research Service, 2020). It is acknowledged in the report that AI 
presents both  advantages and  disadvantages, facilitating autonomous operations
, informed military 
decision
-
making, and  expedited  military actions,  yet  it  remains  vulnerable to  unpredictability and 
unique forms of manipulation (Congressional Research Service, 2020). The strategic and economic 
importance of AI  has led many countries t
o adopt national AI strategies. It’s worth mentioning that 
Canada, in 2017, was the first country to adopt a national ΑΙ strategy (Meltzer & Kerry, 2021). 

However,  U.S.  AI   policy  also  recognizes  the  importance  of  international  cooperation  on  AI 
development. The  State  Department recognizes that partnerships with  U.S.  allies and  partners can 
advance democracy and human rights by  collaborating to recognize and 
capitalize on opportunities, 
all the while addressing challenges. This involves fostering mutual norms and agreements regarding 
the  ethical  and  responsible  application of  AI.  The  main  goals  of  engagement  include  supporting 
trustworthy AI innovation and pro
moting trust in and adoption of AI technologies for economic growth 
and global security (U.S. Department of State, n.d.). Something vital in the context of greater economic 
and political volatility, a phenomenon which threatens to undermine an otherwise co
operative world 
order.
Another global player is China, which also has a comprehensive and ambitious plan of AI policies. In 
2017 the Chinese State Council published the “Next Generation AI Development Plan” includes a plan 
to become the global leader in AI  by 2030 (State Council
, 2017). In addition, China’s AI strategy is 
included in the “Belt and Road Initiative” as the “Digital Silk Road” which combines the efforts of the 
Chinese government and the involvement of Chinese tech companies. The DSR includes a wide array 
of  areas,  r
anging from telecommunications networks to ‘Smart City’ projects, to e
-
commerce, to 
Chinese  satellite navigation  systems  (Vladisavljev,  2021).  Also,  China’s  AI  program 
includes
cooperation mainly with AI universities and research centers all over the world (State Council, 2017). 
In considering China’s role in such initiatives, it is important to understand the extent to which material 
gains depend on such development, this is be
cause of the historical precedent of stopped commitments 
in the  face of worsening economic conditions, such  a precedent could be  seen in  the belt and road 
projects pause in Pakistan post American sanctions.
However,  Artificial Intelligence (AI)  significantly impacts diplomacy and  international relations in 
various ways, bringing notable consequences and implications within the field. Below, we  highlight 
some of the notable consequences and implications of AI 
within diplomacy:
Data Analysis and Prediction
AI  has the capacity to scrutinize extensive datasets from diverse sources, such as social media, news 
outlets, and government reports. This capability aids diplomats and policymakers in gaining insights 
into global trends, public sentiment, and potential c
onflicts. Predictive analytics can be instrumental 
in  the early  identification of  emerging issues and  crises (Al  Fahim, 2022;  Deloitte AI  Institute for 
Government & Google Cloud, 2022). Also, Governments and think tanks use AI and machine learning 
to  analy
ze  vast  datasets to  make  predictions about  international relations and  conflicts, informing 
diplomatic strategies and decisions (Perricos & Kapur, 2019).

Meleouni & Efthymiou (2023)
https://doi.org/10.12681/jpentai.35803
5
Policy Analysis 
AI can evaluate and model the potential consequences of policy decisions, enabling diplomats to make 
well
-
informed choices regarding international agreements and treaties (Moore, 2023).
Conflict Resolution
AI
-
driven  tools  can  contribute  to  conflict  resolution  and  negotiation  processes  (
Efthymiou
& 
Koukouliou,
2022)
.   Natural   language   processing   (NLP)   algorithms  can   examine   diplomatic 
communications and offer potential compromises or solutions. For example, UNICEF uses AI
-
enabled 
chatbots to engage with youth affected by conflict and crises. These chatbots collect dat
a, understand 
the needs of affected populations and provide timely responses to mediate conflict and provide support 
(UNICEF  EAP Blog, 2023).
Security and Cybersecurity 
In  the realm of diplomacy, AI  plays a  pivotal role in cybersecurity efforts. It  can swiftly detect and 
respond to cyber threats, safeguarding sensitive diplomatic information and communications. Also, AI 
is used in detecting and attributing cyberattacks, w
ith diplomatic consequences. For instance, AI was 
instrumental  in  attributing  the  2017  WannaCry  ransomware  attack  to  North  Korea,  leading  to 
diplomatic actions (UNICRI & UNCCT, 2021).
Language Translation
AI
-
driven  language  translation  tools  have  the  power  to  overcome  language  barriers,  making 
communication more  accessible for diplomats as  they interact with  their counterparts from diverse 
nations. This,  in  turn,  promotes enhanced  diplomatic discourse and  n
egotiation. Leading  AI
-
based 
translation tools,  such  as  Google  Translate and  DeepL,  have  played  a  pivotal role  in  streamlining 
diplomatic  communication  across  linguistic  divides,  thereby  fostering  diplomacy  and  facilitating 
negotiations (Frąckiewicz, 2023)
.
Diplomatic Communication 
Everyday diplomatic correspondence and  queries can  be handled by  chatbots and  virtual assistants, 
freeing up  human  diplomats to  focus on  intricate and  strategic duties.  Additionally, diplomats and 
government officials utilize AI  tools to observe social med
ia for public sentiment and  responses to 
diplomatic initiatives, thereby shaping policy choices and public diplomacy endeavors (Sakhri, 2023). 
Trade and Economic Diplomacy 
AI  can scrutinize economic data and trade patterns, offering insights into strategies and negotiations 
for economic diplomacy. AI  is used  to analyze trade data, market trends, and economic indicators, 


informing trade negotiations and diplomatic efforts to promote economic cooperation (Frąckiewicz, 
2023).
International Law and Compliance 
AI  can  assist in  the  analysis  of  international agreements, treaties, and  legal  documents  to  ensure 
compliance and pinpoint potential violations (Deeks, 2020).
Global Health and Crisis Management 
AI proves invaluable in monitoring and managing global health crises, such as pandemics. It can assist 
in tracking disease spread, evaluating the effectiveness of containment measures, and modeling various 
scenarios. During the COVID
-
19  pandemic, AI was em
ployed for tracking the virus's spread, vaccine 
development,  and  predicting  healthcare  resource  needs.  International  collaboration  on  AI
-
driven 
research and data sharing played a pivotal role in diplomacy and pandemic response (Fontes, Corrigan, 
& Lütge, 2
023).
Disaster Response and Humanitarian Aid 
AI
-
driven   tools  can   enhance  disaster  response  endeavors  by   forecasting  and   evaluating  the 
ramifications of natural calamities and enhancing the allocation of humanitarian assistance. Through 
AI  and  machine learning, data from various sources such  as satell
ite imagery and social media are 
scrutinized  to  gauge  the  consequences  of  natural  disasters  or  conflicts,  assisting  international 
organizations in streamlining humanitarian relief actions and diplomatic reactions (Sachdev, 2023).
Environmental Diplomacy 
AI is used in climate modeling, carbon emissions tracking, and environmental monitoring, supporting 
international agreements like the Paris Agreement (UN environment programme, 2022).
Ethical and Security Concerns 
The integration of AI into diplomacy raises ethical and security concerns. Diplomatic communication 
and negotiations may be vulnerable to AI
-
driven cyberattacks, and there are concerns about potential 
misuse of AI for propaganda or disinformation campaigns
(Krisnata, 2023).
Diplomatic Training 
AI  can  be  employed  for  training diplomats in  various  facets  of  diplomacy,  including  negotiation 
techniques, cultural understanding, and crisis management (Konovalova, 2023).
It  is  obvious  the  progression  and  maturation  of  AI  have  ignited  a  global  race  to  advance  AI 
technologies,  spanning  multiple  sectors,  from  security  to  the  economy.  Partnerships  and  the 

development and deployment of Artificial Intelligence (AI) may well redefine the dynamics between 
nations and their interactions. 
In essence, AI holds the potential to greatly bolster the effectiveness of diplomatic efforts by equipping 
diplomats  and  policymakers  with  invaluable  insights,  advanced  analytical  tools,  and  enhanced 
communication capabilities. Nonetheless,  it  also  poses  m
ultifaceted challenges, spanning  security, 
ethics,  and  the  imperative for  international cooperation  on  AI
-
related  diplomatic  matters.  As  AI 
continues to progress, its role in diplomacy is poised for expansion and adaptation. The impact of AI 
on diplomacy r
emains in a state of continuous evolution, carrying substantial promise for enhancing 
diplomatic endeavors, refining decision
-
making processes, and tackling global challenges. However, 
it  simultaneously raises  pivotal  ethical  and  security  considerations  th
at  demand  the  attention  of 
diplomats and policymakers as they integrate AI into their strategic practices.
Artificial Intelligence  and Ethical Considerations in International Relations
Ethics in  Artificial Intelligence (AI)  within the  context of  international relations is a  complex and 
evolving  topic.  It  involves  examining  the  ethical  implications of  using  artificial intelligence and 
machine learning in various aspects of international r
elations, including diplomacy, conflict resolution, 
security, and global governance (
Vousinas
et al., 2022)
. 
One of the primary ethical concerns in AI is bias. In  international relations, AI systems can reinforce 
existing biases in decision
-
making processes. For example, if the data used to train AI models is biased, 
it can  lead  to  discriminatory outcomes in  dipl
omacy and  international negotiations. Also,  ensuring 
transparency  in  AI  systems  used  for  international relations  is  essential.  Decision
-
makers  should 
understand  how   AI   algorithms  arrive  at  their  recommendations.  Additionally,  there  must  be 
mechanisms in  pl
ace  to  hold  those  responsible  for  AI  decisions  accountable  for  any  mistakes or 
unethical actions (Sutaria, 2022). 
Also, the use of AI  in military applications, such as autonomous weapons,  raises significant ethical 
concerns (Meleouni, 2021). There is an ongoing debate about the moral implications of delegating life
-
and
-
death decisions to  AI  systems in  the  context of  a
rmed conflicts (Geiss,  2017).  AI  can  disrupt 
traditional diplomacy  by  influencing decision
-
making processes  and  information warfare.  Ethical 
considerations involve understanding how  AI  can  be  used  for strategic advantage or to  manipulate 
international even
ts.


Journal of Politics and Ethics in New Technologies and AI 
Volume 2, Issue 1 (2023)                                                                                                     
8
Another  aspect  is  how  resources  are  allocated  for  AI  research  and  development  in  international 
relations  can  have  ethical  implications. There  may  be  concerns  about  inequality in  access  to  AI 
technology, which could affect the power dynamics among nations (
European Parliament, 2020). 
AI  applications in international relations should respect and promote human rights. There should be 
safeguards in place to prevent AI  from being used to infringe upon human rights or suppress dissent. 
However,  AI  systems used in international relations sho
uld be culturally sensitive and  consider the 
diversity of  nations  and  societies. Lack  of  cultural awareness  can  lead  to  misunderstandings and 
conflicts (ÓhÉigeartaigh et al., 2020).
The United Nations General Assembly in September 2023, discussed the role of AI  in international 
relations. The U.N. Secretary
-
General's envoy on technology, Amandeep Gill, hosted an event titled 
""Governing AI  for Humanity"" to address the risks and challen
ges of international cooperation on AI. 
Secretary
-
General António Guterres and Gill believe that a new U.N. agency may be needed to manage 
AI  globally, but the specifics are yet to be determined, and strong support from powerful nations is 
uncertain. One p
roposed model suggests creating an International AI Organization (IAIO) that partners 
with  national  regulators  and  certifies  jurisdictions, which  could  be  more  acceptable  to  powerful 
countries.  Cooperation  with  China  and  Russia  presents  unique  challenges, 
as  both  nations  have 
differing views on international AI oversight (Henshall, 2023).
Finally,  several  scientists and  entrepreneurs among  them  Elon  Musk  and  Mark  Zuckerberg  have 
expressed their concern about the evolution of AI 
–
mainly in the military field
-
in a letter sent to the 
United Nations (Tegmark, 2017; Future of Life, 2015).
Ethics in  AI  in  international relations is  a  critical and  evolving field. It  involves addressing issues 
related to  fairness, transparency, accountability, privacy,  and  the  responsible use  of  AI  in  various 
aspects  of  diplomacy,  conflict  resolution,  and  glob
al  governance.  It  also  requires  international 
cooperation to establish norms and regulations that guide  the ethical use of AI  on  the global stage. 
Addressing AI ethics in international relations requires global cooperation and regulatory frameworks. 
Multil
ateral agreements and international norms are necessary to ensure that AI  is used responsibly 
and ethically across borders.
Conclusions
The impact of Artificial Intelligence (AI) on the realm of International Relations is profound, reshaping 
various facets of global affairs and introducing both opportunities and challenges. AI has the capacity 
to transform conflict resolution through its a
dvanced data analysis tools, which can assist in predicting 

Meleouni & Efthymiou (2023)
https://doi.org/10.12681/jpentai.35803
9
and  preventing conflicts. Furthermore, AI
-
driven diplomatic tools can enhance  communication and 
negotiation processes among nations.
Another domain where AI  can have a positive effect is in bolstering security. It can be  deployed to 
more effectively identify and respond to security threats, encompassing both conventional and cyber
-
related dangers (Cong Truong, Bao Diep, & Zelinka, 2020)
. Nonetheless, alongside these prospects, 
AI also presents novel challenges. The use of AI in cyber warfare has the potential to disrupt a nation's 
infrastructure and compromise its security, giving rise to ethical concerns regarding AI's role in warfare 
a
nd the necessity for international agreements to regulate such practices.
Moreover, the growing reliance on AI  in international relations raises inquiries about the framework 
of  global  governance.  Nations  must  collaborate  to  establish  explicit  guidelines  and  regulations 
governing the responsible utilization of AI on the internat
ional stage. 
In  conclusion,  AI  is  fundamentally  reshaping  the  landscape  of  International  Relations,  offering 
possibilities for conflict resolution, improved diplomacy, and enhanced security. Yet, it simultaneously 
poses fresh challenges in the realms of cyber warfare, 
ethics, and global governance. To navigate this 
evolving terrain successfully, international cooperation and the establishment of responsible AI usage 
standards are imperative, shaping the future of global politics and diplomacy. The influence of AI on 
glo
bal  politics and  diplomacy  will  continue  to  evolve,  exerting  a  lasting  impact  on  international 
relations",https://ejournals.epublishing.ekt.gr/index.php/jpentai/article/view/35803/26837
Do digital technologies pay off? A meta-analytic review of the digital era,ScienceDirect,"Stephen oduro, Alessandro de Nisco, Giado Mainolfi","META ANALYSIS this study aims to examine the extent to which four selected DTs—artificial intelligence (AI), big data analytics (BDAs), internet of things/cyber-physical system (IoT/CPS), and 3D printings (3DP)—impact the overall, financial, innovation, and operational performance of the firm through a meta-analysis",2023,"Highlights
	• DTs have a positive and moderate effect on overall firm performance.  • Aggregate impact of DTs on innovation performance is larger than operational efficiency and financial performance.  • The effect of AI on firm performance is the largest, followed by BDAs, IoTs/CPS, and 3DP.  • DTs performance increases over time but is contingent on the performance outcome.  • Firm size, sector, country development, and industry intensity moderate the DTs/firm performance nexus. 
Abstract
The adoption of digital technologies (DTs) has become a predominant management practice in recent years. However, empirical findings on their performance outcomes are mixed and fragmented across cultures, contexts, and disciplines. Inspired by the resource-based view of the firm, resource orchestration theory, and contingency theory, this study aims to examine the extent to which four selected DTs—artificial intelligence (AI), big data analytics (BDAs), internet of things/cyber-physical system (IoT/CPS), and 3D printings (3DP)—impact the overall, financial, innovation, and operational performance of the firm through a meta-analysis. Employing a random-effects model and psychometric meta-analysis approach based on subgroup and meta-regression analyses, we examine 109 studies with 1,346,242 observations and 165 effect sizes from 2006 to 2021. The findings reveal that DTs have a positive and moderate effect on overall firm performance; however, the impact on innovation performance is the largest, followed by operational efficiency, and finally, financial performance. Moreover, the results show that the impact of AI on firm performance is the largest, while 3DP exerts the lowest impact. Furthermore, boundary-spanning conditions—contextual factors (i.e., firm size, firm age, sector, country development, sector technology intensity, time frame) and methodological factors (i.e., study type, sampling technique, type of performance measure)—moderate the focal relationship (i.e., account for between-study variance). The theoretical and managerial implications of these findings are highlighted.
	Previous article in issue
	Next article in issue
Keywords
Digital technologiesFirm performanceMeta-analysis
1. Introduction
Research on the digital technology (DT)/firm performance relationship has been at the forefront of scientific inquiry in the literature on management and innovation in the past three decades. In particular, amid today's fourth industrial revolution (IR.4.0), with its accelerating and changing work environment, the role of DTs has become a key issue for firms and national economies in facilitating and enhancing competitiveness. For instance, the “UK CATAPULT – High-Value Manufacturing,” the American “Manufacturing USA,” the Dutch “Smart Industry,” and the French “Industrie du Futur” strategies highlight the strategic plan of these countries. They all incorporate universities and industrial players to promote the adoption of digital technologies in manufacturing and service industries with a view to IR4.0 (Calenda, 2016). DTs involve the digitization and integration of business processes both vertically (i.e., across the entire value chain, from product development to manufacturing, distribution, and customer service) and horizontally (i.e., across functional areas) through the adoption of digital tools such as artificial intelligence, big data analytics, robots, internet of things (IoT), 3D printing, etc. (Yu et al., 2021). Recent industry reports (e.g., PWC report, 2017; Deloitte, 2020) indicate that enabling DTs enhances competitiveness in global supply chains.
According to the resource-based view (RBV) of the firm, DTs are complementary resources that can enhance the value of other firms’ resources and capabilities. For example, studies show that DTs enable companies to create links between supply systems, machinery, final products, production facilities, and customers to gather, generate, and share real-time market and operational and strategic information (e.g., Bag et al., 2020). By adopting DTs, companies can generate superior competitive advantage with respect to differentiation, efficiency, and innovation support (Yu et al., 2021), rethink their business models, better organize the operations and structure of the firm (Mubarik et al., 2021; Holmström et al., 2016), transform the source of value generation (Bai et al., 2021), develop and introduce new products and services (e.g., Blichfeldt and Faullant, 2021; Ramadan et al., 2020), transform their innovation processes by dynamically involving customers in product design and development (Agrifoglio et al., 2017), and share common data, information, and knowledge throughout the firm (Dalenogare et al., 2018).
Within this context, it can be assumed that adopting DTs will generate superior firm performance. However, discourse on the effect of DTs on firm performance have always been a topic of hot debate in the literature on innovation, as some scholars postulate that such investment may be “slow to be cashed in” (Benassi et al., 2021, p. 1) because of the high cost and expertise involved in their deployment (e.g., Chae et al., 2018) and that it may not be effective and efficient for all firms (Cirillo et al., 2021). Some scholars also stress that some DTs are insignificant for industrial and company performance (e.g., Dalenogare et al., 2018).
Scholars from different disciplines have examined numerous outcomes of DTs using theoretical underpinnings such as information processing theory, the RBV, dynamic capability theories, and knowledge management theories. For example, some studies significantly focused on the way in which operations are reorganized by adopting DTs (Bag et al., 2020; Dubey et al., 2020). At the same time, others also discuss how the entire organization is affected by these DTs (Holmström et al., 2016), and others mention the efficiency and cost advantages (Anand et al., 2013; Weill, 1992). However, more recent studies consider strategic outcomes such as operational efficiency, innovation performance (Díaz-Chao et al., 2021), supply chain management effectiveness (Erkmen et al., 2021), and financial performance (Zhu and Yang, 2021; Del Gaudio et al., 2021).
Despite these significant research efforts, some relevant gaps remain to be addressed, forming the thrust of this study. First, because empirical studies on the relationship between DTs and firm performance are scattered across cultures, contexts, and disciplines, we have limited comprehension and insight into how and to what extent DTs relate to firm performance and the boundary conditions (moderators) that facilitate or impede the success of DT adoption. Second, as Rialti et al. (2019, p. 2) postulate, “part of the research is still theoretical or at most qualitative,” offering a limited understanding of the investment return of DTs and the strength and direction of the quantitative relationship. Third, previous research investigating this focal relationship has reported mixed and inconsistent findings regarding the DT paradox, which may be partly because efforts to adopt DTs take time and are still ongoing. Some studies report positive relationships (e.g., Cathles et al., 2020; Baabdullah et al., 2021; Rialti et al., 2019), some do not report significant relationships (e.g., Santoro et al., 2020; Agarwall et al., 2022), and others report negative relationships (e.g., Niaki et al., 2019). Lastly, some studies theoretically envisioned some contingencies that might affect the DT/firm performance relationship, such as firm size, type of business, industry sector, industry intensity, socio-economic aspects (Hofmann and Rüsch, 2017; Vacek, 2016), and country development (Zhu and Yang, 2021). Consequently, little is known about how and which types of firms and economies have a greater openness to supplementary DT tender opportunities. This has led scholars to call for an investigation into the underlying mechanisms and organizational factors connecting DTs to firm performance (Erkmen, Günsel, and Altındaǧ, 2020). To date, however, no meta-analysis has synthesized the extant literature on the relationship between DTs and firm performance to highlight the mean/average effect (i.e., strength and direction) of this focal relationship.
To this end, we employ an evidence-based research method with a comprehensive cross-country and temporal perspective (Rosenbusch et al., 2019). Meta-analysis is a “research method for systematically combining and synthesizing findings from multiple quantitative studies in a research domain” (Paul and Barari, 2022, p. 1099). Through meta-analysis, research from numerous contexts is integrated to explore the contingencies in the DT/firm performance association. Moreover, it is the “best method to reach consensus” (Combs et al., 2011, p. 194) when the empirical findings of primary studies are mixed and inconclusive.
More specifically, we aim to address the following research objectives: (1) examine the extent to which DTs—artificial intelligence (AI), big data analytics (BDAs), IoT/CPS, and 3DP—influence firm performance across the extant literature; and (2) explore boundary-spanning conditions. The conditions cover contextual moderators—i.e., firm size (large vs. small), firm age (young vs. mature), country development (emerging economies vs. advanced economies), industry sector (service vs. manufacturing), sector technology intensity (low vs. high-tech), and time frame (before 2019 vs. after 2019 (COVID-19))—and methodological factors—i.e., type of firm performance (accounting vs. marketing measures), sampling technique (probability vs. non-probability), theory use (theoretical vs. atheoretical), study design (experiment vs. survey), and study type (cross-sectional vs. longitudinal)—that may account for between-study variance in this focal relationship.
By addressing the above research gaps, we make several contributions to research in management and innovation. First, a meta-analysis—i.e., a systematic quantitative review—offers a synopsis of the state of the art of a stream of research. When the stream reaches maturity but the empirical findings are contradictory and fragmented, a meta-analysis can unearth novel research avenues, particularly in a fragmented and scattered strand of research such as DT outcomes. To the best of our knowledge, coupled with evidence from the literature, this is the first study to employ a meta-analysis to integrate studies on the relationship between DTs and firm performance. Earlier meta-analyses (i.e., Kohli and Devaraj, 2003; Liang et al., 2010) had different objectives, and studies have yet to capture the contemporary development of the phenomenon since they were conducted more than a decade ago. Accordingly, our analysis draws on the RBV of the firm and resource orchestration theory to clarify the anecdotal results in the field, identify the extent of the impact of DTs on firm performance, and contribute to the methodological rigor of the field.
Second, our study draws on the contingency theory to address the direction and strength of the DT/firm performance relationship by investigating their context dependence. Building an understanding of DT outcomes requires an examination of contextual and methodological factors. Therefore, our findings corroborate the long-term call for adopting context-sensitive methodologies (Meyer and Peng, 2016) for research at the intersection of DTs and firm performance.
From a pragmatic point of view, it can be observed that albeit the object of much interest, deep insights about when, why, and how investments in DTs enhance company performance are scarcer for managers and practitioners due to the diverse and fragmented nature of the studies. Our subgroup analysis regarding aggregate effects, as well as the elasticity of each of the investigated DTs with respect to firm performance, will enable innovation managers, business executives, and practitioners to understand which DTs are very effective and efficient and for which performance outcomes. This in turn can help such figures to select the right DTs to achieve their strategic goals and orientations in line with their budget and resource allocation. Moreover, since some DTs are in their infancy and a high degree of uncertainty surrounds the future impact of these technologies, the findings of the study, which amalgamates the effect sizes across multiple studies, will provide managers and practitioners with the expected mean/average effect of the DT/firm performance nexus and the factors that moderate this focal relationship.
The remaining sections of the review are organized as follows. Section 2 presents the theoretical framework and hypotheses of the study. Section 3 addresses the meta-analytic method of the study, addressing our search process, exclusion and inclusion criteria, coding, and integration scheme. The results and analysis section follows in Section 4, while Section 5 presents the discussion and conclusions. Section 6 contains some implications, and Section 7 presents the limitations and suggestions for future research.
2. Theoretical background and hypotheses
The resource-based view of the firm and the resource orchestration theory underpin this study. The RBV of the firm underscores that organizations should secure unique corporate resources to obtain sustainable competitive advantage (Barney, 1991) through the VRIO framework: a resource model that comprises value, rarity inimitability, and organization (Terziovski, 2010). Resources consist of tangible (e.g., plant) and intangible assets (e.g., brand image) that can enhance a firm's productivity and quality of service. Thus, the DTs infrastructure and resources such as AI, BDAs, IoT/CPS, and 3DP have the potential to generate a firm-specific capability that is valuable, inimitable, rare, and essential for sustainable competitive advantage (Wamba et al., 2020; Erkmen, Günsel, and Altındaǧ, 2020). The RBV of the firm advocates that the development or acquisition of DT resources and capabilities should generate a superior competitive advantage. However, this proposition sometimes does not account for synergies among resources or capabilities (Liu et al., 2016) or the key role played by management in gleaning or exacting gains or advantages.
Resource orchestration theory buttresses the RBV of the firm by suggesting that an amalgamation of managerial shrewdness, capabilities, and resources brings about superior performance (Chadwick et al., 2015). Based on this theory, securing performance resulting from technological or digital capabilities hinges on alignment and configuration among tasks/activities in the firm. For instance, a firm's ability to translate a supply chain management capability to a competitive advantage may depend on its capability to leverage information technology (IT) (Liu et al., 2016). At the same time, supply chain management and IT activities alone may only produce superior performance with adequate alignment among them (Chadwick et al., 2015). This implies that the way in which resources are employed is equally as important as the resources possessed.
To this end, managers must be able to synchronize the type and level of resources required to implement the strategy with the specific strategy adopted. The resource orchestration theory states that coordinating technological capabilities and synchronizing processes are essential for superior firm performance (Candi and Beltagui, 2018). Accordingly, we draw on these theories to postulate that DTs can trigger firm performance—financial, innovation, and operational efficiency/performance—by enhancing organizational capabilities and management orchestration and synchronizing processes and activities.
2.1. DT Classification and firm performance
Industrial expansion has evolved over the decades, and it is currently in the era of Industry 4.0 (Bai et al., 2021). Industry 4.0 involves the “integration of complex physical machinery and devices with networked sensors and software, used to predict, control and plan for better business and societal outcomes” (Kagermann et al., 2013, p. 5). Digital technologies for Industry 4.0 are transforming business models and driving transformative change. The digital transformation involves “the use of new digital technologies to enable major business improvements in operations and markets, such as enhancing customer experience, streamlining operations, or creating new business models” (Paavola et al., 2017, p. 2). Thus, DTs generate value by enhancing operational processes, customer experience, and business models. DTs for Industry 4.0 has taken on much importance and visibility in firms and industries as these technologies support productivity, production efficiency, and flexibility through different emerging smart information, and communication capabilities and technologies (Rüßmann et al., 2015a, Rüßmann et al., 2015b). DTs involve the digitization and integration of business processes both vertically (i.e., across the entire value chain) and horizontally (i.e., across functional areas) through the adoption of digital tools to enhance automation, communication, and connectivity (Yu et al., 2021; Ardolino et al., 2018).
Several archetypes of DTs for Industry 4.0 are identified in the literature. For instance, some scholars (e.g., Vial, 2019) have identified models such as SMACIT (social, mobile, analytics, cloud, and Internet of Things), and others (e.g., Schwab, 2015; Van Veldhoven and Vanthienen, 2022) have identified BRAICQ (blockchain (and smart contracts), robotics (including 3D printing), artificial intelligence (and big data), and cognitive and quantum computing) as important DTs that play a vital role in digital transformation. Others (e.g., Bag et al., 2020; Dalenogare et al., 2018; Bag et al., 2020) have noted that DTs include, but are not limited to, big data and analytics, artificial intelligence, additive manufacturing (3D), industrial Internet of Things, blockchain, simulations, and the cloud. Likewise, Benassi et al. (2021), drawing on the reports and academic articles by international organizations—namely the United Nations Industrial Development Organization, European Commission, EPO, and Organisation for Economic Co-operation and Development (OECD)—identified cyber-physical systems/industrial Internet of Things, artificial intelligence, cognitive computing, and big data analytics, cloud computing/manufacturing, augmented reality, and wireless technology as important DTs driven by Industry 4.0.
This study considers four elements of DTs—namely, artificial intelligence (AI), big data analytics (BDAs), Internet of Things/cyber-physical systems (CPS), and 3D printing (3DP)—because (1) these technologies are the most quantitatively examined concerning firm performance, and (2) they have a significant connection with improved performance (e.g., Imran et al., 2018; Mubarik et al., 2021). Scholars note that while these DTs may share some aspects, they are distinct elements of DTs for Industry 4.0 (e.g., Benassi et al., 2021; Van Veldhoven and Vanthienen, 2022; Cathles et al., 2020). Research shows that by integrating information and communication technologies (ICTs) within firms and transforming control systems in business operations, these DTs can generate superior performance benefits for organizations (e.g., Dubey et al., 2020; Liu et al., 2022).
Firm performance involves organizational effectiveness in terms of operational and financial performance (Saraf et al., 2007). This study divides firm performance into three strands: financial performance, operational efficiency/performance, and innovation performance. Financial performance demonstrates the firm's ability to make a profit or economic gains and can be measured by financial indicators such as return on investment (ROI), profits, economic performance, etc. (Cirillo et al., 2021). Innovation performance refers to a firm's ability to develop new technology and processes and buy new tools to enhance its product/service quality and performance (Blichfeldt and Faullant, 2021; Brea, 2021). Finally, operational efficiency/performance refers to a firm's ability to produce products and services cost-effectively and with minimal resources without compromising quality (e.g., supply chain efficiency, cost, cycle time, waste reduction, productivity, etc. (Bag et al., 2020). By encapsulating these key performance dimensions, our study addresses both internal use and product/service applications of DTs—internally-oriented capabilities and optimization of processes/production and externally-oriented digital features of products and/or services.
Empirical evidence shows that by adopting DTs, firms can generate superior competitive advantage vis-a-vis differentiation, efficiency, and innovation support (e.g., Bag et al., 2020; Yu et al., 2021). In addition, studies show that DTs can help firms build additional sales channels that facilitate external knowledge and information availability related to product characteristics and varieties (Bianchi and Mathews, 2013; Nieto et al., 1998). Further research shows that DTs can generate economic benefits in terms of cost reduction and profitability, while increasing firm diversification and pre-empting radical and incremental innovations (e.g., Blichfeldt and Faullant, 2021). Scholars (e.g., Cassetta et al., 2020) have also found that DTs can facilitate internationalization, reduce cycle times, enhance flexibility in manufacturers' supply chains, and deliver products/services to end users on time. However, studies have also underscored that some DTs are not significant for industrial and company performance (e.g., Dalenogare et al., 2018) and are too expensive and risky (e.g., Chae et al., 2018; Yu et al., 2016). Despite the mixed findings, we draw on the RBV and the resource orchestration theory that a firm's possession of DTs coupled with the management's ability to synchronize the technological capabilities and coordinate processes can lead to superior performance. Thus, we postulate:
H1
DTs have an overall significant, positive effect on a firm's (a) overall performance, (b) financial performance, (c) innovation performance, and (d) operational efficiency/performance.
The first element of DTs examined is artificial intelligence. AI relates to the ability of a system to interpret and learn from data by mimicking human intelligence (Rahman et al., 2021). This area of computer science emphasizes the creation of intelligent machines that work and react like humans (Bag et al., 2020). AI enables applications such as analytics, dynamic pricing, process automation, and optimization and prediction to help firms benefit from greater revenue, efficiency, agility, productivity, reliability, and better decision-making and customer experience (Baabdullah et al., 2021). AI principally involves two facets, i.e., perceptual intelligence and computational intelligence. Perceptual intelligence denotes the perceptive capabilities of humans such as touch, hearing, and vision, while computational intelligence relates to quickly calculating and storing memory, including the application of algorithms and data. Implementing AI involves four steps: (1) understanding AI and the organizational capabilities required for digital transformation; (2) understanding the present business model, its potential for innovation, and its role in the business ecosystem; (3) developing and refining capabilities required to implement AI; and (4) reaching organizational acceptance and developing internal competencies (Reim et al., 2020).
AI practices make it much easier to process and interpret highly complex and diversified information (Dubey et al., 2020), which helps firms become more aware about decisions concerning resource flow, market demand, and capacities. Thus, it has been shown that AI can enhance a firm's business capability and value (Agarwall, Das, and Swain, 2022), operational performance (Dubey et al. (2020), and technological innovation (Liu et al., 2020). Research shows that adopting AI increases market share and higher revenues (Basri, 2020), relational governance, and business-customer interactions (Baabdullah et al., 2021). These reviews show both internal uses and product/service AI applications. However, Xu et al. (2021) highlight the dark side of AI regarding privacy, security, and ethics, which can impede performance gains. Nevertheless, we expect that AI will positively affect firm performance. Formally, we postulate:
H1a
AI has a positive, significant effect on (a) overall performance, (b) financial performance, (c) innovation performance, and (d) operational efficiency/performance.
The second element of DTs that we explore is big data analytics. Big data can be defined as large, heterogenous data with a large volume, high velocity, and great variety (Ghasemaghaei et al., 2018), allowing firms to monitor the performance of business units, assets, internal processes, and value-chain bottlenecks in real-time (Rialti et al., 2019). It refers to the strategy of analyzing large volumes of data that are used when traditional data mining and handling techniques cannot uncover the insights and meaning of the underlying data (Bag et al., 2020). Akter et al., 2016a, Akter et al., 2016b categorize BDAs into three types: BDA technology capabilities, BDA management capabilities, and BDA talent capabilities. Implementing BDA capabilities involves a three-stage process: acceptance, assimilation, and routinization integrated into the corporate commitment (Singh and El-Kassar, 2019). The existing literature essentially measures BDAs with variables such as real-time data, large volumes of data, different types of data, automated data recording, processing, and transmission (e.g., Niebel et al., 2019). BDAs are important in production, recovery, design, and operations. Studies reveal that BDAs can help firms improve overall customer experience, ambidexterity and agility, and economic performance (Akter et al., 2016a, Akter et al., 2016b). Dubey et al. (2020) found that BDA positively affects operational performance. However, Ghasemaghaei and Calic (2019) found that BDAs negatively influence intrinsic decision-making quality, indicating that big data volume and prescriptive dimensions have no significant impact on innovation competency or insight generation. Despite the mixed results, we hypothesize that:
H1b
BDAs have a positive, significant effect on (a) overall performance, (b) financial performance, (c) innovation performance, and (d) operational efficiency/performance.
The third element of DTs examined is IoT/CPS. First proposed by Ashton (2009), the Internet of Things (IoT) refers to uniquely identifiable objects interlocked with radio-frequency identification (RFID) technology that have the potential to generate and share data for automated analysis. The use of tags and sensors in IoT/CPS technologies allows system devices and objects to communicate and disseminate information. Moreover, IoT/CPS involves a network that connects things wirelessly via smart sensors, with interaction free of human intervention. Atzori et al. (2010) showed that IoT/CPS technologies align with three main visions: orientation around the Internet, things, and semantics. The IoT is operationalized as the interconnection of things for different purposes, such as communication, sensing, gathering, and identification, which help the management to control processed information, monitor/observe processes, measure results, and drive efficiency gains (Ehret and Wirtz, 2017). For instance, Honda is using IBM IoT for automotive solutions based on IBM Watson IoT technology to directly deliver car-generated data involving pressure, power levels, and temperature to the cloud for real-time analysis (Rehman et al., 2021). Empirical evidence reveals that IoT/CPS can have a positive impact on firms' market value (Tobin's q) and financial performance, particularly with respect to return on assets (ROA) (Tang et al., 2018). Research indicates that a firm's IoT/CPS capability also aids in improving innovation processes by utilizing existing resources, information, and knowledge (Yu et al., 2016), as well as supply chain efficiency (Rehman et al., 2021). However, some studies offer no empirical evidence that the IoT/CPS can improve supply chain integration, and therefore operational efficiency (de Vass et al., 2018). Despite this, we draw on the RBV and resource orchestration theory to propose that:
H1c
IoT/CPS have a positive, significant effect on (a) overall performance, (b) financial performance, (c) innovation performance, and (d) operational efficiency/performance.
The last element of DTs examined is 3DP (additive manufacturing). This is a set of additive manufacturing processes that generate products by forming layers of plastic, metal, or other material directly from digital design files (Holmström et al., 2016). It is operationalized using versatile, inventive manufacturing machines that facilitate the flexibility of manufacturing systems by transforming three-dimensional models into solid, 3D physical products through a cycle of additive or layered digital development models. These processes involve the laser hardening of liquid polymers, laser melting of metal powders, and extrusion of molten plastic into solid objects (Candi and Beltagui, 2018). These procedures combine digital and physical elements, entrenching information technology in manufacturing processes. The implementation of 3DP therefore spreads the digital transformation of information-based procedures into the digitization of physical processes (Rindfleisch et al., 2017). Based on the resource orchestration theory, the mere ownership of resources cannot generate a competitive advantage. Instead, managers must be able to leverage, bundle, and structure resources and capabilities to achieve competitiveness. With respect to 3DP, this implies that the synchronization of IT and manufacturing capabilities determines superior performance. Candi and Beltagui (2018) found that 3DP positively influences innovation performance. Furthermore, research shows that 3DP improves supply chain performance (Delic et al., 2019), increases the reliability and speed of order fulfillment (Cathles et al., 2020), and stock returns (Lam et al., 2019). Delic et al. (2019) found that while additive manufacturing strongly influences supplier integration and internal integration, it has a weak effect on customer integration. We therefore propose the following:
H1d
Three-dimensional printing (3DP) has a positive, significant effect on (a) overall performance, (b) financial performance, (c) innovation performance, and (d) operational efficiency/performance.
2.2. Contextual and methodological moderators
2.2.1. Contextual factors
Several boundary-spanning factors, both contextual and methodological, are included in the framework to examine the factors that may account for between-study variance in the primary studies. Moderator analysis allows researchers to capture variations in the relationship(s) of interest and resolve contradictory findings (Paul and Barari, 2022). From the theoretical perspective of the contingency theory, different contexts or environments mostly have different needs, therefore requiring differentiated approaches to operations management (Romero-Silva et al., 2018). In short, this theory postulates that a firm's innovation strategies depend on internal and external variables (Jansen et al., 2006). Following this, we examined firm size, firm age, country development, industry sector, technology intensity of the sector, and sampled period as context-dependency moderators that may influence the DT/firm performance relationship.
First, firm size matters because it may influence the firm's ability to adopt and implement DTs. Large enterprises are more likely to exploit DTs than small firms since they possess more resources and are not very prone to environmental vicissitudes and performance fluctuations (Brynjolfsson and McElheran, 2016). However, small firms are at least inclined to diseconomies in implementing DTs due to the higher cost and complexity of managing multiple synergies across technologies (Belderbos et al., 2006). Nevertheless, Cassetta et al. (2020) argue that the potential advantages of deploying DTs may even be higher for small firms since it can mitigate traditional burdens associated with size and the inability to commit human and financial resources. Therefore, we hypothesize that:
H2a
The effect sizes of DTs on firm performance are larger for small firms than for large firms.
Moreover, research shows that the effects of DTs seem to be more concentrated among mature firms than younger firms (Cirillo et al., 2021; Tambe, 2014). However, some studies show that possible structural inertia associated with mature firms may slow their adoption of DTs, whereas younger firms (start-ups) require less coordination, communication, and influence to support and implement such technologies (Zhu et al., 2006). Research indicates that most new ventures—for instance, in the IoT industry—may need more business motives to innovate because of inherent risks related to innovation, coupled with erratic investment limitations (Yu et al., 2016). Nevertheless, we believe the new ventures can focus on process innovation to minimize material consumption and production costs since this risk is lower. Therefore, we propose that:
H2b
The relationship between DTs and firm performance is moderated by firm age, such that effect sizes are larger for young/start-up firms than for mature firms.
According to the dynamic capabilities theory, a firm's ability to reconfigure and integrate resources and capabilities may vary by industry sector because each sector has a unique set of business circumstances, strategic orientations and capabilities, and distinct customer relationships and resources, which in turn influence performance (Upadhyay and Kumar, 2020). For instance, service firms are more stable with respect to economic cycles and less sensitive to price competition than manufacturing firms (Belderbos et al., 2006). But research also shows that manufacturing firms need greater levels of eco-efficiency, innovation, and sustainable systems and strategies to implement technological capabilities and resources compared to service firms (Garay and Font, 2012). Nonetheless, we believe this will empower manufacturing firms to adopt more cost-saving strategies to reduce costs and increase competitive advantage. Moreover, opportunities for applying some of these technologies are much larger in manufacturing (e.g., CPS) than in service firms. This leads to the following hypothesis:
H2c
The effect sizes of DTs on firm performance are larger for manufacturing firms than for service firms.
Furthermore, DT performance effects might be influenced by the technological intensity of the sector (taxonomy restricted to manufacturing) because, for example, high-tech firms are more motivated by their strong interest in technology-based innovations to enhance performance compared to their low-tech counterparts (Mubarik et al., 2021). However, some studies also show that low-tech firms mostly enjoy constant periods of stability from IT investments, slow product updates, and great homogeneity (Santoro et al., 2020). Moreover, low-tech firms are more likely to invest in DTs since they have a stronger need for operational efficiency and business model innovations. Thus, we propose:
H2d
The nexus between DTs and firm performance is moderated by the technological intensity of the sector, such that effect sizes are larger for low-tech firms than for high-tech firms.
Based on the structural economic theory, it is necessary to consider structural characteristics when conducting economic and technological development analysis. This means that structural differences among nations, how economies adjust, and their responsiveness to development policies may affect how DTs and sustainable innovations spread to all countries worldwide (Huber, 2008). Thus, unequal development can restrict the adoption of technological innovations in developing countries compared to developed ones. Some studies (e.g., Feng, Ma, and Jiang, 2020) underline that country development might influence DTs performance, indicating that emerging economies are more effective than advanced ones. Thus, using the Human Development Index of countries (Kuzma et al., 2020), we grouped the countries into developing/emerging economies and advanced economies. The countries were identified and grouped based on where the data collection occurred (operating place). Developed countries have relatively developed enterprises with abundant resources or “reserves” willing to invest in DTs and the availability of venture capital, but they are relatively mature, and there may be less room for more development. On the other hand, emerging countries may be at the initial or growth stage of implementing DTs, with a greater urge for operational efficiency and flexibility. This leads to the hypothesis:
H2e
The effect sizes of DTs on firm performance are larger for emerging economies than for advanced economies.
Lastly, we explored time to understand how DTs and performance have evolved. Based on the technology discontinuity theory, studies show that technology performance increases over time (Rothaermel and Hill, 2005). To analyze this, we divided the sample into two periods: before 2019 (COVID) and after 2019. According to Tushman and Anderson (1986, p. 441), technological discontinuity involves a “technical advance so significant that no increase in scale, efficiency, or design can make older technologies competitive with the new technology.” Thus, it has been suggested that technological performance increases substantially over time with new digital tools (Papadopoulos et al., 2020). The McKinsey Global Survey shows that COVID-19 pushed firms over the “technology tipping point,” that firms’ adoption of digitization in customer interaction, supply chain interactions, and internal processes increased tremendously (McKinsey, 2020), and that the firms that truly enhanced their performance were those who employed DTs to modify their products and services faster (Li et al., 2022). Given that COVID-19 led companies to use real-time analytic methods in their operations, we can expect that DTs helped firms realize enhanced performance during and after the COVID-19 pandemic. Thus, we propose that the digital performance of firms during and after the pandemic will be higher than the performance before the pandemic. This leads to the following hypothesis:
H2f
The effect sizes of DTs on firm performance are larger for studies conducted after and during 2019 (COVID) than in those conducted before 2019.
2.2.2. Methodological factors
Besides contextual factors, field experts maintain that the inconsistent results can be partly attributed to methodological factors that may influence the strength of the focal relationship. Thus, we explored the type of performance measure, sampling technique, theory use, study design, and study type as methodological moderators to check if they account for between-study variance.
For instance, the literature shows that accounting-based measures (e.g., ROI) have a higher effect size than market-based measures (e.g., Tobin's Q) (Allouche and Laroche, 2005).
H3a
Proxy of firm performance will positively moderate the DT/firm performance relationship, such that it is closer in studies using marketing measures than those using accounting measures.
Moreover, a meta-analytic review can reveal whether specific sampling techniques affect the results. In particular, probability sampling tends to be more representative of the population and minimize the random error in variance than non-probability sampling (Fern and Monroe, 1996).
H3b
The effect sizes of DTs on firm performance are larger for studies using probability sampling technique than those using non-probability sampling.
We explore the influence of theory use since studies show that theory-driven studies are more likely to generate quality, accurate responses than atheoretical studies (Lu et al., 2016). This moderator is worth testing because traditional thinking in the literature is that theory-driven studies are based on well-grounded models and constructs that reflect real-life scenarios (Lu et al., 2016), so they have the potential to produce quality, accurate responses compared to atheoretical studies. Thus, we hypothesize:
H3c
Theory use positively moderates the DT/firm performance relationship, such that it is larger in theory-driven studies than in atheoretical studies.
Furthermore, the choice of study design (survey vs. experiment) may embody a potential moderator factor because experimental studies allow researchers to exercise significant control over scenarios and assign subjects randomly from different groups of participants. This in turn yields less variance error in the denominator of the correlations and generates larger effect sizes (Oduro et al., 2022). Surveys, on the other hand, may suffer low response rates and self-reported data (Peterson, 2001).
H3d
The effect sizes of DTs on firm performance are larger for studies using survey design than for studies using experimental design.
Finally, we investigate the influence of study type, whether cross-sectional or longitudinal, because Cook and Ware (1983) suggest that longitudinal studies may yield more precise estimates of temporal changes or treatment effects than cross-sectional studies. This is due to their statistical power to eliminate between-individual variation, which in turn reduces bias. The studies that examined the DT/firm performance relationship for more than one year (e.g., 2008–2011) are considered longitudinal, while the one-off studies are considered cross-sectional. We propose that:
H3e
Study type positively moderates the DT/firm performance relationship, such that the effect sizes are larger for longitudinal studies than for cross-sectional studies.
2.3. Meta-analytic model
Fig. 1 shows a diagram of the meta-analytic conceptual framework, which served as the roadmap of the study. The model shows that the four most frequently investigated DTs with performance (i.e., AI, BDAs, IoT/CPS, and 3DP) synchronously affect firm performance—overall performance, financial performance, innovation performance, and operational efficiency/performance. With respect to the sign of the focal association, we anticipate that the four (4) selected DTs will positively affect overall performance, financial performance, innovation performance, and operational efficiency/performance despite the mixed, fragmented results. Moreover, we expect the moderating variables (contextual & methodological) to have a positive impact on the DT/firm performance relationship. Thus, this meta-analytic model guided the theoretical reasoning of the study.

3. Methodology
3.1. Search Process and article identification
In this study, we chose to conduct a meta-analytic review because earlier reviews in this research stream appear to be qualitative. However, no meta-analysis synthesizes the extant literature on the relationship between DTs and firm performance to highlight the mean effect of this focal relationship or the strength and direction of the associations. Therefore, following the example of previous meta-analytic reviews in the management literature, we started the review with a bibliographic keyword search to select articles that investigated the relationship between DTs and firm performance from 2006 to 2021 in internationally recognized business and management databases, namely Web of Science, Scopus, EBSCO, Google Scholar, ABI Proquest, and Emerald Insight.
Furthermore, we conducted a reference analysis in the previous narrative reviews and a manual search in some leading innovation journals such as the Journal of Cleaner Production, Journal of Innovation Management, Technovation, Decision Support Systems, Technological Forecasting, and Social Change, Research Policy, JPIM, Industrial and Corporate Change, R&D Management, European Journal of Information Systems, Journal of Computer Information Systems, Information and Management, International Journal of Production Economics, and Decision Support Systems, where studies on DTs are most likely or frequently published, thereby optimizing the reach of our search.
The search was conducted by entering the following keywords in the title, abstract, and keywords field in the databases with the use of the Boolean operations such as “OR, AND, NOT: DTs (e.g., “digital transformation,” “digital technologies,” “digital innovation,” “information technology,” “industry 4.0”, “fourth industrial revolution,” “internet of things (IoT),” “3D printing,” “additive manufacturing,” “robotics,” “artificial intelligence,” “Big data analytics,” “Cyber-physical systems,” “CPS,” “Clouding,” “digital technologies,” and firm performance: financial performance (e.g., ROA, ROE, market share, profit, economic performance, sales revenue, etc.); operational efficiency/performance (e.g., customer integration, internal business process; supply chain efficiency, cost, quality, etc.); composite firm performance, (firm performance, company performance, organizational performance, innovation performance, operational efficiency/performance, financial performance, nonfinancial performance, etc.); innovation performance (e.g., business model innovation, new product development, new product patent application, process innovation; technological innovation, etc.).
3.2. Inclusion and exclusion criteria
Studies were included in the analysis based on the following criteria: (1) peer-reviewed English language, full-text online scholarly articles that examined at least one type of DT (e.g., artificial intelligence, big data analytics, IoT/CPS, and 3DP) in association with firm performance, so the study was focused on one of the relationships following DT/firm performance; (2) the study had to be quantitatively manipulated, i.e., the study provides a coefficient metric or enough statistical information to facilitate the computation of correlation coefficients; (3) studies without correlation coefficients provided r-variants (F-test, T-test, regression, p-value, etc.) that could be converted to r-coefficients; and (4) the study must be independent, that is, does not present two different results from the same sample, in which case we selected the sample with detailed information.
In the initial search, 1786 articles were generated across the databases, but these articles were filtered against different elements such as scholarly articles, year, English language, and relevant topics. We also removed duplicate articles and articles that were not analyzed statistically. After the inclusion and exclusion phases, 109 scholarly articles were identified, with 1,346,242 observations and 165 effects from 2006 to 2021 which met the specifics and inclusion criteria of our model. Our selection process is shown in Fig. 2.

The relevant studies were then coded based on their conceptual features (i.e., DT, firm performance) and methodological and contextual characteristics (e.g., firm size, type of performance measure). Of the total, 98 studies examined just one technology, four examined two technologies, three examined three technologies, and only one study examined four technologies (Bag et al., 2020). All authors coded the independent, dependent, and moderating variables separately based on the conceptual definitions of the constructs, and we agreed with more than 96% of the independently coding decisions. Where disagreements arose, we resolved them through discussion and cross-examination, thereby confirming the validity of the study.
3.3. Meta-analytic metric and integration of effect sizes
We adopted the Pearson correlation coefficient metric as the meta-analysis metric for the following reasons: (a) it is easy to interpret; (b) it allows r-contrast to be computed in cases where no correlation coefficients are directly reported (Oduro et al., 2021); and (c) it is the standard metric frequently used in the business and management literature (Feng et al., 2020; Rosenbusch et al., 2019; Rousseau et al., 2016). The correlation coefficients were either picked directly from the study or computed through the r-variants (Roschk et al., 2017). For the studies that provided only p-values, we followed the conversion procedure suggested by Rosenthal and DiMatteo (2001, p. 72) to convert them to correlation coefficients, whereas regression coefficients were converted to r-correlations using the formula suggested by Peterson and Brown (2005): r = 0.98β + 0.05λ with λ = 1 when β > 0 and λ = 0 when β < 0.
Likewise, regarding the studies that provided more than one correlation coefficient for the same relationship (e.g., financial indicators like ROA and ROS), we averaged the effect sizes to suppress the problem of sample overrepresentation (Kirca et al., 2005; Roschk et al., 2017). We then adjusted the effect sizes for reliability to correct for attenuation arising from random measurement error (Hunter and Schmidt, 2004), where the effect size (rxy) is divided by the square root of two variable measurement reliability products √(rxx)*√(ryy). We used the mean sample size-weighted reliability for studies that employed single-item measures or did not provide reliability indices (Hunter and Schmidt, 2004). Finally, we examined the sample-size weighted means of all correlation coefficients across the studies for each association, denoted as rz (Hunter and Schmidt, 2004).
With respect to the two meta-analysis models—namely, fixed and random effects—the latter was chosen because of the broad theoretical and methodological disparities across the studies investigated here. The fixed effect assumes no heterogeneity across studies (only sampling error), but the random effect assumes that study-to-study variance is not the same for all subgroups or studies that employed different methodologies (Borenstein et al., 2021).
As for the issue that multiple effect sizes from a single study have a higher probability of being correlated, which produces invalid statistics and inaccurate inferences in hypothesis testing, this was addressed by using Huber-White (H–W) OLS in adjusting the variance-covariance matrix to correct for autocorrelation within studies and for arbitrary heterogeneity, thereby obtaining robust standard errors (Wooldridge, 2006). Again, one major concern in meta-analysis is that environmental factors (contextual factors) may be highly correlated, thereby potentially leading to multicollinearity issues. For this reason, and analogous to previous meta-analyses (e.g., Mueller et al., 2013), we included the contextual variables in separate models in the meta-regression analysis.
3.4. Publication bias
Furthermore, we tested publication bias, which indicates that positive results are easier to publish than negative results (Feng, Ma, and Jiang, 2021) and that research with positive, statistically significant results is published in more prestigious journals and cited more times, making it more visible and easier to find (Borenstein et al., 2021). Using the conventional method known as the funnel plot in Trim and Fill, and we found no publication bias issues (see Fig. 3).


Moreover, we used Q statistics to evaluate the heterogeneity of the generalizable effect sizes (study-to-study variance). Finally, the significance of the moderator analysis was examined via both subgroup analysis using QB and meta-analytic regression analysis (MARA), which simultaneously examines interdependencies in the relationship.
4. Findings
4.1. DTs and firm performance
Table 1 shows the aggregate effects of DTs on overall firm performance, financial performance, innovation performance, and operational efficiency/performance. Overall, our findings show that the average strength of the generalizable effect sizes is significantly moderate-large (0.29–0.58), based on the criteria suggested by Cohen (1988), according to which a coefficient size of 0.20 is small; 0.50 is medium, and 0.80 is large. First, our findings reveal that DTs positively and significantly influence the overall performance of firms—financial, innovation, and operational efficiency (r = 0.47) since the confidence intervals do not include zero, and the fail-safe N shows that 15,928 studies are required to render this result nonsignificant. Fig. 4 shows the distribution of the true effects, which indicates the normal distribution of the mean effect size of the analysis.

The results further reveal that the impact of DTs on innovation performance (r = 0.47) is larger than on operational efficiency (r = 0.45) and financial performance (r = 0.41), implying that DTs have more potential to enhance the firms’ inclination toward innovation in terms of business model and technology. In addition, our findings indicate that the effect of AI on overall firm performance ranks the highest (r = 0.58), followed by BDAs (r = 0.49) and IoT/CPS (r = 0.40). Interestingly, 3DP ranks the lowest (r = 0.29), which shows that the three most influential DTs on firm performance are AI, BDAs, and IoT/CPS, in that order.
Table 2 shows the disaggregate or relative effects of DT archetypes on financial performance, innovation performance, and operational efficiency/performance. As observed, the comparative results of each DT on the three performance dimensions within groups show that each DT archetype is positively and significantly related to financial, innovation, and operational efficiency/performance. However, the findings present some nuances that should be noted. For example, the impact of AI on financial performance is stronger (r = 0.74) than its impact on operational efficiency/performance (r = 0.57) or innovation performance (r = 0.52). This implies that AI is more correlated with financial performance than with nonfinancial performance.
In contrast, BDAs have a greater impact on the two nonfinancial performance dimensions, namely innovation performance (r = 0.49) and operational efficiency (r = 0.48), compared to the financial performance dimension (r = 0.37). This indicates that BDAs have a larger impact on innovation performance than on operational efficiency or financial performance. Likewise, IoT/CPS correlates more strongly with financial performance (r = 0.33) than with operational efficiency (r = 0.30). Surprisingly, we found that IoT/CPS does not significantly influence innovation performance since the confidence interval includes zero. Lastly, the average effect of 3DP is larger for operational efficiency/performance (r = 0.36) than for innovation performance (r = 0.19). For financial performance, we found no significant impact of 3DP (r = 0.10; CI = −0.17–0.35).
It is also worth underscoring the disaggregate effects across groups. For financial performance, it can be seen that the most influential DT archetype is AI (r = 0.74), followed by BDAs (r = 0.37), and IoT/CPS (r = 0.33). With regard to innovation performance, AI is at the top (r = 0.52). This is followed by BDAs (r = 0.49) and 3DP (r = 0.19). Likewise, AI has the largest impact on operational efficiency (r = 0.57), followed by BDAs (r = 0.48). Our findings show that AI and BDAs are the most influential DTs for enhancing firms' financial performance, innovation performance, and operational efficiency/performance.
4.2. Study moderators
The Q-statistic and Higgins I-squared statistics (Table 1) show the presence of significant heterogeneities across the studies. We therefore conducted a moderator analysis via subgroup analysis (QB) and meta-regression (MARA) to identify the contextual and methodological moderators responsible for heterogeneities in the effect sizes. Table 3 provides the subgroup analysis, while Table 4 shows the MARA.
Our second objective was to examine the relationship between the contextual and methodological moderators in the DT/firm performance relationship. The first part of the analysis addresses the contextual moderators and the first contextual moderator concerns the sector of operation. Both the subgroup analysis (QB = 12.94, p = 0.00) and regression analysis (B = −0.124, p = 0.015) show that industry sector moderates the focal relationship, such that it is larger for service firms (r = 0.49) than for manufacturing firms (r = 0.44). The regression indicates that the average performance difference between manufacturing and service firms is 12.4% less on average. Second, our findings suggest that firm size moderates the DT/firm performance relationship (subgroup analysis (QB = 51.64, p = 0.00); regression analysis (B = −0.126, p = 0.036), such that it is stronger for small firms (r = 0.54) than for large firms (r = 0.45). The regression results show that the average performance difference between large firms and small firms (reference group) is 12.6% less on average. Likewise, the DT/firm performance relationship is moderated by the technological intensity of the sector (QB = 4.98, p = 0.00; B = 0.127, p = 0.004), such that low-tech firms (r = 0.53) benefit more from DTs than their high-tech counterparts (r = 0.44).
Again, both our subgroup analysis (QB = 5.27, p = 0.00) and regression analysis (B = 0.113, p = 0.005) support our prediction that firm age moderates the DT/firm performance relationship, such that start-ups firms stand to benefit more from DTs (r = 0.49) than mature firms (r = 0.45). As well, our findings support the prediction that the relationship between DTs and firm performance is moderated by country development (QB = 73.14, p = 0.00; B = 0.164, p = 0.001), in such wise that the effect is stronger in emerging economies (r = 0.53) than in advanced economies (r = 0.41).
Lastly, our analysis confirms the proposition that the DT/firm performance relationship increases over time (QB = 2.77, p = 0.001; B = −0.059, p = 0.05), such that performance in studies before 2019 (r = 0.43, p = 0.001) is lower than those after 2019 (r = 0.48, p = 0.001) (see Fig. 5, Fig. 6).
However, the disaggregate results, shown in Table 5, suggest that this technological discontinuity varies from technology to technology and from performance to performance. For instance, BDA—firm performance was higher in studies before 2019 (r = 0.50) than after 2019 (r = 0.47).
Vis-à-vis the methodological moderators, our findings support the prediction that the type of performance measure (QB = 7.10, p = 0.001; B = 0.115, p = 0.05) moderates the DT/firm performance relationship, such that marketing measures (r = 0.49) yield stronger effect sizes than accounting measures (r = 0.39). Analogously, the moderating role of study design is supported (QB = 4.70, p = 0.001; B = 0.119, p = 0.031), indicating that survey studies (r = 0.49) produce stronger effect sizes than experimental study designs (r = 0.39). Likewise, we find support for the moderating role of theory use (QB = 9.74, p = 0.001; B = −0.197, p = 0.001), such that theoretical studies produce larger effect sizes (r = 0.46) than atheoretical studies (r = 0.41). In addition, our findings confirm the moderating role of study type (QB = 20.48, p = 0.001; B = −0.259, p = 0.031), showing that cross-sectional studies (r = 0.49) yield stronger effect sizes than longitudinal studies (r = 0.27). Finally, we confirm the moderating effect of sampling technique (QB = 5.16, p = 0.001; B = 0.096, p = 0.01) in the DT/firm performance relationship, such that it is larger for probability sampling (r = 0.49) than non-probability sampling (r = 0.41).
5. Discussion
Empirical research on the DT/firm performance relationship has been at the forefront of scientific inquiry in the management and innovation literature in the past three decades. However, the studies are mixed and fragmented across cultures, contexts, and disciplines, limiting our understanding of how and to what extent DTs relate to firm performance and which underlying boundary conditions (moderators) shape this focal relationship. This meta-analysis addresses this gap in the research by quantifying and calibrating the research on DTs and firm performance.
First and foremost, our findings show that DTs positively and significantly influence overall firm performance (r = 0.47), with the average strength ranging from small to moderate (r = 0.27-0.58). This threshold or effect size falls within the conventional range of other meta-analyses in the innovation and strategic management literature: ambidextrous capabilities (r = 0.06; Junni et al., 2013), strategic resources (r = 0.17; Crook et al., 2008), and general innovation (r = 0.19; Rousseau et al., 2016). Hence, our findings show that DTs can significantly enhance the competitiveness of firms despite the uncertainties and high costs involved in implementing these novel DTs in a firm's operations. This finding therefore confirms that DTs are enabling tools to help firms enhance their performance and competitiveness (e.g., Wamba et al., 2020; Erkmen, Günsel, and Altındaǧ, 2020).
Moreover, in line with the RBV of the firm and resource orchestration theory, the management's synchronization of processes and coordination of technological/digital resources and capabilities can generate a competitive advantage. While investment may be “slow to be cashed in” (Benassi et al., 2021, p. 1), DTs can integrate information and communication technologies (ICTs) within firms and transform control systems in business operations to generate superior performance benefits for organizations (e.g., Dubey et al., 2020; Liu et al., 2022). At the same time, our results disagree with studies (e.g., Dalenogare et al., 2018) that found or argued that DTs might not have a significant effect on industrial performance and that DTs negatively influence firm performance (e.g., Niaki et al., 2019), particularly financial performance. Our results show that DTs positively and significantly improve financial performance, particularly with respect to AI, BDAs, and IoT/CPS.
In addition, this study reveals that the aggregate impact of DTs on innovation performance is larger than on operational efficiency and financial performance. This indicates that firms that adopt DTs have more potential to enhance their ability to develop new technology, rethink their business model, and acquire new tools to enhance product quality through innovation than to improve their efficiency in operational activities and financial performance (profit/economic gains). This finding supports aspects of the literature that found that DTs help companies to generate superior competitive advantage with respect to differentiation and innovation support (Yu et al., 2021), develop and introduce new products and services (e.g., Blichfeldt and Faullant, 2021), and rethink their business models (Mubarik et al., 2021; Holmström et al., 2016).
One possible reason for the reduced impact of DTs on financial performance is that the outflow of firm resources and technologies may not yield immediate financial returns. Another reason is that customers may not be prepared to pay for these benefits. This explanation agrees with Gebauer et al. (2020), who observed that many digital services are offered for free. The finding corroborates prior research showing that the effects of digital innovation on efficiency and innovation—akin to the impact of acquiring new customers—“occur more quickly than changes in accounting-based performance” (Stuart, 2000, p. 801).
On the aggregate level, our findings regarding the archetypes of DTs on overall firm performance show that the impact of AI is the largest. Indeed, the results indicate that AI has the greatest impact on all three performance indicators examined: financial performance, innovation performance, and operational efficiency/performance. This means that a firm's adoption of AI applications such as analytics, dynamic pricing, process automation, and optimization and prediction can lead to increased market shares and higher revenues (Benzidia et al. (2021); Basri (2020), innovation performance (e.g., Liu et al., 2020), operational efficiency/performance in terms of relational governance, supply chain efficiency, and business customer interaction (Baabdullah et al., 2021; Dubey et al., 2020). Hence, AI can enhance a firm's business capability and value. Interestingly, we found that IoT/CPS does not significantly improve innovation performance, although it significantly influences financial and operational performance. This disagrees with Yu et al. (2016), with a possible reason that these technologies are used mainly for operational activities, not for business model innovations or to introduce new products.
Furthermore, the comparative effects of the DTs on each level of firm performance—financial performance, innovation performance, and operational efficiency—show that the impact of AI on financial performance is stronger than its impact on innovation performance and operational efficiency, which falls in line with the literature stating that AI has the potential to produce increased market shares and higher revenues (Tang et al., 2018). In contrast, BDAs exert a greater impact on innovation performance and operational performance than on financial performance, which implies that BDAs can increase firms' innovation prowess and operational efficiency rather than directly improving their financial performance. Likewise, IoT/CPS are more strongly correlated with financial performance than with operational efficiency, which means that IoT/CPS can directly increase the financial capabilities of firms rather than improving operational activities (Tang et al., 2018). Lastly, with respect to 3DP, the average effect is larger for innovation performance and operational efficiency than for financial performance, indicating that additive manufacturing can enhance innovation capabilities (e.g., business model innovation) and operational activities such as supply chain efficiency. These findings imply that the aggregate strengths of DTs are contingent on the performance outcome.
The second objective of this meta-analysis was to examine the moderating role of contingency-based factors. With regard to contextual moderators, our findings show that if the size of a firm is small, the impact of DTs is stronger, indicating that small firms benefit more from DTs technologies than larger counterparts. This confirms strands of the literature that potential gains from the deployment of DTs may be larger for small firms since they can mitigate traditional burdens associated with size and the lack of ability to commit human and financial resources (Cassetta et al., 2020). It could also be because small firms are more flexible and adaptable than larger firms when implementing DTs and have a greater need for efficiency. This finding therefore shows that it is not so much about the number of technological resources and capabilities that a firm has, but how well the firm is able to synchronize and coordinate actions to deliver on the strategy. Therefore, smaller firms, with their scarce resources, can invest in DTs and gain operationally and financially.
Furthermore, we found support for the moderating role of the particular industry sector, confirming previous studies (e.g., Liang et al., 2010; Kohli and Devaraj, 2003) that showed that industry sector can influence the DT/firm performance relationship, a finding advocated by the dynamic capabilities theory that a firm's ability to reconfigure and integrate resources and capabilities may vary by industry. As observed, and contrary to our expectation, service firms stand to benefit more from DTs than manufacturing firms, which indicates that service firms are more stable with respect to economic cycles and less sensitive to price competition than manufacturing firms (Garay and Font, 2012).
Moreover, the findings show that the technological intensity of the sector impacts DT/firm performance relationships, such that low-tech firms benefit more from DTs than high-tech firms. This finding agrees with studies (e.g., Santoro et al., 2020) that show that low-tech firms mostly enjoy constant periods of stability from IT investment, flexible product updates, and high homogeneity, thereby disagreeing with previous studies (e.g., Mubarik et al., 2021). Low-tech firms are more likely to invest in DTs since they have a stronger need for operational efficiency and business model innovations. In addition, our meta-analysis does support the moderating role of firm age, revealing that the performance effects of DTs are stronger for start-ups firms than for mature firms (Zhu et al., 2006). One possible reason is that structural inertia associated with mature firms may slow down their adoption of DTs, whereas younger firms (start-ups) demand less coordination, communication, and influence to support and implement such technologies (Zhu et al., 2006). Thus, we disconfirm the literature stating that DTs performance effects are more effective among mature firms than young/start-up firms (Cirillo et al., 2021; Tambe, 2014).
Likewise, we found that country development moderates the DT/firm performance nexus, such that emerging/developing economies benefit more from DTs than advanced economies. This means that developed countries may have reached maturity in the innovation cycle while emerging economies are now more innovative and welcoming of digital economies, hence more effective and efficient than developed economies (Feng et al., 2020). It is also possible that emerging countries may be at the initial or growth stage of implementing DTs, with a greater urge for operational efficiency and flexibility.
Lastly, drawing on the technological discontinuity theory to underscore that the DT/firm performance relationship changes over time, we observed that studies conducted after 2019 (during and after the COVID pandemic) produced larger effect sizes than those carried out before 2019. While we agree with the literature that COVID-19 forced companies to adopt DTs in their operations (Li et al., 2022; McKinsey, 2020), leading to enhanced performance (Papadopoulos et al., 2020), it should be noted that this depends on the DT and performance indicator in question. For example, while we observed that the impact of AI, IoT/CPS, and 3DP was higher after 2019, the opposite is true of BDA. Likewise, while the impact of DTs on innovation performance and operational efficiency was higher in the period after 2019, there is no significant difference for financial performance.
As for methodological factors, we found that type of performance measure, study design, theory use, sampling technique, and study type account for between-study variance. With regard to the type of performance measure, we discovered generally weaker effects in studies using accounting measures (e.g., ROA, ROE, and ROI) compared to those using marketing-based measures, thereby disagreeing with Allouche and Laroche (2005) and agreeing with Gullardo-Vázquez et al. (2019). In relation to study design, we found that surveys produce larger effect sizes than experimental studies (Wang and Yang, 2008; Peterson and Brown, 2005). Moreover, we found that theory-driven studies generate larger effect sizes than atheoretical studies. With respect to study type, we found that cross-sectional studies yield larger effect sizes than longitudinal studies, disagreeing with Cook and Ware (1983). Finally, in relation to sampling technique, the findings show that probability sampling techniques produce larger effect sizes than non-probability sampling, thus, concluding that sampling technique does account for between-study variance (Fern and Monroe, 1996).
6. Theoretical and managerial implications
6.1. Theoretical implications
From a theoretical perspective, we make numerous critical contributions. First, this is one of the first studies to employ a meta-analysis to examine the relationship between DTs and firm performance in this research stream. Therefore, we contribute to digital innovation research by providing a comprehensive understanding of how and the degree to which DTs impact firm performance.
Second, our analysis is grounded in well-established theoretical frameworks—namely, RBV of the firm, resource orchestration theory, and contingency theory, which propose the interplay between resources, capabilities, managerial acumen, environment, and firm performance—and therefore allows for greater comparability and generalizability of the phenomenon. Our study advances and extends these theories to the digital transformation domain by explaining how DTs account for differences in firm performance in different contexts and times. By decoupling the individual effects of DTs on the different performance outcomes—financial, innovation, and operational—three new insights are revealed: (1) financial performance relates more to AI and IoT/CPS; (2) operational performance relates more to BDAs and 3DP; (3) innovation performance relates more to AI and BDAs. By appropriately accounting for different types of DTs and different dimensions of firm performance, our article improves the accuracy of estimates and offers a comprehensive view of firms’ digitalization behavior.
Third, an open question in the digital innovation literature is whether DTs pay off. By combining multiple studies, our study provides a clear answer to this by clarifying the anecdotal and fragmented findings in the field. Thus, we introduce an organizational framework that integrates the various types of DTs and performance dimensions, thereby substantially enriching and extending theories on DTs and firm performance. Our results highlight the importance of DTs on firm competitiveness and superior performance, demonstrating that digitalization of business processes and strategies can pay off, that is, create revenue for firms by orchestrating a firm's innovation and operational activities through managerial acumen in coordinating and synchronizing IT resources and firm processes.
Fourth, by investigating a broad range of conceptual and methodological moderators, our article motivates a shift from questioning the DT/firm performance nexus to examining the role of moderating contingencies in ascertaining the nexus. Therefore, our study advances a contingency-based model that facilitates the prediction of a firm's attitude toward DTs and answers recent calls to explore the relevance of boundary-spanning factors to predict the consequences of DTs on firm outcomes and behavior.
Finally, our study contributes to the methodological rigor of digital innovation research by employing a meta-analysis using MARA and subgroup analyses, since most of “the research is still theoretical or at most qualitative” (Rialti et al., 2019, p. 2).
6.2. Managerial implications
From a managerial perspective, we advance the following relevant insights for management practice. First, both small and large, high-tech and low-tech companies, are motivated to “digitize” and “digitalize” their business strategies by implementing DTs in their business processes and innovation strategies because they positively and significantly influence overall firm performance, as well as financial, innovation, and operational efficiency/performance. Indeed, the overall average effect (r = 0.47) is considered for practical purposes (Cohen, 1988). In particular, the results reveal that the three most effective DTs include AI (r = 0.58) followed by BDAs (r = 0.49), and IoT/CPS (r = 0.40), in that order. This insight can help managers in their strategic orientation and resource allocation.
More specifically, these results show managers the most effective and efficient DTs to adopt based on specific objectives. For example, our findings reveal that the relevant DTs for enhancing financial performance are AI, IoT/CPS, and BDAs, in that order. However, for innovation performance, the order is AI, BDAs, and IoT/CPS, and for operational efficiency/performance, the order is AI, BDAs, and 3DP. We believe this finding provides vital information for managers in terms of their scale of preference for DTs in relation to the strategic objectives of the firm at any given point in time.
In addition to the expected mean effect (r = 0.47), the findings show that the relative strength of DTs on firm performance varies from technology to technology (e.g., AI > BDAs > IoT/CPS), a fact that has key implications for practitioners in terms of their strategic orientations and resource allocation. Therefore, the adoption of DTs should be tailored to the specific objectives. Moreover, our aggregate results offer practitioners more valid and reliable insights about the expected mean effect (r = 0.47) of DTs on firm performance amid the vast array of conflicting and disparate results in the literature, coupled with the uncertainty surrounding the adoption of these DTs. Thus, despite the high degree of uncertainty surrounding the future impact of these technologies, the findings of the study, which amalgamate effect sizes across multiple studies, indicate that DTs investment is worthwhile, both financially and non-financially (innovation and operational efficiency).
It is worth noting, however, that because the average strength ranges from small to medium, business executives, innovation managers, and practitioners should not consider DTs as short- and medium-term strategies but rather as long-term innovation strategies. In other words, managers should invest in DTs with the awareness that the return on investment may not necessarily come in the short term, although such DTs can improve efficiency in operational processes, supply chain networks, and other firm activities. To this end, we suggest that managers take a long-term rather than short-term view in their DTs investments and deployment.
Finally, the impact of DTs on firm performance depends on the context: firm size, industry sector, firm age, sector technological intensity, and country development. Thus, managers must note that each context demands different approaches to operations management and strategic orientation. For example, the finding that the elasticity of DTs on firm performance is larger for small firms and start-ups than for large, mature enterprises informs managers of small firms and start-ups about the relevance of integrating DTs into their strategic initiatives and firm activities.
Our findings also inform governments about the need to provide financial support to small and young firms as they adopt DTs to enhance their performance, since these firms employ a large portion of the population and spark innovation. This, in turn, will at least reduce the “widening gap” between large and small firms and between mature firms and start-ups in terms of their lack of resources and capabilities required to fully embrace novel DTs. This being said, to realize gains from DT investments, managers must integrate their digital with business strategies; have a committed digital CEO; adopt and implement agile governance; possess a talent core of digital supporters; develop business-led, flexible digital and technology platforms; and introduce effective and efficient monitoring and evaluation mechanisms to monitor progress toward a defined strategic goal.
7. Limitations and recommendations for future research
Any meta-analytic review synthesizing and calibrating the results of primary studies has its limitations, which also serve as a guide for future research, both in addressing what is already known and discovering potential avenues for future research. First, we made significant efforts to reduce selection bias and optimize our search by exploring all relevant business and management databases to identify the most frequently and quantitatively examined DTs—artificial intelligence, big data analytics, IoT/CPS, and 3DP articles. However, our search may have excluded other relevant articles, since we only included articles published in English and those with the full text online. The interpretation of the study results should therefore consider this limitation, although our publication bias analysis did not find a threat of bias.
Moreover, our meta-analysis focused on only the most frequently and quantitatively examined DTs—AI, BDAs, IoT/CPS, and 3DP. However, other DTs are available, such as data mining, cybersecurity, digital forensics, e-mobile networks, remote sensing, RFID in smart cities, interoperability, computer modeling, simulation, etc. Therefore, we encourage future research to build a more extensive database to include articles examining these DTs with respect to firm performance when quantitative data allows for such analysis. There may also be complementarities between the DTs to consider in future studies—for instance, if companies can draw on their knowledge in data management and storage to master big data, for example, or if they use data collected by IoT/CPS in big data applications. This would imply that the effects of individual technologies on performance are overestimated because the estimate does not account for this cross-fertilization effect. We also recommend that future studies parse the DT analysis into internally-oriented capabilities and optimization of processes/production and/or externally-oriented digital features of products and/or services to allow for sub-group analysis and hypothesis formulation in future meta-analysis.
Furthermore, while we made several robustness checks to test potential bias in the effect sizes, future studies could employ more sophisticated methodological advances such as multilevel modeling to decompose and investigate between-study variance (heterogeneity) (Cheung, 2014). Moreover, we took significant steps in the primary studies to minimize the problem of p-hacking (i.e., when “findings are selectively published to confirm hypotheses” (Brinkerink, 2023) by incorporating studies from different contexts and disciplines and cultures, but we nevertheless call for future studies to further explore this issue.
In addition, our meta-analysis does not account for a causal relationship by examining mediators in the direct relationship between DTs and firm performance. Therefore, a meta-analytic review that adopts meta-analysis structuring equation modeling (MASEM) is warranted to investigate mediating factors such as innovation ambidexterity, agility, supply chain management capability, intellectual capital, knowledge management, etc., when sufficient data is available. Relatedly, it is observed that most existing studies on DTs are qualitatively manipulated, which could not be accounted for in this study since they do not provide sufficient statistical data to allow for quantitative integration. Thus, it would be interesting for future scholars to adopt qualitative meta-analysis (QMA) (Combs et al., 2019) to address such studies.
Moreover, we explored some contextual and methodological factors, but our examination is not exhaustive. For instance, we did not examine other factors such as plant size, level of adoption, national culture, dynamism, type of firm (profit vs. nonprofit), nature of business (B2B vs. B2C), etc. Therefore, investigating some of these factors in a further study is necessary to explain the cause of differences in the results across the field.
Furthermore, we recommend that future studies employ a probability sampling technique that is more representative of the population to minimize random error. There also appears to be an over-concentration of research on large enterprises and mature firms with respect to small enterprises and start-ups. As start-ups and small enterprises embrace DTs, however, future research should also pay attention to these contexts to broaden our understanding of the cost and gains of DTs to these enterprises.
Finally, since most studies are cross-sectional, longitudinal research is suggested to examine the impact of the DTs prior to, during, and after implementation of DTs, particularly given the reality that returns on investment from these technologies may not be forthcoming in the short and medium term but rather in the long run.
Despite these limitations, we believe that our study design makes significant contributions to the digital innovation literature because it is the “best method to reach consensus” (Combs et al., 2011, p. 194) when empirical results are mixed and fragmented across disciplines, cultures, and contexts.",https://www.sciencedirect.com/science/article/pii/S0166497223001475
Augmenting organizational Change and Strategy Activities: Leveraging Generative Artificial Intelligence,Journal of Applied Behavioral Science,"Rouven Kanitz , Katerina Gonzalez , Roman Briker, Tammo Straatmann
","ESSAY, we discuss the potential for generative artificial intelligence (GAI) to shape the work of organizational change, development, or strategy implementation professionals. Using a case example of a culture change initiative, we illustrate how practitioners can benefit from using GAI tools to augment relevant change activities in planning initiative content and roll-out, mobilizing stakeholders, and monitoring initiative progress. We conclude with a reflection on the limitations of GAI systems and outline directions for future research related to (1) stakeholder responses to GAI, (2) GAI impact on the nature of change work, and (3) GAI value creation for change work and unintended consequences of GAI use.",2023,"In this scoping essay, we discuss the potential for generative artificial intelligence (GAI) to shape the work of organizational change, development, or strategy imple- mentation professionals. Using a case example of a culture change initiative, we illustrate how practitioners can benefit from using GAI tools to augment relevant change activities in planning initiative content and roll-out, mobilizing stakeholders, and monitoring initia- tive progress. We conclude with a reflection on the limitations of GAI systems and out- line directions for future research related to (1) stakeholder responses to GAI, (2) GAI impact on the nature of change work, and (3) GAI value creation for change work and unintended consequences of GAI use.
Keywords
change management, digital technology, artificial intelligence, strategy implementation
Organizational change and strategy scholars (Kanitz & Gonzalez, 2021; Weiser et al., 2020) as well as practitioners (DiLeonardo et al., 2020; Jick & Sturtevant, 2017) have become increasingly interested in how digital technologies provide new opportunities and risks for managing change. A digital technology with much recent attention is
1Erasmus University Rotterdam, Rotterdam, The Netherlands 2Suffolk University, Boston MA, USA 3University of Maastricht, Maastricht, The Netherlands 4University of Osnabrueck, Osnabrueck, Germany
Corresponding Author:
Rouven Kanitz, Erasmus University Rotterdam, Burgemeester Oudlaan 50, 3062 PA, Rotterdam, The Netherlands. Email: kanitz@rsm.nl
346 The Journal of Applied Behavioral Science 59(3)
generative artificial intelligence (GAI), which refers to computer-assisted systems that can generate text, images, audio, or videos (Pataranutaporn et al., 2021). Recent media attention on OpenAI’s ChatGPT (Sundar, 2023) or Google’s Bard (Metz & Grant, 2023) has put the spotlight on GAI systems, which function using large language models. Due to advancements in machine learning and natural language processing (Vaswani et al., 2017), these large language models can produce texts such as essays, poems, or lines of code in a human-like fashion within seconds. Scholars and practition- ers predict that such GAI systems will shape the way humans approach innovation, problem-solving, and content creation in organizations going forward (Bouschery et al., 2023).
In this future scoping essay, we claim that GAI systems also hold great potential to shape the work of organizational change, development, or strategy implementation professionals. Using a case example (i.e., leveraging ChatGPT to enhance a culture change initiative), we aim to illustrate where current GAI tools can augment change management activities. We end with a reflection on the limitations of GAI tools and outline directions for future research.
Using GAI Tools for Organizational Development, Change and Strategy Activities
Organizations need to adapt to dynamic environments to survive and thrive. Yet, suc- cessfully realizing change, and in particular radical-transformational change, is often challenging to navigate (e.g., Huy et al., 2014). Researchers have argued that one reason that changes fail to meet expectations and get people on board is the transition process itself (Ford et al., 2008; Stouten et al., 2018). Planning and implementation of change initiatives is a resource-intense and time-consuming process that needs to be tailored to a workforce with heterogeneous needs and attitudes—often with some embracing, others ambivalent, and some rejecting the initiative. It is against this backdrop that GAI systems can yield critical improvements to augment change activities.
To illustrate the opportunities and limitations of GAI tools, we utilize OpenAI’s Chat GPT-3 (Jan 30, 2023 Version) to augment typical change activities. We illustrate the examples using the context of a culture change initiative in a technology manufacturer— here labeled as TechCorp—building on field research of the first author (Kanitz et al., 2022). We showcase this type of initiative because it is familiar to change scholars (Schein, 1985) and practitioners (Katzenbach et al., 2012). Moreover, shaping an orga- nization’s culture is a multifaceted process that is particularly challenging (Walker & Soule, 2017). Hence, we aim to elucidate how practitioners may benefit from using GAI tools to augment three activities common to most change management models (see, Stouten et al., 2018, for a review): (1) planning initiative content and roll-out, (2) mobilizing stakeholders, and (3) monitoring initiative progress. Table 1 summarizes examples of the application of GAI in these areas, limitations, and directions for future research.
Kanitz et al. 347 Table 1. Challenges, Examples, Limitations, and Future Research on Generative Artificial
Intelligence (GAI) Use in Change and Strategy Activities. 1. Planning initiative
3. Monitoring initiative progress
Need for real-time analysis of data and reporting of initiative progress (e.g., often no timely monitoring)
Example 3.1: Automate analysis of feedback: Inspire content development
Prompt GAI to summarize stakeholder feedback from text and evaluate sentiment
Example 3.2: Develop metrics. Prompt GAI to suggest appropriate metrics for progress, as well as how and when to measure
Change activity
Current general challenges
GAI- augmenting examples
content and roll-out
Need for creative content and roll-out development (e.g., often generic value statements are used)
Example 1.1: Inspire content development. Prompt GAI to generate content to manage and convey the change initiative
Example 1.2: Draft roll-out plan. Prompt GAI to design a roll-out plan with specific activities to get inspiration for own activities
2. Mobilizing stakeholders
Need for efficient but customized communication with recipients (e.g., often one-size-fits-all approaches)
Example 2.1: Customize communication: Inspire content development Prompt GAI to write customized versions of communication materials for different target groups
Example 2.2: Automate training. Prompt GAI to conduct a role-play to train and empower leaders to address employee concerns and reduce resistance
Limitations
Open issues and future research themes
• Specificity of provided suggestions: Vague and generic suggestions (i.e., not sufficiently problem- and/or context-specific)
• Transparencyofcontentgenerationandethicalissues:Intransparentoreven false (i.e., misattributed, or made-up) sources may lead to questionable suggestions
• Stakeholder responses to GAI: How will stakeholders (e.g., employees, top managers) respond to GAI as a change actor and GAI-generated content? For which GAI-augmented activities will they show increased or decreased aversion?
• GAIimpactonthenatureofchangework:HowwillGAItoolsshapethework of change professionals (e.g., skills, tasks) and widely used change methods (e.g., large group interventions)?
• GAIvaluecreationforchangeandunintendedconsequences:Howcantheuse of GAI create value for different change activities (e.g., can automated monitoring enhance change decision quality)? What might be unintended consequences of integrating GAI?
348 The Journal of Applied Behavioral Science 59(3) GAI use for Planning Initiative Content and Roll-out
Planning content and roll-out activities for culture change initiatives are resource-intensive processes that require time, manpower, and creativity. GAI tools are useful to foster creative processes in an efficient way (Bouschery et al., 2023) because of their ability to generate content from various sources, thereby broadening the possible solution set swiftly.
Example 1.1 Inspire Content Development. To illustrate this, we assume that man- agers at TechCorp have agreed on three themes (e.g., passion for computer chips, life- long learning, and active listening) that will serve as a guiding value for the culture change, but face the challenge of articulating these themes in effective ways to employ- ees. Indeed, managers tend to over-utilize value-laden rhetoric (i.e., use of too many values, see Carton et al., 2015) and formulate values that are too generic (Lencioni, 2002), thus undermining effective communication. We argue that managers can use the GAI to overcome these pitfalls and improve communication in this regard. Table 2 illustrates ChatGPT’s response to a prompt for generating content that may inspire how to communicate more effectively.
Example 1.2 Change Roll-Out Plan. GAIs can also be useful for generating ideas related to the roll-out of the initiative. The associated activities need to create aware- ness of the new values and motivate employees to support the initiative. Often, however, culture initiatives may be seen as hypocritical because espoused values are deemed as incongruent with the everyday experience of employees related to, for example, how top leaders actually behave (e.g., “the leadership gap”, Bourne & Jenkins, 2013). At worst, such incongruency can lead to change cynicism and resis- tance (Stanley et al., 2005). Hence, managers may utilize GAI for inspiration on how to convey values with more clarity, suitability, and sincerity. As displayed in Table 3, we asked ChatGPT-3 to provide a roll-out plan for TechCorp.
GAI use for Mobilizing Stakeholders
Another important activity where GAI can make a difference relates to mobilizing stakeholders to engage with and support the culture initiative.
Example 2.1. Customizing Communication Materials. One critical lever that man- agers can use to mobilize stakeholders is the way that messages are framed. However, generating communication materials can be time-consuming, especially when tailoring the material to different stakeholder groups. For instance, although displaying emotions can increase change communication effectiveness (e.g., Venus et al., 2013), writing emotion-evoking messages for different change recipients is often challenging. GAI tools can assist with producing such difficult-to-write materials. In the example below, we build on research showing that framing a change as providing gains (promotion focus) is more effective when accompanied by a display of enthusiasm, whereas framing change as preventing losses (prevention focus) is more effective when accom- panied by displaying agitation (Venus et al., 2013). Hence, we instructed ChatGPT to draft two versions of change messages to employees (see Table 4)—one that displays
Kanitz et al. 349 Table 2. Example 1.1 on Planning.
User Prompt:
ChatGPT Output:
We are running an organizational culture change initiative and have defined the three themes as important for our future cultural development. Please provide a catchy title for each of the themes, add a real, fitting, inspiring quote (e.g., from movies, songs, politics, or history) that helps to excite people for the theme, and provide two behavioral principles for each theme to guide employee behavior. Please be specific, creative, and use imagery language. The three themes are passion for computer chips, lifelong learning, and active listening. 1. “Ignite Your Drive: Unleashing Passion for Computer Chips”
Quote:“Theonlylimittoourrealizationoftomorrowwillbeour doubts of today.”—Franklin D. Roosevelt 
Behavioralprinciples: 
Embraceagrowthmindsetandcontinuouslystrivetoexpandyour  knowledge and skills in the computer chip industry. 
Showarelentlesscommitmenttoexcellenceandbringyourfull  passion and energy to every project, inspiring others to do the same. 2. “Soar to New Heights: Embarking on a Journey of Lifelong Learning” 
Quote:“Educationisnotthefillingofapail,butthelightingofafire.”— William Butler Yeats 
Behavioralprinciples: 
Seekoutopportunitiesforpersonalandprofessionaldevelopment  and embrace new challenges with a curious and open mind. 
Shareyourknowledgeandskillswithothersandencourage  collaboration, continuously striving for improvement. 3. “Tune In, Connect: Cultivating Active Listening” 
Quote:“Themostbasicofallhumanneedsistheneedtounderstand and be understood.”—Ralph Nichols 
Behavioralprinciples: 
Practiceempathyandactivelyseektounderstandtheperspectives,  emotions, and needs of others. 
Respondwithopennessandrespect,buildingtrustandcollaboration  through active, engaged listening. 
enthusiasm and is framed with a promotion focus and another that displays agitation and is framed with a prevention focus.
Example 2.2: Train Leaders to Address Employee Concerns and Resistance.
Preparing leaders in the organization to communicate and promote a change ini- tiative through training programs is key to the success of such an initiative (Stouten et al., 2018). Leaders often face different local conditions and diverse employee groups with various interests. These complex employee preferences are not usually fully addressed in standardized change training programs, often leaving leaders unprepared to navigate these challenges. Hence, allowing leaders to utilize GAI systems for role-play training, for example, by simulating how to deal with skeptical employees (see Table 5) can be a cost-efficient way of supporting leaders during implementation.
350 The Journal of Applied Behavioral Science 59(3) Table 3. Example 1.2 on Planning.
User Prompt:
We are now planning the roll-out of the culture change initiative. Could you please develop a plan for how we can bring these values to life? The organization has 10,000 employees, is a for-profit manufacturer of business-to-business technology and located in Western Europe. A central attribute of the organization is a strong engineering focus with an emphasis on quality and technological excellence. Please consider a time frame of 12 months, be specific in the suggested steps and activities, make sure they address behavioral artifacts, and be creative (e.g., use inspiration from art, pop culture, or social movements) such that we can create awareness and desire for the initiative. Some of the activities should specifically address the themes. Please provide sources for your recommendations.
Sure, here’s a plan for how you could bring these values to life over the course of 12 months: • Month 1–2: Launch and Awareness
Launch an internal campaign to promote the culture change initiative. Create a catchy slogan that represents the three themes, design posters, email signatures, and social media posts with the slogan and the three themes. Develop a culture change microsite or intranet page to provide a central hub for information and resources related to the initiative. Use this as a platform to share progress updates, success stories, and resources for learning and development.
Host a company-wide kick-off event to introduce the initiative and highlight the importance of each theme. Involve senior leadership and employees who embody the values and share their stories.
• Month 3–6: Passion for Computer Chips Organize hackathons, engineering competitions or code sprints to encourage innovation, creativity, and the pursuit of excellence in technology. Ensure these activities are accessible to all teams, including those who are not directly involved in tech development. This will help create a sense of collective ownership for technological excellence across the organization. Invite guest speakers from industry experts or leaders in the technology field to speak to the employees about the latest trends and future developments. The talks could be focused on the latest technological advancements, the future of artificial intelligence (AI) or cybersecurity, or anything relevant to the company’s products. Recognize and reward employees who demonstrate passion for computer chips in their work. Create an award to recognize individuals or teams who go above and beyond in their work related to this theme.
• Month 7–9: Lifelong Learning Offer a wide range of training and development opportunities for employees to enhance their skills and knowledge. These could include online courses, workshops, and coaching sessions. Make these opportunities easily accessible and encourage employees to take advantage of them. Establish mentorship programs to help employees learn from one another and grow professionally. Pair up experienced employees with those who are new to the company or new to the field, and create a structured program to guide their interactions.
(continued)
ChatGPT Output:
Kanitz et al. 351 Table 3. (continued)
Provide opportunities for employees to share their knowledge and expertise with others through presentations or knowledge-sharing sessions. This will help create a culture of learning and development across the organization.
• Month 10–12: Active Listening Introduce a regular employee feedback program to collect feedback and ensure their voices are heard. Use anonymous surveys, suggestion boxes, or town hall meetings to gather feedback and ideas. Develop communication training programs that focus on active listening and empathy. Host workshops or webinars to teach employees how to listen actively, communicate effectively, and practice empathy. Recognize and reward employees who demonstrate active listening in their work. Create an award to recognize individuals or teams who are particularly effective at listening and responding to others. Sources:
HarvardBusinessReview:HowtoMakeYourCultureWorkwithWhat  You’ve Got 
Forbes:FourStepsToBringACultureChangeInitiativeToLifeInYour  Organization 
McKinsey&Company:Cultureforadigitalage 
TEDTalk:TheartofactivelisteningbyLeslieFine  GAI use for Monitoring Potential Stakeholder Issues  Change analytics and data-driven change management have received increasing atten- tion in practice over recent years (e.g., Wolf et al., 2023). GAI tools can be especially useful for facilitating data analysis and enabling the monitoring of a change initiative’s progress.  Example 3.1 Automate Analysis of Stakeholder Feedback. One common monitor- ing activity is to analyze stakeholder feedback. This feedback can be used to understand stakeholder attitudes, feeding into the development of interventions. One way to monitor progress is by using change readiness surveys with standardized questions and rating scales. To gain even deeper insights, organizations may choose to rely on qualitative data (e.g., unstructured text). However, analyzing such large, unstructured data (e.g., open response comments from thousands of employees) at a large scale is costly and requires at least basic data skills and the use of analytics software. By utilizing GAI, even managers without such skills can generate insights quickly and cost-efficiently. In Table 6, we dem- onstrate how GPT can help to extract topics and evaluate the sentiment of real open-ended employee responses collected in a survey of a culture change initiative (see Kanitz et al., 2022). We limited the input to 10 employee responses to ease the judgment of the accuracy of the extracted themes. However, this capability becomes particularly useful as the amount of text increases, and it becomes more unwieldy for humans to extract themes.  Example 3.2 Developing Metrices for Monitoring Change Progress. Another monitoring challenge is to define appropriate performance indicators that allow 
352 The Journal of Applied Behavioral Science 59(3) Table 4. Example 2.1 on Mobilizing.
User Prompt:
To increase awareness and motivation to support the culture change initiative, we would like to write a short newsletter introducing the three values. Please write a draft of the newsletter email and structure it as follows: start with introducing the culture change initiative and why it is needed, then shortly state and elaborate on the three cultural themes from above, and close with a call for support of the initiative. Please create two versions of the newsletter. The first version should display enthusiasm and be written with a promotion focus (the motivation to achieve gains). The second version should display agitation and be written with a prevention focus (the motivation to avoid loss). Here is the draft for the newsletter mail: Version 1: Enthusiasm and promotion focus Subject: Exciting news! Join us in shaping our culture
Dear [Colleagues], We are thrilled to announce our new organizational culture change initiative! As we look toward the future, we are committed to building a strong and vibrant culture that reflects our values and drives our success. This is a unique opportunity for us to come together and shape the way we work, learn, and grow as a team. Our culture is rooted in three core themes that are critical to our success:
Passionforcomputerchips:Webelieveinadeepandunwavering commitment to our work and our customers. We strive to bring energy, creativity, and excellence to everything we do. 
Lifelonglearning:Webelieveincontinuousimprovementand never-ending growth. We embrace challenges, seek new opportunities, and embrace change. 
Activelistening:Webelieveinempathy,respect,andopen communication. We value diverse perspectives and seek to understand the needs and goals of others.  We need your help to bring these themes to life! Let’s work together to create a culture that inspires and motivates us to do our best work. Get involved in our culture initiatives, attend our events, and share your ideas and feedback. Together, we can shape the future of our organization and drive our success for years to come. Join the culture change movement today!  Best regards, [Your Name]  Version 2: Agitation and prevention focus  Subject: Urgent action needed! Protecting our culture  Dear [Colleagues], Our organization is facing a critical challenge. Our culture, which has always been our greatest strength, is under threat. We are at risk of losing the things that make us unique and successful: our passion for computer chips, our commitment to lifelong learning, and our focus on active listening. If we don’t  (continued) 
ChatGPT Output:
Kanitz et al. 353 Table 4. (continued)
take action now, we risk losing the foundation of our success. We risk becoming just like any other organization, losing our competitive edge, and our ability to deliver exceptional results for our customers. This is why we are launching a new organizational culture change initiative. We need your help to protect our culture and keep it strong and vibrant. Join us in shaping our culture, by embracing the following core themes:
Passionforcomputerchips:Wemustneverlosesightoftheimportanceof  our work and our customers. We must remain focused, creative, and  excellent in everything we do. 
Lifelonglearning:Wemustcontinuetogrow,improve,andembrace  change. We must never become complacent or stop seeking new  opportunities. 
Activelistening:Wemustremainempathetic,respectful,andopen.We  must continue to value diverse perspectives and understand the needs and  goals of others. We cannot afford to wait. We need your support now more than ever. Get involved in our culture initiatives, attend our events, and share your ideas and feedback. Together, we can protect our culture and secure our success for the future. Join the culture change movement today!  Best regards, [Your Name]  tracking of the progress of a change initiative. This challenge becomes particularly salient in the context of culture initiatives that target intangible aspects (e.g., adoption of new values) which are more difficult to measure and quantify. Hence, managers can benefit from a GAI’s support for developing metrics to assess the progress of the ini- tiative. Hence, we prompted ChatGPT to provide suggestions on what, how, and when to measure the progress of a culture change initiative at TechCorp (see Table 7).  Reflection on Limitations of Exemplary GAI use Cases  The examples above showcase how the use of GAI tools (i.e., ChatGPT) can augment the work of organizational change, development, or strategy professionals. Next, we reflect on limitations related to the presented use cases.  Specificity of the Suggestions Provided by the GAI System  A key element determining the value of GAI systems to augment the work of change pro- fessionals is the specificity of the suggestions provided by the GAI. One salient limitation is that the suggestions in our examples often appear rather vague and generic. This can be problematic as designing successful change initiatives is a highly context-sensitive activity 
354 The Journal of Applied Behavioral Science 59(3) Table 5. Example 2.2 on Mobilizing.
User Prompt:
I would like to engage in a role-play with you to practice how to deal with employees who are resistant to the culture change initiative. My name is Emily, and I am a 32-year-old team leader in the supply chain department at TechCorp. You will be playing the role of Mike, a 55-year-old engineer who has been working at TechCorp for about 18 years. You believe that the culture initiative is vague and will not make a significant difference for workers. You are skeptical and resistant to this culture change initiative, having experienced two prior culture initiatives at TechCorp that did not seem to improve things. Please get into character as Mike, the engineer who is skeptical about the culture change. When I ask questions, please provide detailed answers, and feel free to ask questions yourself. Let’s begin with the first question.”
Hi Mike, how are you? You have heard about the change initiative; you are an important contributor to our organization, and I’d like your opinion. What do you think about it? Hi Emily, I’m doing well, thanks for asking. As for the culture change initiative, I’m not convinced it’s going to make a significant difference for workers. In my experience, these kinds of initiatives tend to be vague and not well thought out. I’ve been through two prior culture initiatives at TechCorp, and neither of them seemed to improve things. So, to be honest, I’m skeptical about this one too.
That being said, I’m open to hearing more about what the initiative entails and how it will be implemented. Maybe there’s something I’m missing or a new approach that could make a difference. Can you tell me more about the specifics of the initiative and how it will be carried out?
We really want to change how employees at TechCorp behave so we can be better prepared for the future. For example, we want to increase our employees’ skills and knowledge. And we want to improve organizational commitment of everybody. On an organizational level, we as TechCorp want to become a market leader. The world economy is in decline and there are many crises—so we must change as well. You know how valuable you are to us, Mike. What can I do to convince you?
I appreciate your honesty and concern, Emily. I understand the need for TechCorp to adapt and become a market leader in the current economic climate. However, I still have some reservations about the effectiveness of this initiative based on my prior experiences with similar initiatives. To convince me, it would be helpful to have more specific details about the initiative and how it will address the issues we face. It’s important to me that the initiative is well-planned, clearly communicated, and has tangible goals that are achievable. I’d like to see how the initiative aligns with our company’s vision and mission, and how it will help us achieve our goals.
Additionally, it would be helpful if we could have some transparency around the implementation process, including who will be leading the initiative and what resources will be provided to support it. I think it’s important that we involve all employees in the process, and provide opportunities for feedback and input.
(continued)
ChatGPT Output:
User Prompt:
ChatGPT Output:
Kanitz et al. 355 Table 5. (continued)
User Prompt:
Ultimately, I want to see the initiative succeed and make a positive impact on our company, and I’m willing to work with you to make that happen. So, let’s discuss further and see how we can move forward together. We are not 100% there yet. And of course, we are flexible to integrate the feedback of critical stakeholders like you. To be more specific, we want to increase sales, receive better customer ratings, reduce turnover, and improve our employer ratings (e.g., on Glassdoor). You will all work for an employer that cares more about their employees as before and shows excellence across the board. The first phase of the initiative will take 12 months and we will have a highly-esteemed change consultancy on board to smoothen the process and get evidence-based support. We will use a substantial amount of money and time to get this done and make sure this resonates with our values (meaning passion for computer chips, lifelong learning, and active listening) but also is fully transparent for all employees at TechCorp. How does that sound? You are leading a big team and you know how it is. If you are against it, they will be to. Can I count you in?
Thank you for sharing those specific goals and details about the initiative, Emily. I can see how this initiative could potentially benefit TechCorp in various ways, such as improving customer satisfaction and reducing employee turnover. It’s also reassuring to hear that a change consultancy will be involved to support the process and that transparency will be a priority. As a team leader, I understand the importance of getting my team on board with any changes and initiatives that we undertake. While I do have some initial reservations, I am open to giving this initiative a chance and seeing how it progresses. However, I also want to ensure that my team members have a clear understanding of the initiative and how it aligns with our goals and values as a company.
Therefore, I would like to learn more about how the initiative will be communicated to employees and what opportunities there will be for feedback and input. It’s important to me that my team members feel heard and involved in the process, and that we work together to make the initiative a success. So, while I can’t commit to being fully on board just yet, I am willing to learn more and provide feedback along the way.
ChatGPT Output:
(e.g., tailored to the urgency of the situation, the capability of managers, etc., see Hailey & Balogun, 2002). For instance, elements in Example 1.2 (roll-out plan) or in Example 3.2 (metrics) would be more helpful if specifically tailored to the problem in question. Hence, the generated suggestions are a useful starting point—but certainly need further develop- ment by humans with expertise in change management and a deep understanding of the organization, its history, and its employees.
Transparency of Content Generation and Ethical Issues
Another main limitation in the presented examples is the lack of transparency of the sources which have been used for generating the suggestions. Unless explicitly prompted
356 The Journal of Applied Behavioral Science 59(3) Table 6. Example 3.1 on Monitoring.
User Prompt:
Below, you find open responses from employees who shared their feedback about the organizational culture initiative. Please give a rating of the valence of each of the ten responses below ranging from 1 to 5 (1 = negative 2 = somewhat negative 3 = mixed 4 = somewhat positive and 5 = positive). Moreover, please extract the most important themes and flag if a theme was rather positive or negative. Please also draft a short summary report.
1. I don’t know, I’m not sure what and how exactly employees and the company need to behave daily to ensure we are using the values correctly. As stated, some are almost a given to any job (e.g., passion, listening, learning) but others are so vague I’m not sure how exactly they should apply in this case.
2. The Values reflect what was already being practiced and are not an innovation. I believe that I and most of my colleagues work to most of the values most of the time without having them spelled out to us. So I don’t feel they add anything to support my work.
3. As mentioned above I see no great impact from the TechCorp Values in my daily work. Most of the values are ones by which we already work. 4. The values are an excellent guiding principle, to help shape our behaviors within the wider business. Support for the initiative seems broad and strong. I like it a lot!
5. I work to the best of my ability to ensure the success of our organization. I am passionate about my job, team, and customers and would be doing what I do with or without the values. I do not think they bring much value, to be honest.
6. These values are the same as every other initiative that has been set by the company, they are great in theory and they are the values that will continue to keep us as a market leader and a forward-thinking, innovative, competitive employer of choice. However, the reality is that on the shop floor, it is output and efficiency that are the values and nothing else. Extra activities to promote the values are discouraged and frowned upon and most of the time I am not allowed any time to do anything other than produce products as quickly as I can.
7. These values back up the decisions I make, and I use them if I am challenged. Hence, I think this is a good initiative. They are also a very useful tools to start a discussion. 8. There is NO real support for the initiative. The company mainly focused on profit, efficiency, and productivity and not the health and welfare of its employees as some of these values may suggest. 9. I feel supported in as much as me adopting the values, to the extent to which I am able, as guidelines for my own personal efforts. I really appreciate the cultural initiative and the work that is going into this. I think this nicely reflects a shift in how top leaders think and what they pay attention to. Well done and please continue with this wonderful work. 10. My personal experience of these values has been quite mixed (and left me rather confused sometimes), as I have discovered that although these values may be an overall company desire, the main driver within the business is ALWAYS cost.
(continued)
Kanitz et al. 357 Table 6. (continued)
ChatGPT Output:
1. Valence: 3. Theme: Ambiguity. 2. Valence: 3. Theme: Redundancy. 3. Valence: 3. Theme: Redundancy. 4. Valence: 5. Theme: Positive impact. 5. Valence: 2. Theme: Limited value. 6. Valence: 2. Theme: Dissonance between theory and practice. 7. Valence: 4. Theme: Positive impact. 8. Valence: 1. Theme: Lack of support. 9. Valence: 5. Theme: Positive impact. 10. Valence: 2. Theme: Dissonance between theory and practice.
Summary Report: The organizational culture initiative has received mixed feedback from employees, with some expressing confusion and redundancy regarding the values, while others see them as positive guiding principles. The values are perceived as redundant and already present in employees’ day-to-day work. Some employees express concerns about the dissonance between the stated values and the actual priorities of the organization. However, there are employees who appreciate the initiative and see it as a positive shift in the company’s priorities. The most significant themes that emerge from the responses are ambiguity, limited value, dissonance between theory and practice, and positive impact. The organizational culture initiative could benefit from increased clarity and alignment with the organization’s priorities, as well as more active engagement and support from leadership.
to provide its sources, references are not provided by ChatGPT. In a similar vein, the basis for the sentiment analysis in Example 3.1 remains unclear, which is highly problematic for replication and credibility purposes. When asked to provide sources, current GAI systems may hallucinate sources that cannot be differentiated from real sources at first glance. For instance, in Example 1.2 (roll-out plan) and Example 3.2 (metrics), some of the sources are not stated correctly or do not exist at all. Another related issue is that some sources are mis- attributed by ChatGPT. For instance, the William Butler Yeats quote provided in Example 1.1 (content development) is discussed as a misattribution (Sullivan, 2013). Building change activities on potentially false, non-existing, or misattributed sources may lead to losses of credibility, potentially endangering the success of change activities.
These lapses in transparency also lend themselves to legal or ethical issues. For instance, the lack of transparency may lead to conflicts with copy and usage rights. Moreover, large language models produce output based on statistical associations of words in the underlying text to create plausible, human-like but not necessarily truthful responses. Even when GAI systems are programed to refuse discriminant prompts or filter out illegal suggestions, the responses might be problematic due to a bias in the underlying text dataset (Bender et al., 2021). For instance, the suggestions for the roll-out plan in Example 2.1 should be taken with caution because they are likely
358 The Journal of Applied Behavioral Science 59(3) Table 7. Example 3.2 on Monitoring.
User Prompt:
ChatGPT Output:
We would like to monitor the progress of the culture change initiative related to the three themes passion for computer chips, lifelong learning, and active listening. What do you think are relevant performance indicators to measure if the change is truly happening and sustainable in the long run? How can we measure them in creative ways? Please provide for each of the suggested indicators a specific example of how (which instrument, metric, or tool), when (at which point in time of the change initiative), and where (which group of organizational members) to measure it and provide an academic source in APA style for each suggestion. Sure, here are some suggestions for performance indicators to measure the progress of the culture change initiative: 1. Passion for computer chips:
Indicator:Increaseinemployeeengagementinchip-relatedprojects and initiatives 
How:Employeesurvey,pulsesurveysorfocusgroups 
When:Regularly,e.g.,everyquarteror6months 
Where:Acrossthecompany 
Source:Cameron,K.S.,&Quinn,R.E.(2011).Diagnosingandchanging  organizational culture: Based on the competing values framework.  Wiley. 2. Lifelong learning: 
Indicator:Increasedparticipationintraininganddevelopment programs 
How:Trackenrollmentandcompletionratesofemployeesintraining programs 
When: Regularly, e.g., every quarter or six months 
Where:Acrossthecompany 
Source:Taylor,S.,&Beldoch,M.(2010).TheOxfordhandbookof  organizational learning and knowledge management. Oxford University  Press. 3. Active listening: 
Indicator:Improvedcommunicationandcollaborationamong employees 
How:Employeesurveyorpulsesurvey 
When: Regularly, e.g., every quarter or six months 
Where:Acrossthecompany 
Source:Robbins,S.P.,Judge,T.A.,&DeCenzo,D.A.(2017).  Fundamentals of management. Pearson. These are just a few examples of how to measure the success of the culture change initiative. It’s important to have a robust evaluation system in place that can capture the impact of the changes, and to use multiple metrics to get a comprehensive understanding of the initiative’s effectiveness. Additionally, involving employees in the evaluation process can help to gain their buy-in and increase the chances of success. 
Kanitz et al. 359
built on cases that are overrepresented in the underlying training data (e.g., top-down driven culture initiatives in large Western for-profit organizations). However, such roll-out plans may backfire if executed without adaptation in other types of organiza- tions or (cultural) environments (Hailey & Balogun, 2002).
Taken together, it is important to keep in mind that GAI tools have general limita- tions and can produce errors or biases in their output. Therefore, it is important to exer- cise caution and critical thinking when using GAI tools to augment change activities and to always verify the accuracy and relevance of their output.
Future Research Themes on the Role of GAI in Change and Strategy Work
Soon, more sophisticated or specialized GAI systems will become available that may overcome some of the current limitations. Potentially, such GAI systems can be trained on granular datasets for a specific change activity (e.g., designing roll-out plans) or organizational context (e.g., US-based for-profit firms) and thereby generating sugges- tions of higher quality. Next, we propose interesting avenues for future research at the intersection of GAI and change management.
Stakeholder Responses to GAI
One challenge when utilizing GAI for change management concerns the psychological reactions of stakeholders (e.g., employees, leaders, board members) to GAI involve- ment. Research has documented that people often disapprove of AI involvement, espe- cially in ethical or HR domains (Bigman & Gray, 2018; Newman et al., 2020) or after seeing it err (Dietvorst et al., 2015)—both of which are likely scenarios in change ini- tiatives. At the same time, for some sensitive actions or areas (e.g., tracking of employee behavior or performance), humans feel less threatened or angry when an AI is utilized (e.g., Raveendhran & Fast, 2021).
In addition, recent research (Jago & Carroll, 2023) suggests that producers of content get more credit when they involve GAI as compared to additional human experts (e.g., in the present case, ChatGPT could be preferred over a change consultancy). Thus, it will be critical for future research to investigate how employees and other stakeholders (e.g., board, clients, or suppliers) may react in terms of satisfaction or support for the change initiative when confronted with change activities that were fully performed or aug- mented by GAI (as in Example 2.1). For instance, it would be interesting to track reactions such as the positive and negative affect of the different stakeholders in Example 2.1, after they learn that the communication was created with the support of GAI tools.
GAI Impact on the Nature of Change and Strategy Work
With the increasing availability and versatility of GAI systems, the nature of change and strategy professionals’ work is going to change as well. We need a better
360 The Journal of Applied Behavioral Science 59(3)
understanding of what constitutes a high proficiency in using GAI systems (e.g., which tools are available, how to write professional prompts, how to work with the responses?) and how this relevant skill for change professionals can be built.
In addition, the impact of GAI tools on change managers’ activities needs to be better understood. For instance, GAI could significantly shape monitoring activities by introducing tools like a chatbot that automatically answers questions about the change and uses this data input to provide real-time analysis of attitudes. GAI systems may also be incorporated into live mobilization activities such as large group interventions (i.e., “methods for involving the whole system in a change process,” Bartunek et al., 2011, p. 1), where facilitators help hundreds of people col- laborate on change initiatives. Specifically, GAI tools can help facilitators identify themes from multiple stakeholder inputs, pinpoint contentious issues, and suggest interventions on the spot to focus discussions and reach a consensus on crucial issues. Finally, research can investigate how GAI tools can be integrated with other technologies (e.g., humanoid robots or avatars) and methodologies to create more com- prehensive change management processes.
GAI Value Creation for Change and Unintended Consequences
One important area for future research concerns how GAI tools can add value to change planning, stakeholder mobilization, and progress monitoring. By automating repetitive tasks and providing advanced analytics, AI tools such as GAI can help change managers make data-driven decisions and optimize their activities in efficient and more customized ways. However, there is currently a lack of empirical evidence on the benefits and risks of using GAI, including its impact on performance, resource uti- lization (personnel and time), or employee change experience.
Other crucial aspects for future research to explore are the potential risks and unin- tended consequences of these tools and how these might be mitigated. For example, it would be interesting to investigate whether and when automated monitoring (as dem- onstrated in Example 3.1) may actually undermine the quality of change decisions due to factors such as information overload or inadequate data quality. Additionally, it is still unclear whether the opportunities created by GAI and the resources it frees up will lead to optimized management of ongoing change initiatives, or whether the use of GAI will instead accelerate the initiation of new change projects (Church & Burke, 2017), potentially further increasing the risks of change fatigue and inconsisten- cies between change projects (Kanitz et al., 2022).
Conclusion
With this future scoping essay, we aim to inspire an active debate within the change, organizational development, and strategy implementation communities about how GAI can shape how we navigate change in organizations. Because both change man- agers and recipients will almost certainly increasingly collaborate with GAI tools in the future, our interest is to initiate a discussion on the applications, opportunities, and
Kanitz et al. 361
limitations that are associated with GAI. Against this backdrop, we hope this essay can serve as a fresh impetus to stimulate critical and relevant work in change management in the new era of GAI.
",https://journals.sagepub.com/doi/pdf/10.1177/00218863231168974
The impact of artificial intelligence on the future of workforces in the US,White House,VARIOUS,US STRATEGIC PLAN ,2022,"Both the US and European Commission (EC) expressed strong interest during the US-EU Trade and Technology Council in late September 2021 in working on a joint study to assess the potential impact of artificial intelligence (AI) on our workforces. The Pittsburgh statement committed to a joint “economic study examining the impact of AI on the future of our workforces, with attention to outcomes in employment, wages, and the dispersion of labor market opportunities. Through this collaborative effort, we intend to inform approaches to AI consistent with an inclusive economic policy that ensures the benefits of technological gains are broadly shared by workers across the wage scale” (White House 2021, European Commission 2021).
b) Scope of the Report
Given the expansiveness of the possible scope of the project, this report is not designed to be exhaustive; rather, it highlights some of the most important themes for the economics of AI in a balanced manner. Because of the unique collaboration between the EC and the Council of Economic Advisers (CEA) on this work, our goal is to synthesize the perspectives of the US and European Union and academic work from both countries with a focus on implications relevant to policymakers. Our goal for this joint report is to strengthen collaboration on analysis and policy to ensure that the benefits of AI are broadly shared. The report is intended to highlight the economics behind AI-driven technological change with a particular focus on the institutional and policy decisions that will shape its future impact on the workforce.
c) Executive Summary
AI is a fast-evolving technology with great potential to make workers more productive, to make firms more efficient, and to spur innovations in new products and services. At the same time, AI can also be used to automate existing jobs and exacerbate inequality, and it can lead to discrimination against workers. While previous technological advances in automation have tended to affect “routine” tasks, AI has the potential to automate “nonroutine” tasks, exposing large new swaths of the workforce to potential disruption. The challenge for policymakers is to foster progress and innovation in AI while shielding workers and consumers from potential types of harm that could arise. Parts I and II of this report introduce AI and document its widespread adoption in the European Union and the United States; part III focuses on AI’s impact on labor; part IV contains case studies on hiring and logistics; part V concludes.
 The OECD defines an AI system thus: “an AI system is a machine-based system that is capable of influencing the environment by producing an output (predictions, recommendations, or decisions) for a given set of objectives. It uses machine and/or human-based data and inputs to (i) perceive real and/or virtual environments; (ii) abstract these perceptions into models through analysis in an automated manner (e.g., with machine learning), or manually; and (iii) use model inference to formulate options for outcomes. AI systems are designed to operate with varying levels of autonomy” (OECD 2019).
As an illustration of the practical use of AI, consider asking AI to give its own definition of AI. The following prompt was submitted to a modern, trained AI engine called Generative Pre-Trained Transformer 3 (GPT-3): “write a paragraph describing what AI is and what it can do.” The output that followed from GPT-3 was:
AI stands for artificial intelligence. AI is the result of applying cognitive science techniques to artificially create something that performs tasks that only humans can perform, like reasoning, natural communication, and problem solving. AI is important because it has the potential to help humans become better at completing cognitive tasks, and to automate tasks that are currently difficult or impossible for humans to do.
GPT-3, a modern AI on the topic of AI
This illustration hopefully highlights the transformative power of AI technology. While many previous technological advances led to the augmentation or automation of routine tasks, AI has the power to augment or automate a much wider variety of tasks that are normally thought to only be possible for humans to complete. In part III of this report, GPT-3’s thoughts on AI’s impact on the workforce are included.
b) Recent Progress on AI
The power of AI comes from its use of machine learning, a branch of computational statistics that focuses on designing algorithms that can automatically and iteratively build analytical models from new data without explicitly programming the solution. It is a tool of prediction in the statistical sense, taking information you have and using it to fill in information you do not have. As shown in Figure 1, machine learning has been a dominant focus of AI research since the 1980s. Over the last 10 years or so, the uses of machine learning as a prediction technology have grown substantially. Machine learning is now commonplace: Pandora learns how to make better music recommendations based on its users’ preferences; Google can automatically translate content into
     different languages based on translated documents found online; and Facebook predicts the responses of individuals to personalized adds to aid in the delivery of ads through its News Feed. One of the most common applications of machine learning is computer vision, or the use of computers to derive information from images and videos and is a major focus of research, reflecting its importance across a range of applications, from determining the content of images online for tagging or moderation, to enabling self-driving cars, to the retrieval of specific images or videos from databases.
Figure 1. AI research publications by topic, 1980-2021
Count of publications
Source: Microsoft Academic via OECD:AI
In the last half decade, there has been an increasing research focus on a specific subset of machine learning algorithms called neural networks. These algorithms use a combination of weights and activation functions to translate a set of data inputs into predictions for outputs, measures the “closeness” of these predictions to reality, and then adjusts the weights it uses to narrow the distance between predictions and reality. In this way, a neural network can learn as it is fed more data. Networks with more than two layers of transformation between input and output are called “deep”. These architectures can learn hierarchical abstractions, which helps them efficiently
   
characterize complex relationships.
Dean (2019) summarizes the evolution of machine learning. Key ideas and algorithms underlying machine learning have been around since the 1960s. In the late 1980s and early 1990s, there was a surge of excitement in the AI community as people realized that machine learning could solve some problems in interesting ways, with substantial advantages stemming from their ability to accept raw forms of input data, and to train algorithms to perform predictive tasks. At that time, however, computers were not powerful enough to process vast amounts of data. It was not until the past several years, after decades of computational performance improvements driven by Moore’s Law, that computers finally started to become powerful enough to allow for this approach. Further, both public and private entities now have access to large and sophisticated data sets that can be used for the development and training of AI models. The availability of data—both in the form of physical exclusivity and in the form of formal intellectual property rights—can shape both the level and direction of innovative activity. This is explored by Beraja, Yang, and Yuchtman (2022), who show that Chinese firms with access to data-rich government contracts develop substantially more commercial AI software.
Consider a few examples of the progress that has been achieved using machine learning. First, Stanford University hosted the inaugural ImageNet Challenge in 2010. The challenge is, given a “training set” set of 1.2 million color images divided into 1,000 categories, to train a model to classify new color images to those same categories. The winning teams in 2010 and 2011 used traditional coding approaches and could not achieve error rates below 25 percent. In 2012, an entrant used a deep neural network for the first time and won with an error rate of 16.4 percent. Subsequent years saw innovations in deep learning applied to the problem with a winning error rate of only 2.3 percent in 2017, significantly lower than the average human tasked with the classification exercise (Russakovsky et al. 2015).
Second, consider AlphaGo, a piece of software designed to play the ancient game Go against human players. It used neural networks and in addition to knowing the rules of Go, the model was trained both by playing against itself, as well as thousands of real amateur and professional games to learn strategies. In March 2016, AlphaGo beat the top-ranked player in the world 4 games to 1. Researchers then considered instead training the neural network by having it solely play games against itself—and the result was AlphaGo Zero. The neural network started with only random strategies and played against itself for 4.9 million games over three days. This new AI then defeated the previous version of AlphaGo by 100 games to 0.
Third, consider DALL-E, which is based on the same technology as GPT-3. DALL-E is a model trained to generate images from a text description provided by a user. It was trained on a set of 250 million text–image pairs. The result is that it can create images that it has never “seen” but that fit the text description with which it was prompted. See Figure 2 for an example of the output given
        

when prompted with “a diverse group of economists and computer scientists assisted by a brown and white dog trying to learn about AI next to a river.”
Figure 2. An Example of AI Output
Source: DALL-E These examples highlight the types of tasks that were previously thought to be impossible but now
can be executed by AI, occasionally in a superior way to what a human could do.
c) Overall Progress and Future Directions
Progress in AI since the 1950s has been characterized by periodic cycles of breakthroughs and massive investment (“AI spring”) and periods of disappointment and little funding (“AI winter”). Technological breakthroughs lead to excited declarations of expected progress, which encourages increased investment. When the research stalls, “the enthusiasm, funding, and jobs would dry up” (Mitchell 2021). The 2010s were clearly a “spring,” with advances in image processing and natural-language processing as well as greatly increased computing power. Some have suggested that AI is now in a “Golden Age.” Nevertheless, there are concerns of a “winter” on the horizon given that some goals remain elusive, such as fully autonomous vehicles (Mitchell 2021). Computer scientists and philosophers thinking about the next leap forward in AI have emphasized the feasibility of a true artificial general intelligence (AGI) that equals or exceeds human intelligence. This AGI concept has been around since the era of electromechanical computing began after World War II. The first AI conference was held at Dartmouth College in 1956. In 1965, the Nobel laureate Herbert Simon predicted that “machines will be capable, within 20 years, of doing any work a man can do.” In recent years, AGI has seen a resurgence because of the development and advances in machine learning. While AGI is not a focal aspect of this study, the economic and societal impact of machines that surpass human intelligence would be extraordinary.
   
7
While the rise of AI promises both to improve existing goods and services and to greatly increase the efficiency with which they are produced, Cockburn, Henderson, and Scott (2019) argue that machine learning
For example, Jumper et al. (2021) showed the successful use of their machine learning-based tool AlphaFold in predicting the physical structure of proteins and subsequently made available to the scientific community a database of over 200 million predicted protein shapes for researchers to use. Machine learning may be able to substantially “automate discovery” across many domains where classification and prediction tasks play an important role and may also expand the set of problems that can be feasibly
addressed.
Previous IMIs help illustrate their importance. For example, the invention of optical lenses had an important direct economic impact on applications such as spectacles. But optical lenses in the form of microscopes, invented in the 17th century, also had enormous and long-lasting indirect effects on the progress of science: by making very small objects visible for the first time, microscopes opened the field of microbiology. Today, deep learning enables us to better understand genomes, thereby progressing the fields of molecular biology and genetics.
d) Economic Opportunities and Challenges Coming from AI
As AI technology continues to improve, it may have a substantial impact on the economy with respect to productivity, growth, inequality, market power, innovation, and employment. Policymakers could also use AI to create more efficient and equitable policymaking, as described in box 1 below.
Quantifying the benefits that AI will bring is difficult both because of the uncertainty of the future evolution of AI and also because the welfare contributions of AI—embedded in the proliferation of new and free goods such as search engines, digital assistants, or social media—are not captured well in our current national accounts. To this end,
These are significant changes, especially considering that Facebook is just one product in the digital economy.
may have an even larger impact on the economy by serving as a new general-
purpose technology (GPT) that is also an “invention in the method of invention” (IMI). What sets
GPTs apart from IMIs is that IMIs that can also reshape the nature of the innovation process and
the organization of research and development (R&D) itself.

Brynjolfsson et al. (2019) propose a new metric
called GDP-B, which quantifies their benefits rather than costs. Through a series of choice
experiments, they estimate consumers’ willingness-to-pay for free digital goods and services. For
example, including the welfare gains from Facebook would have added between 0.05 and 0.11
percentage points to GDP-B growth per year in the US.
8

Box 1. Socially Optimal Applications of AI
Many policy problems facing governments require making decisions under uncertainty, but AI— combined with the large data sets to which governments have access—has given policymakers new tools to tackle that uncertainty. Kleinberg et al. (2018) use data from New York City on the decision by judges to either grant bail to a criminal defendant or require them to remain in prison. The authors find that—when compared with the decisions made by the judge—a machine learning algorithm could produce substantial welfare gains that could be allocated to either crime reduction (up to 24.7 percent) or to reduced incarceration (up to 41.9 percent), all while reducing racial disparities. However, the author’s note that while their model shows promise, actual implementation of such a prediction model as an aid to judges in their decision-making would require both a deep consideration of the objective courts are trying to achieve and how judges would actually use the model. In another case, Aiken et al. (2022) show that using machine learning algorithms and data from mobile phones improved the targeting of COVID-19 relief aid in Togo. In Togo, millions of dollars in aid were allocated to individuals during the early days of the COVID-19 pandemic based on a measure of need derived from machine learning models using mobile phone and satellite records. The authors found that, relative to an alternative proposal of allocating funds based on less granular measures of poverty, the algorithmic method increased the probability of getting the aid to the individuals who were most at risk.
However, there are real costs posed by AI to society that – as noted by Acemoglu (2021) – are all the more important to understand and confront because of “AI’s promising and wide-reaching potential”. Examples that stem directly from AI’s control of information include privacy violations, creating anti-competitive environments, and behavioral manipulation by machine learning techniques that enable companies to identify and exploit biases and vulnerabilities that consumers themselves do not recognize. Further, there is the direct risk that workers will be displaced by AI via excessive automation, as there is no guarantee that the current pace of the development of AI tools will achieve the socially optimal mix of automation and augmentation of tasks. Finally, there are a number of clear ways that AI have exacerbated social problems, including issues of discrimination and concerns about the functioning of democratic governments. There is substantial evidence—as discussed in Box 2 and in the hiring case study below— that AI has introduced and perpetuated racial or other forms of bias, both through issues with the underlying datasets used to make decisions, and by unintentional or seemingly-benign decisions made by algorithm designers. AI also can negatively impact how societies communicate on issues fundamental to the functioning of democracies, such as how echo chambers in social media can propagate false information and polarize society. While these costs are substantial, they are often not inherent to AI, but very much a product of the choices made in the development and deployment of the technology, meaning that there is a central role for governments in the studying, monitoring, and regulating of AI, as evidenced by the United States AI Bill of Rights and the
        
9
European Commission Artificial Intelligence Act.
The particular focus of this report is the impact of AI on the workplace. While in the four decades immediately after World War II, technological progress seemed to result in a prosperous labor market for all workers, a very different path of technological development started to emerge in the 1980s that was much less inclusive for low-paid workers, thus posing several challenges for policymakers. The resulting literature on skill biased technological change (SBTC), surveyed by Acemoglu and Autor (2011), documented how SBTC can account for trends in earnings distributions within the US and across different economics. However, the canonical SBTC model is one of technology generating a greater benefit to high versus low skill workers, while AI could either be a substitute or a complement to relatively higher skilled workers. One example is the strong incentives for firms to develop and adopt AI for automation instead of augmentation of tasks that were previously thought to require a human. Another example is that AI increases the ability to monitor workers. Although some amount of monitoring may be useful, monitoring can also be excessive if it is used to shift rents away from workers. In sum, unfettered AI could result in less democratic labor markets, worse working conditions, and an erosion of labor market institutions that favor workers.
Box 2. Failures in AI: Bias in Health Care
While AI algorithms have shown great promise in their ability to apply data to social and economic problems, there are many cases where they can exacerbate existing societal inequities. Obermeyer et al. (2019) examine the use of algorithms to determine which patients are “high-risk” and therefore receive additional resources and attention from care providers. The authors find that the algorithm they studied assigned poor patients to a lower risk score than an equally ill richer patient. This was because the algorithm used health cost as a proxy for health need, and poor patients generate lower costs than rich ones, potentially due to barriers in accessing care or bias they face in the health care system. The use of this proxy introduced bias into the algorithm and resulted in poor patients losing access to additional help they would otherwise have received.
AI’s enormous potential cannot be realized without a proper understanding and management of the above-noted challenges. Mokyr (2005) highlights the importance of this point by putting it in historical perspective. He argues that sustained economic growth after the onset of the Industrial Revolution was not solely due to the specific inventions made but was also facilitated by our understanding and management of these inventions. While there was also economic growth in the pre-1750 world from inventions such as gunpowder, spectacles, and the mechanical clock, this growth was not sustained because of a lack of understanding and management of these technologies. But how much does society understand about the consequences of the current era of AI and how can we best harness its enormous potential for sustained economic growth? Answering this question is the aim of this report, with a particular focus on the impact of AI on labor markets.
      
10
Part II: The Current State of AI Adoption In the United States, the most recent publicly available data on the adoption of different technologies—including AI—come from the Census Bureau’s Annual Business Survey (ABS). Two recent papers—Acemoglu et al. (2022) and McElheran et al. (2022)—respectively use the 2019 and 2018 ABS modules to describe the adoption of AI technologies across US firms. Both papers find that overall adoption of AI is low, but that adoption is concentrated among a set of large, young firms. McElheran et al. (2022) focus on how owner and management characteristics correlate with AI adoption, finding that firms with younger, more educated, and more experienced owners are more likely to adopt AI technologies. Acemoglu et al. (2022) are able to take advantage of an expanded set of questions on AI adoption, and investigate the reasons behind firms’ adoption of AI, the barriers to further adoption, and the connection between AI adoption and productivity.
Both papers find that few firms overall have adopted AI, but that statistics about firm-level adoption mask the true share of US workers exposed to AI. McElheran et al. (2022) report that in 2017, 2.9 percent of firms used machine learning, 1.8 percent used machine vision, and 1.3 percent used natural-language processing. Similarly, Acemoglu et al. (2022) find that only 3.2 percent of US firms used AI as part of their processes and methods between 2016 and 2018. However, in 2017, 11.7 percent of workers worked at firms that used machine learning (and 6.8 and 8.8 percent were at firms that used machine vision and natural-language processing, respectively), and between 2016 and 2018, 12.6 percent of workers were employed at firms that utilized AI. This difference between firm and worker-level exposure stems from a key finding of both papers: larger firms are more likely to adopt AI technologies.
Important differences in AI adoption also exist irrespective of a firm’s size (see figure 3). First, firms in industries like information, professional services, management, and finance are the most likely to adopt AI technology. But workers in industries like retail trade, transportation, and utilities are also more likely to be exposed to AI than average. Second, irrespective of a firm’s size, younger firms are more likely to adopt AI. For example, of all large firms in the 95th to 99th percentiles of the firm size distribution, roughly 7 percent of firms in the youngest age quartile have adopted AI, whereas only about 3 to 4 percent of firms in oldest age quartile have done so. The fact that AI adoption is concentrated in larger and younger firms most likely reflects the fact that adopting this technology entails substantial costs and organizational barriers. Further, firms with venture-capital funding and other characteristics McElheran et al. (2022) categorize as “startup conditions consistent with high-growth entrepreneurship” are correlated with the use of AI.
     
11
Figure 3. Percentage of firms and workers with some AI Adoption
Percent (%)
Source: Annual Business Survey 2019; CEA calculations
The 2019 ABS module also asks why firms adopt AI, and what barriers they face in implementing this technology. Both adopting and nonadopting firms report that the inapplicability of AI to the firm’s business, and AI being too costly, are the main reasons for either not adopting AI or the major existing barrier to further AI implementation. Of all AI adopters, around 80 percent (employment-weighted) report doing so to improve the quality of their product or service, 65 percent to upgrade existing processes, and 54 percent to automate existing processes. While the share of AI adopters stating that automation is one of their drivers is lower than other reasons, the findings of Acemoglu et al. (2022) regarding labor productivity—that AI adopters have higher labor productivity and lower labor shares than similar firms—are consistent with automation as a major application of AI. The use of AI to automate existing processes could have important adverse consequences for workers. AI competes more intensively with workers than other advanced technologies, with potentially important adverse consequences for the employment of individual workers.
 
12
The survey data discussed above are not a totally comprehensive look at firm-level adoption of AI technology. Many uses of AI might be missed in these surveys. For example, the use of voice assistance—like Siri, Cortana, or Alexa—is exceptionally common in the US. In 2017, 46 percent of Americans used digital voice assistance, according to the Pew Research Center, with the vast majority using the service through their smartphone. In 2019, the share of Americans reporting the use of a digital voice assistant had grown to 72 percent, according to a Microsoft survey. This speaks to the emergence of AI in many areas of life, and not just the adoption of AI by firms.
The private sector is not the only part of the US economy that has begun to utilize AI. The US Federal Government has begun to implement AI in a range of settings, including improving taxpayer waiting times when contacting the Internal Revenue Service (IRS) and creating AI competitions to predict patient health outcomes using Medicare data. The IRS, to address concerns about the long waiting times faced by callers, has implemented an AI-based voice bot system that currently allows taxpayers to set up payments and get notice questions answered. In the next year, this service will be expanded to allow for the bots to retrieve more information about individual taxpayers, further reducing waiting times. In 2019, the Centers for Medicare and Medicaid Services (CMS) created the CMS Artificial Intelligence Health Outcomes Challenge, a competition that was designed to accelerate “development of AI solutions for predicting patient health outcomes for Medicare beneficiaries.” In 2021, the competition concluded, with the winners using Medicare case records to accurately predict patients who were likely to experience adverse events and explain these predictions to clinicians.
     
b) Adoption of AI in the European Union
percent
percent percent percent 13 percent
percent of
The overall trends of firm-level AI adoption appear similar in the European Union to those in the
United States: in 2021 8
percent
of all enterprises with more than 10 employees employed AI
technology. The data, derived from Eurostat’s Community Survey on ICT Usage and E-Commerce

in Enterprises, asks about the usage of a range of AI technologies, including deep learning, the

analysis of images and written/spoken language, and the automation of work. Larger firms were
more likely to use some form of AI technology, with 28
percent
of firms with more than 250
employees reporting their use. The survey also showed that firms most used AI to automate
workflows, employ machine learning, or analyze written language (3
case). The overall story is similar to that told by survey records from the prior year: in 2020, 7
percent
of firms in each
of enterprises in the EU reported to use AI. Some common uses of the technology were
the analysis of large data sets via machine learning and the deployment of chatbots (2
firms in both cases). With these data, we can also see the distribution of AI usage across EU
Member States. In 2021, Denmark reported the largest share of enterprises employing AI, at 24
. Portugal (17
), Finland (16
), and Luxembourg and the Netherlands (both
) come next.
13
Hoffman and Nurski (2021) discuss the Eurostat Community Survey on ICT Usage and E- Commerce in Enterprises as well as other surveys (including a smaller survey by the European Commission) in more detail. As in the US, they find that robots are concentrated in manufacturing, while the adoption of other types of advanced technologies is higher in services such as finance, education, health, and social work. Within each of these sectors, larger firms are more likely to adopt AI, suggesting that there are substantial costs and organizational barriers involved in adopting AI. Skills and financial constraints are the leading reported barriers, with about 80 percent of enterprises citing a lack of skills in their internal workforce and in the external labor market, as well as the high cost of buying the technology and adapting their operational processes to AI.
  
14
Part III: The Impact of AI on Work
The previous parts of this report have argued that as AI continues to evolve and work its way into a wide variety of applications, its potential gains for society are enormous. As GPT-3 notes below in box 3, AI’s benefits could span industries, providing workers with time for new tasks and firms with greater speed and accuracy through automation. The report has also shown how challenges arise for policy in seeking to benefit from the impact of AI—enumerated by GPT-3 in box 4 –, including policies related to the collection and control of data, communication in society, and safeguarding democratic principles.
This part of the report focuses on the opportunities and challenges resulting from AI for labor markets. AI has the potential to increase productivity, create new jobs, and raise living standards. However, by its very nature of performing “non-routine” tasks formerly thought to be strictly the domain of humans, AI is likely to disrupt large swaths of jobs and tasks. This may lead to difficult adjustments for workers as jobs are redesigned or required skills change.
Box 3. AI Prompt: “Write a paragraph describing how AI can benefit the workforce”
In Part I, GPT-3 (a modern, trained AI engine) was asked to give its own definition of AI. Next consider the following prompt submitted to GPT-3: “write a paragraph describing how AI can benefit the workforce.” The output that followed from GPT-3 was:
“AI has the potential to streamline tasks across many industries, from health care to manufacturing. For example, in health care, AI can be used to automate the tasks of sorting through medical images to diagnose conditions. In manufacturing, AI can be used to automate the tasks of quality control and inspection. By automating these tasks, AI has the potential to free up workers' time so that they can focus on higher-level tasks. In addition, AI has the potential to improve the accuracy of these tasks, as well as to identify tasks that are most suitable for automation.”
GPT-3, a modern AI on the benefits of AI for workers
This poses several challenges for policymakers. One example illustrated in the previous part of the report is that much of the development and adoption of AI is intended to automate work instead of augmenting it. Private firms advancing AI technology are likely to do so in a direction that maximizes profits, which may not be the socially desirable direction. Another example is that AI increases the ability of employers to monitor workers. In sum, while the potential benefits of AI for labor markets are numerous, unfettered AI could also result in a less democratic and less fair labor market.
  
15

Box 4. AI Prompt: “What are the downsides of AI for workers?”
Consider the following prompt submitted to GPT-3: “What are the downsides are of AI for workers.” The output that followed from GPT-3 was:
“1. Job Losses: One of the potential downsides of AI is that it could lead to increased unemployment as machines begin to replace human workers in a variety of industries. 2. Inequality: Another potential downside of AI is that it could exacerbate existing economic inequality as the benefits of AI technology disproportionately accrue to those who are already wealthy and have access to the best resources.
3. Security Risks: Another potential downside of AI is that it could create new security risks as malicious actors begin to use AI technology for malicious purposes. 4. Ethical Concerns: Another potential downside of AI is that it could raise ethical concerns as AI technology increasingly begins to impact our human lives in ways that we may not be comfortable with.”
GPT-3, a modern AI on the downsides of AI for workers
Although the literature identifies more challenges than those listed by GPT-3 above, designing policies to tackle these challenges requires a proper understanding of how technological progress has affected labor markets in the recent past, and how AI is likely to change jobs in the future. Autor (2022) provides an overview of the recent thinking about the impact of digital technologies on labor markets. His starting point is a task-based view of labor markets that has become the standard framework in the literature over the past decade.
The hypothesis arising from this view is that digital technologies can automate “routine tasks.” What makes a task routine is that it follows an explicit, fully specified set of rules and procedures. Tasks fitting this description can be codified in computer software and executed by machines (e.g., robots to assemble a car, email to deliver messages). Conversely, “non-routine tasks” have historically been challenging to program because the explicit steps for accomplishing these tasks are often not formally described. Paradoxically, even though we cannot formally express non- routine tasks in an algorithm, many of these tasks are easy for humans to do. This is known as Polanyi’s paradox—that “humans know more than they can tell,” named after the 20th-century philosopher Michael Polanyi and his argument that all our knowing is rooted in tacit knowledge.
Goos, Manning, and Salomons (2014) show that routine tasks are concentrated in middle-paid occupations (e.g., machine operators, office clerks), while non-routine tasks (e.g., waiting tables in a restaurant, cleaning a room, diagnosis diseases, or team management) are concentrated in low- paid occupations (e.g. restaurant server, cleaner) and high-paid occupations (e.g., health professionals, managers). Consequently, typical automation technologies have decreased demand
   
16
for middle relative to low-paid and high-paid occupations, resulting in a process of job polarization. They show that this is happening in the 16 Western European countries that they examine from 1993 to 2010, and similar evidence exists for the US (Acemoglu and Autor 2011).
AI has the potential to fundamentally change the relationship between automation technology, labor demand, and inequality. While studies have so far examined digital technologies such as computers and industrial robots, AI overturns the assumption that technology can accomplish only routine tasks. AI can be used to infer tacit relationships that cannot be fully specified by underlying software, because AI learns to perform these tasks inductively by training on examples instead of by following explicit rules that are programmable.
Consequently, many non-routine tasks done in both low-paid and high-paid occupations that cannot be performed by computers could be performed by AI in the future, with very different implications for labor demand, job polarization, and inequality. For example, we might no longer see a process of job polarization but one of stronger relative employment growth in high-paid occupations (if AI automates non-routine tasks in low-paid occupations) or of stronger relative employment growth in low-paid occupations (if AI automates non-routine tasks in high-paid occupations).
Because of AI’s promise of a paradigm shift in our thinking about its impact on work and inequality, there is much uncertainty about AI’s implications for labor markets. The remainder of this part of the report focuses on these four questions:
	a)  What jobs and tasks are at risk from AI? 
	b)  What new jobs and tasks will emerge from AI? 
	c)  What will be the impact of AI on workers? 
	d)  What will be the impact of AI on the workplace? 
a) What Jobs and Worker Tasks Are at Risk from AI?
Although earlier digital technologies automated occupations that were intensive in doing routine tasks (e.g., machine operators, office clerks), AI as a prediction technology has the potential of also automating various non-routine tasks across a wide range of occupations. To study this question, a small but rapidly growing literature has emerged that applies a task approach to analyze the effects of AI adoption on different occupations (Acemoglu et al. 2022; Brynjolfsson, Mitchell, and Rock 2018; Felten, Raj, and Seamans 2020; Webb 2020). These studies do not start from the premise that AI can only do a given set of tasks. Instead, they rely on various innovative ways to determine what worker tasks AI can and cannot automate.
     
17
Webb (2020) offers one example. He uses natural-language processing (NLP) algorithms that exploit the overlap between the text of job task descriptions and the text of patents to develop a new method for identifying which tasks can be automated by any technology. This allows him to construct a measure of the “exposure” of occupations to that technology. For example, suppose a doctor’s job description includes the task “diagnose patient’s condition.” An NLP algorithm then extracts the verb–noun pairs from this task, which would be “diagnose condition.” The algorithm then quantifies the same verb–noun pairs in a different sample of patents to identify whether any technology could automate a doctor’s tasks.
Using this approach, Webb (2020) first examines the impact of two previous types of new technologies: software and robots. For software, exposure is decreasing with education, with individuals in middle-wage occupations most exposed. Men are much more exposed to software than women, reflecting the fact that women have historically clustered in occupations requiring complex interpersonal interaction tasks, which software is not capable of performing. For robots, individuals with less than a high school education, and men under the age of 30 years are most exposed. By and large, these results are consistent with the literature on job polarization, which has found that computers and robots reduced demand for routine, middle-wage jobs while increasing it for non-routine, low- and high-wage jobs between 1980 and 2010.
Webb’s (2020) study then turns to the impact of AI on the demand for occupations. In contrast to software and robots, AI performs tasks that involve detecting patterns, making judgments, and optimizing. The most-exposed occupations include clinical laboratory technicians, chemical engineers, optometrists, and power plant operators. More generally, high-skill occupations are most exposed to AI. Moreover, as might be expected from the fact that AI-exposed jobs are predominantly those involving high levels of education and accumulated experience, it is older workers who are most exposed to AI. There are also some low-skilled jobs that are highly exposed to AI. For example, production jobs that involve inspection and quality control are exposed. However, these constitute only a small proportion of low-skill jobs.
To conclude, an emerging body of research suggests that AI can outperform workers in an increasing set of complex tasks mainly done by educated workers. Compared with earlier digital innovations, this suggests a paradigm shift in our thinking about AI’s potential to automate worker tasks. For example, the automation of worker tasks by AI could exacerbate a process of occupational deskilling instead of job polarization.
This paradigm shift will not be straightforward. One reason commonly argued is that, because AI is unaware of the rich context of many real-world problems, it cannot accomplish the complex- decision tasks that humans regularly undertake in their work. Another reason is pointed out by Acemoglu et al. (2022): that so far, AI has no detectable effects on the labor market at the aggregate
 
18
occupation level. This current absence of any visible aggregate effects of AI could lower our sense of urgency to understand its impact on work, even when such effects appear likely in the future.
b) What New Jobs and Tasks Will Emerge from AI?
In capturing AI’s benefits, an important lever for policymakers is that AI not only automates but also augments work. History is full of examples of jobs that were predicted to be doomed by automation but that instead flourished and were transformed. The introduction of the first ATMs around 1970 was predicted to end the job of traditional bank tellers, but the US today instead has many more bank tellers, at many more bank branches, doing different tasks than before because ATMs are poorly suited to such as relationship banking (Bessen 2015). If the set of tasks were fixed, then advancing automation would crowd workers into an ever-narrowing subset of tasks, perhaps finally making human labor altogether obsolete, if AI would evolve into a state of AGI. However, it is possible that even AGI will create many new jobs for workers.
While AI’s potential to automate jobs has received relatively little attention, even less is known about AI’s potential to create new jobs for workers. However, it is possible to learn from the larger literature that asks how many new jobs does technological progress create? To answer this question, Autor et al. (2022) exploit the emergence of new job titles in the US Census Bureau’s occupational descriptions that survey respondents supply on their Census forms. Their analyses show that, irrespective of whether a new job is created because of technological progress or some other reason, new work is quantitatively important. They estimate that more than 60 percent of US employment in 2018 was found in job titles that did not exist in 1940. Examples of new titles are “fingernail technician,” which was added in 2000, and “solar photovoltaic electrician,” which was added in 2018. Interestingly, “artificial intelligence specialist” first appeared in 2000.
Turning to the nature of new work, they find that between 1940 and 1980, most new work that employed non-college workers was found in middle-skilled occupations. After 1980, however, the locus of new work creation for non-college workers shifted away from these middle-tier occupations and toward traditionally lower-paid personal services. Conversely, new work creation employing college-educated workers became increasingly concentrated in professional, technical, and managerial occupations. In combination, these patterns indicate that new work creation has polarized after 1980, mirroring (and in part driving) aggregate job polarization.
To further explain the creation of new job titles, and the role of technological progress, Autor et al. (2022) follow a procedure like Webb (2020) by examining patent data using NLP. Different from Webb (2020), however, is that they also instructed their NLP algorithm to look for text that indicates augmentation instead of automation of worker tasks. For example, in 1999, the U.S. Patent and Trademark Office granted a patent for a “method of strengthening and repairing fingernails.” Their algorithm links this patent to the occupational title of “Technician, fingernail,” which was added by Census Bureau in 2000. Similarly, their algorithm links the 2014 patent
  
19
“systems for highly efficient solar power conversion” to the occupational title of “Solar photovoltaic electrician,” which was added in 2018. In sum, Autor et al. (2022) show that new technologies are an important driver of the creation of new worker tasks.
Autor et al. (2022) also instruct their NLP algorithm to look for text in patents that indicate a new technology’s potential to automate (instead of augment) worker tasks. What they find is that some occupations, such as radiologic technologists and machinists, have a high rate of automation relative to augmentation. Therefore, labor demand and thus employment would tend to fall in these occupations. Conversely, in other occupations, including industrial engineers and analysts, augmentation has been more important than automation, resulting in an increase of employment in these occupations. Interestingly, many occupations are either simultaneously exposed to both augmentation and automation or are not exposed to any technology at all. Examples of occupations with very limited exposure to technological progress as of yet include jobs that require interpersonal skills such as childcare workers, hotel clerks, and clergy.
To conclude, though technology’s potential to automate jobs has received widespread attention, it also augments work and is an important driver of new job creation. Autor et al. (2022) term this double-sided impact of innovation on work “the race between automation and augmentation.” In occupations with declining (increasing) employment shares, this race is won by automation (augmentation). Understanding this race gives policymakers important levers to capture the benefits of technological progress. For example, a race between automation and augmentation of worker tasks, even within narrowly defined occupations, implies that new technologies can perhaps be steered toward more augmentation and less automation.
Autor et al. (2022) do not specifically focus on AI. But many new jobs augmented by AI may soon enter as new occupational titles—digital assistant engineer, warehouse robot engineer; and content-tagger on social media, among other jobs. An important policy question is whether these are the jobs that society wants AI to create. Simultaneously, employment in many high-paid jobs that is desirable from a policy perspective might be eroded by AI’s potential to automate their tasks.
c) What Will Be the Impact of AI on Workers?
The impact of technological progress, including AI, on work is characterized by competing forces of automation and augmentation of worker tasks, even (and mainly) within narrowly defined occupations. The focus of researchers—as well as managers, entrepreneurs, and policymakers— should therefore be not only on AI’s automation or augmentation potential but also on job redesign. For example, Brynjolfsson, Mitchell, and Rock (2018) conjecture that machine learning will require a substantial redesign of tasks for concierges, credit authorizers, and brokerage clerks. The need for job redesign also poses challenges for worker adaptability: worker skills to do certain tasks, and worker mobility across jobs in the labor market.

20
Worker Skills
Acemoglu et al. (2022) leverage a new module introduced in the US Census Bureau’s 2019 ABS not only to assess firms’ adoption of AI but also to explore firms’ self-assessment on the implications of AI for their demand for labor and skills. Among AI adopters, 15 percent report that AI increased overall employment levels and 6 percent indicate that AI decreased them, which points to the limited and somewhat ambiguous effects of AI on employment levels. Instead, 41 percent of AI adopters increased their skill demand, while almost no firms (less than 2 percent) report a reduction in their demand for skills. This self-reported increase in firms’ skill requirements when they adopt AI explains part of the well-known skills gap and highlights the importance of investments in worker skills.
Genz et al. (2022) provide similar evidence for Germany. They examine
find that workers with vocational training benefit more than workers with a college degree. One explanation might be that AI augments vocational workers more than it augments tasks done by college workers. Another explanation is that Germany’s traditionally strong vocational training system (76 percent of all workers in the sample completed vocational education) provides an abundance of specialized skills that direct the development and adoption of AI toward making use of (and thereby augmenting) vocational skills.
Worker Mobility Across Jobs
It is inevitable that workers in some jobs will be displaced because AI automates rather than augments worker tasks and/or workers no longer have the required skills to do their jobs. Job displacement is costly for those made redundant and could be disruptive for labor markets in general. These adjustment costs and disruptions were also characteristic of previous technological upheavals, exemplified by the automation of the role of telephone operator as discussed in box 5. However, because of the rapidity with which AI is evolving, these costs now may be particularly acute, but research documenting the transition of displaced workers to new jobs (or not) due to AI is very limited.
One exception is Bessen et al. (2022). Using Dutch administrative data, they examine what happens to workers who are made redundant when their firm invests in AI with the purpose of automating the firm’s production process. They find that

printing. For this, they collected novel data that link survey information on firms’ technology
how German workers
adjust to firms’ investments into new digital technologies, including AI, augmented reality, or 3D
adoption to administrative social security data for Germany. They then compare technology
adopters with non-adopters. Though they find little evidence that AI affected the number of jobs,
the absence of an overall employment effect masks substantial heterogeneity across workers. They
 
21
the expected annual income loss across

all workers before their firm adopts AI accumulates to 9 percent of one year’s earnings after 5
years. They also show that this annual income loss is driven by spells of unemployment within a
year (rather than, e.g., quickly moving into lower-paid jobs), with unemployment benefits only
insuring partially against their income losses. These adverse effects of AI automation are larger in
smaller firms, and for older and middle-educated workers. In sum, their results suggest that there
are substantial adjustment costs for displaced workers, and that these costs are only partially offset
by unemployment insurance. Relatedly, in a case study given in part IV below, AI’s role in the
hiring process is highlighted. In some ways, AI can improve the transition between jobs by
facilitating matches between employers and employees, although there are also potential
drawbacks discussed in that setting.
Box 5. Labor Market Adjustment after the Automation of Telephone Operating
adjustments in the labor market for young female telephone operators, one of women’s main occupations at the time. They find that telephone operators were significantly less likely to still be working as operators 10 years after their state’s cutover to mechanical switching. While some found other jobs in the telephone industry, others (especially older workers) left the workforce, and those who remained employed were more likely to have switched to lower-paying occupations. They also find that automation of telephone operating did not decrease overall demand for young women in their local labor markets. After automation, young women were less likely to become telephone operators, and they entered different jobs such as middle-skilled clerical and lower-skilled service occupations (mainly typists and waitresses).
d) What Will Be the Impact of AI on the Workplace?
AI will also drastically change how we design our workplaces and companies’ business models. In turn, these changes will affect working conditions.
Wood (2021) discusses the prevalence of algorithmic management of workplaces. Algorithmic management relies on data collection and surveillance of workers to manage workforces in an automated way. Online labor platforms are a well-known example. These platforms enable workers to choose the clients and jobs they take, how they carry out those jobs, and the rates they charge to do them. However, to varying degrees, workers’ ability to make these choices is strongly shaped by platform rules and design features. Increasingly, algorithmic management is also being used in other settings, such as in warehouses, retail, manufacturing, marketing, consultancy, banking, hotels, call centers, and among journalists, lawyers, and the police. Wood (2021) summarizes several detailed case studies from these sectors.
Feigenbaum and Gross (2022) examine the introduction of mechanical switching in operating
telephone calls that took place in half of all US States between 1920 and 1940. They study
 
22
Consider the case of digital platforms for taxi services or home deliveries. On these platforms, algorithms allocate tasks to drivers via their smartphones (or other handheld devices). For example, a taxi platform can notify a driver with a trip request, which the driver must accept within a 15- second window. Only after having accepted the request, the algorithm provides drivers with the passenger’s location, fare, and destination. The limited time frame given by the algorithm to accept a request while withholding key information is done to minimize the chances of drivers’ declining trip requests. Moreover, if drivers decline too many requests, the algorithm can log them temporarily out of the app as a punishment. Once a driver has accepted a trip request, the algorithm recommends a route for reaching the drop-off location. If drivers deviate from the suggested route, the algorithm can send notifications. If the app is also responsible for paying drivers, the app can further punish drivers who take too long to get to their destinations by refusing to release drivers’ payments. In sum, despite the many advantages platforms bring to workers and their clients, their algorithmic management can strongly reduce workers’ ability to choose clients, how to do their tasks, and the rates they charge to do them.
Weil (2017) discusses the broader impact that algorithmic management has on business models and labor relations. In his testimony to the U.S. House of Representatives, he argues that firms can use information and communication technologies to erode the need for traditional employment relationships. Since the 1980s, many large corporations have shed their role as direct employers, in favor of outsourcing work to smaller subcontractors or franchisees. Competition between these subcontractors or franchisees implies that costs, including wages, are lower compared with a situation where the lead corporation directly employs these outsourced workers. Because this fissuring of workplaces, as Weil calls it, mainly affects low-wage jobs, it has exacerbated higher wage inequality, decreased occupational safety, and increased health risks for workers in fissured jobs. Unfettered AI can become the glue to make the overall business strategy of fissuring operate even more effectively. It can further enable lead companies and their shareholders to manage their labor supply chains even better through the intelligent monitoring of outsourced workers.

23
The traditional approach to hiring during the latter half of the 20th century was straightforward: applicants submitted their résumé and cover letter, perhaps with answers to job-specific questions, to opportunities they saw advertised either on job boards or in classified ads. Hiring managers waded through a stack of applicant files, winnowing through the list to determine who they wanted to consider. After a series of interviews, a job offer was made, and the particulars of the offer were perhaps negotiated. Eventually, a candidate would accept an offer and begin work.
During the past decade of AI innovations across modern economies, the hiring process has dramatically changed. While the individual steps of the process are broadly similar, at each stage firms have adopted AI-based tools to increase the speed and scale of the process. AI can match résumés with job listings on a massive scale, saving both the applicant and hiring manager much time. AI can screen résumés to discard applicants that are likely to be a poor fit; it can then put the candidate through assessments to further narrow the field. For many firms, only at the later stages of the process do humans enter the picture: final interviews, negotiations, and convincing a candidate to accept an offer remain important tasks for HR professionals. However, once the candidate has accepted, AI returns to assist in the retention and promotion roles. While the central goal of hiring remains the same, the set of tools available has changed, primarily due to innovations in AI that go far beyond the suite of tools previously available to hiring managers.
In order to explore the recent developments of AI in the field of hiring, staff at the Council of Economic Advisors conducted a series of interviews with stakeholders in this space. Over the span of the summer of 2022, they conducted six interviews with four firms, representatives of an industry group, and a scholar in the field of AI. Each was asked a series of questions about the current use of AI in hiring, with an emphasis on fostering an open and honest discussion. The interviews were compiled by CEA staff and combined with independent research and consultations with European Commission partners to form this case study.
AI in Practice for Hiring
Consider a firm that is looking to hire for a number of open jobs. They want to find the right candidates for each position as fast as possible, a task that requires both maximizing match quality as well as speed. Further, this requires high-volume hiring, as the firm is now facing the same rising job turnover to which many firms in the modern labor market are exposed. This is new for the firm, as historically it was not posting as many jobs and did not feel the pressure to hire with much speed. This new world of hiring is quantitatively different in scale, with an increase in job postings, applications submitted, and offers extended. This pressure to process more applications faster and attract more diverse and qualified workers without sacrificing the quality of a given

24
match, leads firms to turn to AI solutions. The hiring manager is now faced with managing his or her team of recruiters and juggling multiple job openings, each of which is at a different stage in the recruitment process. However, at each stage they can turn to AI for guidance, advice, and support. Trey Causey of the job search site Indeed offered some observations about the broader hiring landscape; “it is hard to think of a place in hiring where AI is not appearing. Hiring managers are able to couple AI solutions with human participation in the hiring process.”
At the very beginning of the process, the hiring manager needs to post a job opening, including crafting the text of the job description that will be published across a range of job search platforms. However, they do not need to do this alone. Rather, they can turn to a range of services that will use natural-language processing to help them write job descriptions. The power of these tools is that they link language to a data set of outcomes, allowing the hiring manager to craft job descriptions that will maximize the chances of attracting the right kind of applicants.
Turning from writing the text for a new job posting, the hiring manager now needs to figure out how to get this opportunity in front of candidates. To do this, they make use of one of the most common applications of algorithms in hiring: the matching of job applicants with job postings. These algorithms rely on the text of résumés and job postings, as well as information about the positions and the background of the candidates to determine which candidates are the best fit for a given job posting, or vice versa. In some cases, this results in a literal “fit score,” which hiring managers can use in evaluating candidates. The use of these systems may require the hiring manager to oversee the purchase of advertising on different recruitment platforms in order to get the job posting in front of the right candidates.
A few days ago, the hiring manager posted a separate job opening, and already candidates are reaching out for details about the position and with questions about the application process. However, the hiring manager is not responding to these messages. Rather, a small army of chatbots—powered by natural-language processing—are tasked with responding to specific and unique questions from candidates about the open positions. And the use of chatbots does not end there. The hiring manager can turn to chatbots to screen the initial round of applicants, an important step given the volume of applications the firm is receiving. These bots collect information on the candidates’ backgrounds and experience that will be incorporated into the decision of whether to advance a candidate to the next round.
With the new, smaller pool of applicants, the hiring manager can now utilize a range of evaluation tools, ranging from recorded interviews that are transcribed and analyzed to “gamified” assessments, essentially logic puzzles and other games that can assess particular skills among applicants. As a result, they can assess a candidate’s personality, skills, and critical thinking, all without interviewing a candidate directly. And these tests have science to back them up, connecting the results of the test to the narrow skill set they are measuring. However, the hiring
  
25
manager is cautious about using these tools, both because the connections between the skill sets and job performance have not been as thoroughly reviewed, and also because they have seen older tools be withdrawn because of issues of blatant bias. At the same time, the hiring manager finds these tools useful, as they both increase the speed of the application process and have the potential to improve the quality of matches.
AI for Applicants
In preparing for the job hunt, all applicants—whether applying directly from college, transitioning between roles within an industry, or thinking about a change of careers—can turn to a number of AI-based tools for developing their interview skills and preparing their résumé. Firms such as Indeed and VMock offer AI-powered tools that often highly weight specific terms to assess one’s résumé and offer suggestions on ways to improve it. In particular, because many résumés now are screened by AI-based tools, an important way to improve a résumé is to use keywords that help a candidate survive an initial screening.
Another way AI can help applicants is by focusing on applicant skills that may apply to a broader set of potential jobs than the candidate had considered. One example given by VMock was that the day-to-day work of a chef involves managing a large number of people in a high-pressure environment while meeting tight deadlines, a set of skills valuable in a wide number of positions outside food service. ZipRecruiter’s platform uses an active learning algorithm to try to understand which open positions appeal most to a candidate based on their interest level in the positions they have shown them so far; they use a similar algorithm on the hiring side to learn what types of candidates hiring managers are looking for. These learning algorithms lead to better matches on both the applicant and hiring sides.
After a candidate has applied to a position, they may be interacting with some of the chatbots described in the previous section; natural-language processing technology has advanced to the point that chatbots provide human-like interactions with job candidates. The firm Indeed candidly discusses the pros and cons of chatbots, noting that while they can be useful in reducing unconscious human biases in the hiring process, they also have the potential to create a negative impression with candidates. This reflects a larger question: to what degree are candidates aware of bots when they are being evaluated by a human versus an algorithm?
Algorithmic Creep and Unintended Consequences
At almost every stage the hiring process at firms across the United States and Europe, the role of AI-driven algorithms is increasing. This trend—dubbed “algorithmic creep” by Alex Engler of the Brookings Institution—encompasses both the expanded use of algorithms across different stages of the hiring process and the higher share of firms employing algorithms at each stage. Most of the firms that were spoken to for this report claimed that the result of this widespread adoption is a
 
26
process they claim is faster, scales up dramatically, and results in a greater number of qualified candidates finding better jobs. However, as Engler notes, “this transition to an algorithm- dominated hiring process is happening faster than firms, individuals, or governments are able to evaluate its effects.”
One of the primary concerns raised by nearly everyone interviewed is that greater adoption of AI- driven algorithms could potentially introduce bias across nearly every stage of the hiring process. Machine learning algorithms are often referred to as “money laundering for bias,” in that they give the appearance of a fair and clean mathematical process while still exhibiting biases. Some firms are aware of this risk and are conscious of the potential for AI to lead to a less-biased hiring process than the former human-centric hiring process was known to be. However, even fair and well- intentioned algorithms have been shown to introduce bias in unexpected ways. For example, Lambrecht and Tucker (2019) show that STEM career ads that were explicitly meant to be gender neutral were disproportionately displayed by an algorithm to potential male applicants because the cost of advertising to younger female applicants is higher and the algorithm optimized cost- efficiency. A team of researchers at Google studied how natural-language models interpret discussions of disabilities and mental illness and found that various sentiment models penalized such discussions, creating bias against even positive phrases such as “I will fight for people with mental illness.”
The AI-powered tools developed to evaluate applicants have been particularly troublesome in this respect. The Center for Democracy & Technology issued a complete report in December 2020 titled “Algorithm-Driven Hiring Tools: Innovative Recruitment or Expedited Disability Discrimination?” The report explored the challenges algorithmic evaluation of candidates face in complying with the Americans with Disabilities Act, noting the many ways in which different screening tools may be biased against those with disabilities. Because many AI assessment tools employ a video interview, it is further worth noting that a study of automated speech recognition software found large racial disparities between interviews with white and African American individuals. Further, the Gender Shades project by the MIT Media Lab shows that three leading AI tools perform systematically worse at analyzing images of darker-skinned individuals, and particularly darker-skinned women, indicating that algorithmic tools introduce error for particular groups. These studies raise serious questions about the introduction of bias when employing AI in candidate evaluation and firms are responsible if they implement AI solutions that violate existing laws and regulations on discrimination.
There was friction in most of our conversations between the knowledge that the pre-AI hiring process was far from bias-free—there is evidence of long-standing bias against nonwhite workers, and people with disabilities, among other groups—and the concern that the use of AI can exacerbate these biases. However, in each conversation, there was—to a lesser or greater extent— the belief that correctly applied AI could reduce bias in hiring. Engler stated that the transition to
         
27
the heavy utilization of AI in hiring has the opportunity to “functionally be a re-set when it comes to labor-market discrimination against workers across racial, gender, disability, and economic lines.” And the Data and Trust Alliance stressed that they were working on utilizing AI to identify existing bias within models, and to help their clients meet their individual diversity and inclusion goals. Audits of AI technology are increasingly seen as necessary step in implementing new AI systems, although clarifying exactly how these audits should be conducted is still an evolving discussion within the field. There are also ongoing efforts within the private sector to create best practices. The Data & Trust Alliance, for example, is a consortium of firms that has developed a set of safeguards in the form of a list of questions that firms can ask of their AI tool vendors to evaluate the potential for bias in their algorithms. The degree to which these efforts have begun to move us toward a more egalitarian world of hiring is very much unknown, leaving all the actors in this space split between the possibilities and perils of AI and bias.
Hiring and Job Loss
One concern that emerges with any discussion of automation is the potential for job loss, in this case within the hiring and human resources profession. As discussed earlier in this report, AI has the capacity to automate non-routine tasks, and the discussions with firms highlighted how, if an HR department is using an AI algorithm to schedule appointments, review résumés, answer candidates’ questions, and, overall, automate a range of tasks—from the mundane to the intricate—it is conceivable that they will need fewer (or different) workers.
The consequence of this change in roles is that HR departments may be looking for a different combination of skills or experiences from their HR professionals. An example raised by Causey of Indeed is that HR managers now need to understand how to manage the promotion of their job listing through various internal and external tools utilizing AI. Though many job-posting platforms allow free posting services, most offer the opportunity to “promote” a job to increase its exposure to more candidates. AI poses the opportunity to make this simple and reduce demands on hiring managers. Different sites have different models for this: some use a “cost-per-click” model, where the firm pays every time the job posting is clicked, while others use a “cost-per-application” model, where the firm pays every time it receives an application. Both types of systems require hiring managers to set daily budgets for how much to spend. This task is new to the HR profession and requires learning about the different systems as well as the value of investing in promoting job posts.
However, some firms we interviewed made the point that this is a potential benefit of the adoption of AI in hiring. Mahe Bayireddi, CEO and cofounder of Phenom, stressed that many firms with slim profit margins are looking to AI as a way to both increase the efficiency of their hiring and reduce the costs associated with HR. He framed Phenom’s work as helping firms ask “where can you personalize, and where can you automate.” With this framing, he both highlighted how AI can
 
28
allow firms to connect with more workers at a greater speed while preserving humans for activities where more strategic value is needed, like convincing someone to accept an offer once it has been extended. This reframing of the HR role as managers of AI and as “talent advisers” makes certain elements of the hiring process more human.
The discussions above mirrored the questions posed in Part III regarding the difficulty of ascertaining the net effect of AI on employment. The AI processes discussed by firms may end up automating many of the tasks currently performed by people, although AI may make employees more productive and create new tasks that require human intervention. While the net effect of AI on employment is unclear, it is likely to result in an HR office that is able to manage a much larger scale of operations.
The Future of AI and Hiring
The firms consulted for this report were asked how AI had changed the hiring process, and many noted that AI has quantitatively changed the process by allowing the large-scale implementation of systems to attract, screen, and assess potential employees. However, qualitatively, the HR function has not changed; it is still the search for the right person for each job opening. AI has been applied as a tool to the existing hiring process; there is optimism, however, that some structural changes in the hiring process may be coming.
One such change cited by several firms was the possibility of “digital credentials,” or “learning and employment records,” as technologies that could improve how AI functions in the roles it plays today. Standardized electronic records for education, training, and skills could streamline how AI matches applicants with job postings, potentially in a fairer and more balanced way. Of course, this relies on the assumption that the process of obtaining such records is itself fair and unbiased. Though AI has proven effective at using existing résumés and job listings, it is not clear that those are the best “inputs” to a matching system. As a result, firms have been thinking about what might be the right way to design the process from scratch. There is therefore great potential for more transformation of the HR process from AI—Indeed’s Causey noted that “we are on the cusp of a lot of qualitative changes” to hiring.
Conclusions
The overarching message from discussions with firms in the hiring space was that AI-powered algorithms could improve nearly every step in the hiring process for firms, HR staff members, and candidates. In fact, some firms very clearly structured their responses to questions by systematically addressing each stakeholder in the hiring process, discussing how each could benefit from the greater deployment of everything from chatbots to predictive models that match candidates with potential employers. However, adoption of AI has been so rapid that firms may
  
29
not entirely recognize the implications of allowing algorithms into the HR function. Firms should be auditing their usage of AI tools to ensure compliance both with labor regulations and also with their own ethical standards.
b) Case Study 2: Warehousing
Supply chains and the logistics industry
In the 1990s, supply chain management became a major field as more and more industries reassessed how products could be made because of new information technologies, combined with the reduction of many international trade restrictions such as quotas and tariffs. Supply chains benefit companies through gains from specialization, resulting in lower prices for intermediate goods and services. Consumers benefit through lower prices for final goods, and workers benefit through increased employment and wages. At the core of supply chain operations are logistics and warehousing to move goods between companies and to consumers.
Supply chains lengthen when companies decide to outsource activities because the benefits of finding outside suppliers capable of providing intermediate inputs at lower cost outweigh the benefits of keeping those activities inside the organization. Supply chains often take the shape of deeply integrated networks with lead companies at the center and their suppliers orbiting around them, resulting in business models known as lean manufacturing, lean retailing, and global-value chains.
Lean manufacturing
Lean manufacturing is a core production strategy famously developed by Toyota after WW-II. Its objective is to reduce inventories of intermediate parts and finished products, carefully matching real-time demand for goods with the quantity moving through the supply. In most supply chains, this requires high levels of coordination at each step in the process, careful management of capital and labor, attention to quality and factors that affect throughput, and strong logistics support systems. Lean manufacturing started in the auto industry, and many other manufacturing and retail sectors have adopted some or all of Toyota’s pioneering practices.
Lean retailing
Like lean manufacturing, lean retailing takes advantage of information technologies, automation, industry standards, and innovations in logistics and warehousing to align orders from suppliers more closely with what consumers are buying in the store. By using sales information collected through millions of scans of bar-coded labels, retailers reduce their need to stockpile large inventories of products, thereby reducing their risks of stock-outs, markdowns, and inventory carrying costs.

30
Global-value chains
The reduction in quotas and tariffs together with falling costs of transportation transformed domestic supply chains into global-value chains. In manufacturing, this implies seeking suppliers for parts and assemblies that are outsourced to producers abroad (instead of at home). Outsourcing to producers abroad is also known as offshoring.
Logistics and warehousing – the core of supply chains
As lean manufacturing, lean retailing, and global-value chains spread across sectors in the economy in the past decades, the importance of logistics and warehousing increased. Moreover, the emergence of digital technologies since the 1980s and, more recently AI, have transformed the nature of logistics and warehousing.
Initially, a warehouse was simply the place where you store inventory—and where that inventory can sit for long periods of time. Although warehousing required tracking and managing where things had been left, it didn’t require a lot of attention to how quickly those things could be accessed and moved once needed. However, with lean production, the warehouse becomes a distribution center—a place where intermediate or final products are efficiently tracked, processed, and moved. Some modern distribution centers are also known as “fulfillment centers” or “FC”.
The growing importance of warehousing
Driven by the emergence of supply chains, the economic importance of warehousing as a sector has increased over the past decades. This is illustrated in Figure 4, using data from the OECD’s Structural Analysis (STAN) database, which includes the US and European countries with sufficiently detailed data. For each country, the first black bar shows the percentage of a country’s total value added in 2018 that is produced by the NAICS Revision 4 subsector “Warehousing and support activities for transportation”. Countries in Figure 4 are ranked by their importance of warehousing in total value added in 2018. In Ireland, warehousing accounted for 0.5 percent of total value added. In the US, this number was 0.7 percent. The highest numbers are 2.7 percent for Belgium and 3.6 percent for Lithuania. For each country, the second blue bar shows the percentage of a country’s total workforce in 2018 that is employed in the subsector of warehousing. For example, the highest fraction is found for Belgium, with 2.3 percent of the total workforce employed in warehousing. In all countries, the subsector of warehousing is an important employer.

31
Figure 4: Warehousing value-added and employment shares by country
Warehousing as a percent of countries total (%)
4 3.5 3 2.5 2 1.5 1 0.5 0
             
Value added in 2018
Employment in 2018
Value added in 1995
Employment in 1995
Source: OECD STAN database for industrial analysis, 2020 ed. Notes: Warehouse industry defined as the NAICS Rev. 4 subsector “D52: Warehousing and related transport support activities”
The markers in Figure 4 also show that warehousing has become more important relative to other sectors since 1995. For example, in Germany – the EU’s largest economy – the share of warehousing in total value-added increased from 1.0 percent in 1995 to 1.8 percent in 2018 (with value added expressed in 2015 euros in both 1995 and 2018). In the US, the value-added share of warehousing was 0.4 percent in 1995 and 0.7 percent in 2018. Overall, Figure 4 shows a growing importance of warehousing in advanced economies since 1995, in line with the rapid expansion of supply chains.
To see what drives the growing importance of warehousing, Figure 5 shows the evolution of average labor productivity and average labor costs in OECD STAN subsector “Warehousing and support activities for transportation in the five largest EU Member States in our sample (Germany, France, Italy, Spain, The Netherlands) and the US. Average labor productivity is defined as value- added (in 2015 euros or dollars for all years) per employee. In the long-run, average labor productivity is expected to increase because of technological progress. Real average labor costs (in 2015 euros or dollars per unit output) are informative about the extent to which this productivity gain from technological progress is shared with workers through a higher average real wage. If

32
average labor productivity per worker increases relative to the average real wage, the labor share (i.e., the share of value-added that goes to workers) decreases. Alternatively, if average labor productivity per worker decreases relative to the average real wage, the labor share increases.
The solid black lines in Figure 5 plot the evolution of value-added per employee in warehousing, normalizing values to an index with 1995=100. For example, panel (a) for Germany shows that average labor productivity (i.e. value-added per employee) in warehousing was relatively low up to 2000, then increased rapidly to peak in 2006, during the 2000-2008 economic boom, fell during the recession of 2008-2010, and remained relatively stable thereafter. In the long-run, average labor productivity in warehousing increased by a substantial 38 percent between 1991 and 2019. The solid light blue line shows the evolution of average labor productivity for the entire economy. Eyeballing both the black and light blue solid lines suggests that warehousing is more pro-cyclical in the short-run and has stronger labor productivity growth in the long-run compared to the total German economy.
The dashed lines in Figure 5 plot the evolution of labor costs per employee (adjusted for productivity) in warehousing (the dashed orange line) and the total economy (the dashed green line). For example, panel (a) for Germany shows that the average real wage in warehousing increased 28 percent from 1995 to 2019. Before 2000, the average real wage in warehousing grew in line with economy-wide changes. After 2000, average real wage growth in warehousing was faster in the early 2000s, negative from 2005 to 2015 while average wages in the rest of the economy continued to grow, and again faster after 2015. In the long-run, average real wage growth in warehousing outpaced economy-wide average real wage growth, again pointing to the growing importance of warehousing in the German economy.
Turning to the group of all five EU Member States, panels (a) to (e) of Figure 5 show some differences as well as some similarities between countries over the last three decades. The following three stylized facts summarize a comparison of all five European countries:
1) Average labor productivity in warehousing increased in Germany, France, and the Netherlands but held relatively constant in Spain and declined in Italy. In Germany, France, and the Netherlands,

36
average labor productivity in warehousing increased by more than the increase in economy-wide average labor productivity. The fast productivity growth of warehousing in some European countries is indicative of the development and successful adoption of new technologies in warehousing over time.
	2)  Average real wage changes in warehousing are more ambiguous across countries. The average real wage grew most in Germany (exceeding economy-wide growth), followed by the Netherlands. However, the average real wage stayed constant in the long run in France and decreased in Italy (but by less than productivity) as well as in Spain (but only in the late 1990s). 
	3)  In all European countries except Italy, average labor productivity has grown faster than the average real wage in warehousing. This decoupling of productivity and wage growth is more outspoken for warehousing than for the entire economy. What this suggests is that only part or even none of the productivity gains in warehousing are shared with workers, resulting in a falling labor share. Stansbury and Summers (2020) examine several reasons why this could be the case. One reason for a decline in the labor share could be that warehouse technologies automate labor more than it augments what workers do. Another reason could be that the bargaining power of workers in warehousing has decreased over time, resulting in less rent sharing of the productivity gains from technological progress with workers. This could not only by driven by a decline in the unionization rates among warehouse workers, but also by the emergence of fissured labor contracts made possible by changes in labor law and labor market regulations. 
Finally, panel (f) of Figure 5 shows the evolution of average labor productivity and the average real wage for warehousing and for the total economy in the US. In line with strong economy-wide productivity growth, average labor productivity in warehousing also increased 41 percent between 1995 and 2019. In contrast to the economy-wide decoupling of productivity and wage growth and fall in the labor share since 2000, average labor productivity and the average real wage in warehousing have shown a similar long-run trend.
Algorithmic management in warehousing
Productivity growth in warehousing could be driven by technological and/or organizational innovations. Delfanti (2019) and Gent (2018) describe how Amazon distribution centers are organized around four core processes: receive, store, pick, and pack. Receiving and storing are part of the “inbound” process, while picking and packing constitute the “outbound process.” Workers at receive stations unpack incoming pallets of commodities and identify each commodity by a unique barcode. Workers then store items in the pick area. The pick area usually is a large multi-floor area dense with thousands of shelves. When goods must be retrieved, workers walk through the warehouse to pick goods and carry them to packing stations. In the pack area, workers receive, package and label orders, which are then sent to shipping. Throughout these processes, goods are sometimes moved in plastic warehouse boxes (e.g., the yellow boxes in Amazon warehouses).
 
37
Delfanti (2019) writes that at the core of receiving, storing, picking and packing are product barcodes and various types of barcode scanners. These barcodes and scanners are not only used to keep inventory, but also to assist workers and collect information about workers’ productivity. Workers pick up a portable barcode scanner at the beginning of their shift and use it to scan the barcode on their badge, thus logging into the system. From that moment on, the scanner mediates between workers and management, assigning tasks, communicating orders, and monitoring work. This is similar to forms of algorithmic management in the “gig economy,” where the barcode scanner is replaced by the phone apps commonly used to collect and use data. Most often, these corporate algorithms are inherently opaque and access to them is prohibited by industrial secrecy and nondisclosure agreements. Therefore, auditing these algorithms is difficult.
Based on interviews with Amazon warehouse workers, Delfanti (2021) further describes the tasks of pickers in Amazon’s MXP5 warehouse in Piacenza, a small town in northern Italy. A picker walks among the shelves pulling a cart which carries a box she needs to fill up with her batch of items to pick. Once an item has been picked, the picker uses her barcode scanner to scan the item’s barcode. The barcode scanner records, approves, and communicates to the picker the next item she is to pick. It also communicates the position on the shelves and the time the picker has to complete the task—often a minute or so. One effect of this form of algorithmic management is that it augments what workers do: no individual human being can efficiently navigate an area of several thousand shelves to pick a list of wanted goods without the aid of an algorithm. Moreover, algorithmic management further increases labor productivity by requiring pickers to keep a fast “Amazon pace” (i.e., one cannot run but must walk as fast as possible).
However, the increased efficiency of warehouse operations due to data driven technologies also comes at a cost for warehouse workers. As the data that they generate is managed by algorithms and managers (who are often not working in the warehouse themselves but in some faraway global headquarter), warehouse workers loose agency over the tasks they do. Algorithms can dictate the pace and contents of work because they monopolize the knowledge about warehouse inventory and processing, while workers can only guess what data are being extracted and what analytics are being used to organize and surveil their activities. To illustrate this for the case of Amazon, Delfanti (2021) writes:
Technology dictates the pace of work at Amazon. It is used to increase workers’ productivity, standardize tasks, facilitate worker turnover, and ultimately gain control over the workforce. Workers are acutely aware of the uneven nature of their relationship with machinery, and at the same time, they know the warehouse needs their living labor. As I was told by a manager, “technology codifies, understands, and manages. But the real machine is the human.”
- Delfanti (2021)
 
38
Working conditions in warehousing
On the one hand, warehouses need workers because receiving, storing, picking, and packing still relies on workers’ dexterity. Only humans possess the flexibility and speed to efficiently store items that are different in shape, weight, volume, color, and so on. On the other hand, workers need algorithms because what workers do not possess, or quickly surrender to algorithms, is their knowledge of the position of the commodities they have stored. In sum, workers and algorithmic management must co-exist in today’s warehouses.
However, Wood (2021) argues that the balance of power between workers and algorithms may be increasingly titled in favor of algorithms. As Figure 5 showed for all European countries in our sample except Italy, the productivity gains in warehousing are increasingly going to shareholders, in terms of dividends and stock options, at the expense of warehouse workers, in terms of their average compensation. This could be due to an increased use of algorithmic management combined with a decline in workers’ bargaining power. For example, Delfanti (2021) argues that algorithmic management implies that minimal training for warehouse workers is needed to do their jobs. Usually, it only takes hours to train new associates to work as pickers. This enables warehouses to sustain high turnover rates while having access to a productive but also flexible workforce needed if there are sudden spikes in sales. But minimizing the costs of training and worker turnover is not enough: management also needs to ensure a smooth and productive relation between workers and algorithms on the warehouse floor. Therefore, workplace discipline and worker self-discipline are imposed by monitoring workers through the data they generate. To further cement worker effort, a workplace culture is created through slogans such as Amazon’s “Work Hard. Have Fun. Make History.”
Conclusions
During the past decades, the importance of warehousing has increased as lean manufacturing, lean retailing, and global-value chains spread across the economy. Today, employment in warehouses alone accounts for a sizeable 1% to 2% percent of total employment in advanced economies. Moreover, the emergence of digital technologies since the 1980s and, more recently, AI have transformed the nature of warehousing. Indicative of the successful adoption of new digital technologies in warehouses is an increase in average labor productivity and the average real wage.
However, growth in average labor productivity has outpaced average real wage growth, resulting in a fall in the labor share in warehousing. In modern fulfillment centers this could result from the use of algorithmic management together with a less strong representation of employees at the workplace. Despite strong growth in employment and wages in warehousing, these changing working conditions in warehouses pose an important challenge for workers. With advances in AI, the future of warehousing could be converging to algorithmic management systems that are fully
 
39
independent of workers. Fully automated warehouses that operate without the use of human labor are popularly known as “dark warehouses”. Dark warehouses imply that all the productivity gains in warehousing due to algorithmic management have entirely shifted away from warehouse workers (who are all displaced such that the labor share has fallen to zero). Although it is uncertain that most warehouses will become dark warehouses in the future, technologies currently developed for warehousing are in part geared towards automation instead of augmentation of human labor.
40
Part V: Conclusions
The use of AI undoubtedly presents many opportunities to positively transform the economy. The last decade has seen incredible advances in natural-language processing and computer vision, enabling new applications of AI to tasks previously thought to be firmly in the domain of humans. Firms are rapidly adopting AI around the world for its ability to scale and lower costs, to absorb and process enormous amounts of data, and to help make better decisions, often assisted by humans. And all this process of transition is likely to create new jobs that never would have existed without AI.
At the same time, AI poses several challenges. Huge swaths of the workforce are likely to be exposed to AI, in the sense that AI can now address nonroutine tasks, including tasks in high-skill jobs that until now had never been threatened by any kind of automation. The primary risk of AI to the workforce is in the general disruption it is likely to cause to workers, whether they find that their jobs are newly automated or that their job design has fundamentally changed. The additional risk of AI is that it may lead firms—unintentionally or not—to violate existing laws about bias, fraud, or antitrust, exposing themselves to legal or financial risk, and inflicting economic harm on workers and consumers. Given the black box nature of these systems, detecting, and addressing these violations is far from a simple task. This presents governments with a clear agenda on how to guide AI development in a positive direction.
a) Investing in training and job transition services so that those employees most disrupted by AI can transition effectively to new positions where their skills and experience are most applicable.
The introduction of AI across all areas of firms is likely to lead to large disruptions to the workforce. As seen in the case study on HR in part IV, AI can be a useful tool for helping workers find new opportunities with the same company by matching skills with job openings. This use of AI could help soften the disruption for some workers. However, there is likely to be a need for large investments in training, either to develop the new skills required for existing jobs that are being redesigned due to AI or for new jobs where there is growing demand for workers.
Long-term trends in employment complicate the idea of retraining workers. The increased prevalence of shorter contract durations lowers incentives for firms to invest in worker training, leading to underinvestment in skill acquisition. Policies that promote or subsidize intermediaries that share the costs and benefits of training are needed to reduce skill gaps, especially for workers at risk of automation. For example, temporary help agencies that provide training and match workers to employers for a fee avoid the flight risk that makes employers reluctant to invest in- house training, and can also split the cost of training across multiple employers or between
41
employers and workers. Employers pay a premium to such agencies for access to already trained workers, and workers receive part of their increased productivity in higher wages.
Intermediaries that invest in workers’ skills to reduce skill gaps can be public, private, or hybrid. Examples include Public Employment Services, which offer training; outplacement offices, funded by companies that serve laid-off workers, which assist displaced workers in finding new jobs; and temporary help agencies specialized in training and finding jobs for workers who are otherwise unlikely to participate in the labor market. Katz et al. (2020) show that such policies could be particularly effective in increasing earnings and job mobility for trained workers.
Another set of programs that have been shown to increase wages focus on providing digital skills to workers. Companies report that the lack of staff with adequate digital skills is an obstacle to investment. Therefore, digital education action plans should ensure that more workers have basic digital skills. Digital skills not only include knowledge about computer science, technology, engineering, or mathematics (STEM), but also contain other skills that can complement new technologies. These other skills include communication and social skills that remain important competencies for workers even in workplaces that adopt AI systems.
b) Encouragement of development and adoption of AI that is beneficial for labor markets.
Firms, given their goal of maximizing profits, are most likely to conduct AI research and deploy AI systems that will directly benefit their bottom line. As a result, the development and adoption of AI is likely to diverge from what would be optimal for labor markets, i.e. workers’ wages and employment. Three concerns emerge from our study:
1. Investing in the development of AI that augments workers: The most straightforward concern many workers face when it comes to AI is that of automation. Acemoglu et al. (2022) document that 54 percent of AI adopters do so to automate existing processes and a recurring theme raised by firms in the hiring space was that automation of certain elements of the HR profession was a key part of their business model and was being sought by their clients. Also, the case study of warehousing showed that algorithmic management of distribution centers is geared toward process and workforce automation. The use of AI to automate existing processes has important adverse effects on some workers, either through the full automation of their job or by substantially shifting the skill set required to perform the role. Acemoglu (2021) claims that while investment in AI and other technologies can lead to economic growth, firm incentives to reduce costs, increase shareholder value, and increase profits can direct firms away from a socially optimal split between automation and augmentation of worker tasks. One way to correct for private incentive-driven deviations from the optimal development of AI technology
    
42
is to utilize public funds to encourage and stimulate AI research that augments instead of automates work.
Further, in contrast to private efforts to develop AI, publicly funded academic research can focus on a wider array of AI methods and topics. This encompasses everything from an expanded set of AI methods, exploring the impact of AI on workers’ wages and employment, the effects of algorithms on anti-competitive behavior in markets, the development of AI ethics, as well as how AI can exacerbate existing biases that are present in society, resulting in discrimination along racial, gender, and economic lines.
There is an ongoing effort along these lines to involve the public sector in AI research in both the US and the European Union. In the US, the National Security Commission on Artificial Intelligence, in its final report, urged Congress to double federal R&D spending on AI each year, until it reached $32 billion in 2026. The Biden Administration, in its fiscal 2023 budget request, proposed increasing the federal R&D budget to more than $204 billion, a 28 percent increase from 2021 enacted levels. Part of this funding would support new and existing National Artificial Intelligence Research Institutes. These institutes bring federal, state, and local agencies together with the private sector, nonprofits, and academia to tackle AI research. In Europe, several funds support AI research, including Horizon Europe, the European Research Council, the European Innovation Council, and European Partnerships. An important focus in these ongoing efforts should be whether AI augments workers in their current jobs and whether AI creates new jobs for workers.
Public procurement of AI that augments workers: Public bodies can also direct the development of AI that complements workers through their procurement of AI systems. This public procurement can be aided by access to public data for AI developers. The availability of data can shape both the level and direction of innovative activity. This is explored by Beraja et al. (2022), who show that Chinese firms with access to data-rich government contracts develop substantially more commercial AI software. 
Incentivizing private firms to adopt AI that augments workers: Beyond these ongoing efforts to fund AI research, governments have other mechanisms at their disposal to incentivize private firms to invest responsibly in AI. While public research efforts can be used to consciously prioritize AI research that improves worker productivity and encourages a diversity of techniques, governments should also be aware of the set of incentives faced by firms. These include firm business models that promote cutting costs, economic distortions in the tax and regulatory space that increase the cost to firms of using labor relative to capital, and even the “aspirations of researchers” at private firms who are excited and motivated to develop branches of AI that are more suited to automation (Acemoglu 2021). All these channels might push the society towards an 
       
43
undesirable equilibrium in terms of the balance of automation and augmentation AI technologies.
c) Investing in the capacity of regulatory agencies to ensure that AI systems are transparent and fair for workers.
The case studies in part IV showed that AI systems can be made more transparent and fairer for workers in hiring processes and in algorithmic management of workplaces:
Algorithmic hiring: As discussed in part IV of this report, well-intentioned algorithms may still exhibit biases due to unforeseen implementation issues. And, as noted in the case study of AI and hiring, instances of bias are exacerbated when algorithms are being employed at nearly every step of a multistage process. And bias is not the only concern in this space. The black box nature of these tools means that there is a risk of fraud, where firms market their product as providing a service but their customers have no mechanism to determine the accuracy of their claims. Additionally, there is evidence that AI algorithms can learn to effectively collude with each other when setting prices. Existing discrimination, fraud, and antitrust rules and enforcement practices may be insufficient to counteract AI-created fraud and bias. 
Algorithmic management of workplaces: The case study of warehousing also illustrates that workplace algorithms are inherently opaque, are shrouded in industrial secrecy, and are protected by nondisclosure agreements with workers. Even warehouse workers themselves are not always made aware of the software that manages them. This reflects workers’ own relation with corporate technology: workers’ relations with AI are based on information asymmetries, given that workers can only guess the procedures of data extraction and analytics that organize and surveil their activities. 
Firms that utilize AI are not freed of the responsibility of abiding by antifraud, antitrust, and antidiscrimination laws, as well as workplace safety and health regulations. It should be a principal goal of policymakers to make sure that government institutions are well-equipped to investigate and enforce these laws when necessary. Doing so is not a straightforward process. A recent Brookings Institution report highlighted several necessary steps: creating robust standards for algorithmic audits, assuring that regulatory agencies have the access to firms when they need to perform an audit, building technical expertise within agencies, and revising and crafting policies that are aware of the challenges of overseeing algorithm-driven workplaces. The goal is to create the appropriate incentives so that firms develop fairer algorithms that abide by national laws. As noted in the hiring case study, well-designed algorithms have the potential to actual reduce instances of bias, and firms have voiced a desire to use algorithms to address instances of
     
44
discrimination. However, without the proper oversight and regulation, this potential for positive transformative change is unlikely to be realized.
Governments are already moving toward more effective regulation of the impact of AI. In October 2022, Spain launched a pilot regulatory sandbox on AI. This sandbox is a way to connect policymakers with AI developers and adopters. It is expected to generate easy-to-follow best practice guidelines for companies, including small and medium-size enterprises and start-ups, to stimulate the development of and reduce barriers to adopt AI, in compliance with the future European Commission Artificial Intelligence Act. Further, the US has announced an initiative to create a “Bill of Rights” for AI covering many areas, such as consumer protections and equity of opportunity in employment, education, housing and finance, and health care.
  
45
References
Abid A., M. Farooqi, and J. Zhou. 2021. “Large Language Models Associate Muslims with Violence.” Nature Machine Intelligence 3: 461-463. https://www.nature.com/articles/s42256-021-00359-2
Acemoglu, D. 2021. “Harms of AI.” NBER Working Paper 29247. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w29247
Acemoglu D., and D. Autor. 2011. “Skills, Tasks and Technologies: Implications for Employment and Earnings.” Handbook of Labor Economics, 4: 1043-1171. https://doi.org/10.1016/S0169-7218(11)02410-5
Acemoglu, D., D. Autor, J. Hazell, and P. Restrepo. 2022. “Artificial Intelligence and Jobs: Evidence from Online Vacancies.” Journal of Labor Economics, 40. https://doi.org/10.1086/718327
Acemoglu D., G. Anderson, D. Beede et al. 2022. “Automation and the Workforce: A Firm- Level View From the 2019 Annual Business Survey.” Paper presented at the NBER/CRIW conference on Technology, Productivity and Economic Growth, Washington DC, March 2022. http://pascual.scripts.mit.edu/research/abs/
Aiken, E., S. Bellue, D. Karlan, et al. 2022. “Machine Learning and Mobile Phone Data Can Improve the Targeting of Humanitarian Assistance.” NBER Working Paper 29070. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/system/files/working_papers/w29070/w29070.pdf
Albernathy, F., J. Dunlop, J. Hammond, D. Weil. 2000. “Control Your Inventory in a World of Lean Retailing.” Harvard Business Review, November-December, 2000. https://hbr.org/2000/11/control-your-inventory-in-a-world-of-lean-retailing
Center for Democracy and Technology. 2020. “Algorithm-driven Hiring Tools.”
https://cdt.org/wp-content/uploads/2020/12/Full-Text-Algorithm-driven-Hiring-Tools- Innovative-Recruitment-or-Expedited-Disability-Discrimination.pdf
AlphaFold Protein Structure Database. “AlphaFold Protein Structure Database.” https://alphafold.ebi.ac.uk/.
DeepMind. “AlphaGo.” https://www.deepmind.com/research/highlighted-research/alphago
          
46
Ameri, M., L. Schur, M. Adya et al. 2017. “The Disability Employment Puzzle: A Field Experiment on Employer Hiring Behavior.” SAGE Journals ILR Review, 71, no.2. https://doi.org/10.1177/0019793917717474
Autor, D. 2022. “The Labor Market Impact of Technological Change: From Unbridled Enthusiasm to Qualified Optimism to Vast Uncertainty.” NBER Working Paper 30074. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w30074
Autor, D., C. Chin, A. Salomons, and B. Seegmiller. 2022. “New Frontiers: The Origin and Content of New Work 1940-2018.” NBER Working Paper 30389. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w30389
Bennet, M., C. Booker, S. Brown et al. 2020. Letter to Janet Dhillon. United States Senate. https://www.bennet.senate.gov/public/_cache/files/0/a/0a439d4b-e373-4451-84ed- ba333ce6d1dd/672D2E4304D63A04CC3465C3C8BF1D21.letter-to-chair-dhillon.pdf
Beraja, M., A. Kao, D. Yang, N. Yuchtman. 2022. “AI-tocracy.” Working paper, MIT Economics. https://economics.mit.edu/sites/default/files/2022- 09/aitocracy_20220701.pdf
Beraja, M., D. Yang, and N. Yuchtman. 2022. “Data-intensive Innovation and the State: Evidence from AI Firms in China.” Working paper, MIT Economics.
https://economics.mit.edu/sites/default/files/publications/dataproduction_draft_20220110
.pdf
Bessen, J., M. Goos, A. Salomons, W. Van den Berge. 2019. “Automation Reaction – What Happens to Workers at Firms that Automate?” Boston University School of Law, Law and Economics Research Paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3328877
Brown, S. 2021. “Machine Learning, Explained.” MIT Sloan. https://mitsloan.mit.edu/ideas- made-to-matter/machine-learning-explained
Brynjolfsson, E., A. Collis, E. Diewert, F. Eggers, and K. Fox. 2019. “GDP-B: Accounting for the Value of New and Free Goods in the Digital Economy.” NBER Working Paper 25695. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w25695
           
47
Brynjolfsson, E., T. Mitchell, and D. Rock. 2018. “What Can Machines Learn, and What Does it Mean for Occupations and the Economy?” AEA Papers and Proceedings, 108: 43-47. https://doi.org/10.1257/pandp.20181019
Calvano, E., G. Calzolari, V. Denicolo et al. 2020. “Protecting Consumers from Collusive Prices due to AI.” Science, 370, no. 6520: 1040-1042. https://www.science.org/doi/full/10.1126/science.abe3796
Centers for Medicare and Medicaid Services. 2021. “Artificial Intelligence (AI) Health Outcomes Challenge.” Centers for Medicare and Medicaid Services. https://innovation.cms.gov/innovation-models/artificial-intelligence-health-outcomes- challenge
Centers for Medicare and Medicaid Services. 2021. “CMS Selects Winner and Runner-Up in Artificial Intelligence Health Outcomes Challenge.” Newsroom, April 30, 2021. https://www.cms.gov/newsroom/press-releases/cms-selects-winner-and-runner-artificial- intelligence-health-outcomes-challenge
Cockburn, I., R. Henderson, and S. Stern. 2019. “The Impact of Artificial Intelligence on Innovation: An Exploratory Analysis,” In The Economics of Artificial Intelligence: An Agenda, edited by A. Agrawal, J. Gans, and A. Goldfarb, 115-146. Chicago: University of Chicago Press. https://www.nber.org/books-and-chapters/economics-artificial- intelligence-agenda/impact-artificial-intelligence-innovation-exploratory-analysis
OpenAI. “Dall·E 2.” https://openai.com/dall-e-2/ Dean, J. 2019. “The Deep Learning Revolution and its Implications for Computer Architecture
and Chip Design.” Working Paper, Google Research.
https://arxiv.org/ftp/arxiv/papers/1911/1911.05289.pdf
Delfanti, A. 2019. “Machinic Dispossession and Augmented Despotism: Digital Work in an Amazon Warehouse.” SAGE Journals, 23, no.1. https://doi.org/10.1177/1461444819891613
Delfanti, A. 2021. The Warehouse: Workers and Robots at Amazon. London: Pluto Press https://www.jstor.org/stable/j.ctv2114fnm
Engler, A. 2021. “Auditing Employment Algorithms for Discrimination.” The Brookings Institution, Washington, DC. https://www.brookings.edu/research/auditing-employment- algorithms-for-discrimination/
             
48
European Commission. 2020. “European Enterprise Survey on the Use of Technologies Based on Artificial Intelligence: Final Report.” Directorate-General for Communications Networks, Content. https://data.europa.eu/doi/10.2759/759368
European Commission, 2021, “EU-US Trade and Technology Council Inaugural Joint Statement”,
https://ec.europa.eu/commission/presscorner/detail/en/STATEMENT_21_4951
European Commission. 2022. “First Regulatory Sandbox on Artificial Intelligence Presented.”
https://digital-strategy.ec.europa.eu/en/news/first-regulatory-sandbox-artificial-
intelligence-presented
European Commission. 2022. “A European Approach to Artificial Intelligence.” The European Union. https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial- intelligence
European Commission. No date. “Research and Innovation: Artificial Intelligence (AI).” https://research-and-innovation.ec.europa.eu/research-area/industry/key-enabling- technologies/artificial-intelligence-ai_en
Eurostat. 2017. “Glossary: Community Survey on ICT Usage in Enterprises.”
https://ec.europa.eu/eurostat/statisticsexplained/index.php?title=Glossary:Community_su
rvey_on_ICT_usage_in_enterprises
Eurostat. 2021. “Artificial intelligence in EU enterprises.” Eurostat News, April 13, 2021. https://ec.europa.eu/eurostat/web/products-eurostat-news/-/ddn-20210413-1
Eurostat. 2022. “Digital Economy and Society Statistics – Enterprises.” Eurostat Statistics Explained, January 20, 2022. https://ec.europa.eu/eurostat/statistics- explained/index.php?title=Digital_economy_and_society_statistics_-_enterprises
Feigenbaum, J. and D. Gross. 2020. “Answering the Call of Automation: Labor Market Adjusted to the Mechanization of Telephone Operation.” NBER Working Paper 28061. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w28061
Felten, E., M. Raj, R. Seamans. 2019. “The Occupational Impact of Artificial Intelligence: Labor, Skills, and Polarization.” Working Paper, NYU Stern School of Business. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3368605
          
49
Forbes Insights. 2019. “How Pandora Knows What you Want to Hear Next.” Forbes, October 1, 2019. https://www.forbes.com/sites/insights-teradata/2019/10/01/how-pandora-knows- what-you-want-to-hear-next/?sh=1e6d5f613902
Fuller, J. and W. Kerr. 2022. “The Great Resignation Didn’t Start with the Pandemic.” Harvard Business Review, March 23, 2022. https://hbr.org/2022/03/the-great-resignation-didnt- start-with-the-pandemic
Data & Trust Alliance. “Data & Trust Alliance.” https://dataandtrustalliance.org/ Gaynor, M. 2020. “Automation and AI Sound Similar, but May Have Vastly Different Impacts
on the Future of Work.” The Brookings Institution Blog, January 39, 2020.
https://www.brookings.edu/blog/the-avenue/2020/01/29/automation-and-artificial-
intelligence-sound-similar-but-may-have-vastly-different-impacts-on-the-future-of-work/ Gender Shades “Gender Shades: Results.”. http://gendershades.org/overview.html Gent, C. 2018. “The Politics of Algorithmic Management Class: Composition and everyday
struggle in distribution work.” Warwick, University of Warwick.
http://wrap.warwick.ac.uk/132957/
Genz, S., T. Gregory, M. Janser, F. Lehmer, and B. Matthews. 2021. “How do Workers Adjust When Firms Adopt New Technologies?” Working Paper, ZEW – Centre for European Economic Research Discussion Paper no. 21-073. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3949800
Gershgorn, D. 2022. “Companies are on the Hook if Their Hiring Algorithms are Biased.” Quartz, July 20, 2022. https://qz.com/1427621/companies-are-on-the-hook-if-their- hiring-algorithms-are-biased/
Goger, A., A. Parco, E. Vegas. 2022. “Learning and Working in the Digital Age.” The Brookings Institution, Washington DC. https://www.brookings.edu/wp- content/uploads/2022/05/Learning-and-working-in-the-digital-age_FINAL.pdf
Goos M., A. Manning, and A. Salomons. 2014. “Explaining Job Polarization: Routine-Biased Technological Change and Offshoring.” American Economic Review, 104 no. 8: 2509- 2526.
Hardesty, L. 2017. “Explained: Neural Networks.” MIT News Office, April 14, 2017. https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414
              
50
Hoffman, B. 2018. “Competition and Consumer Protection Implications of Algorithms, Artificial Intelligence and Predictive Analytics.” Remarks at Competition and Consumer Protection in the 21st Century. https://www.ftc.gov/system/files/documents/public_statements/1431041/hoffman_- _ai_intro_speech_11-14-18.pdf
Hoffmann, M., L. Nurski. 2021. “What is Holding Back Artificial Intelligence Adoption in Europe?” Bruegel, Nov 30, 2021. https://www.bruegel.org/policy-brief/what-holding- back-artificial-intelligence-adoption-europe
Digital Credentials Consortium. “Digital Credentials Consortium.”
https://digitalcredentials.mit.edu/
Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster, Y. Zhong, and S. Denuyl. 2019. “Unintended Machine Learning Biases as Social Barriers for Persons with Disabilities.” Special Interest Group on Accessible Computing Newsletter, October 2019. http://sigaccess.org/newsletter/2019-10/hutchinson.html
ImageNet. ImageNet Large Scale Visual Recognition Challenge 2010.” https://www.image- net.org/challenges/LSVRC/2010/index.php
ImageNet “ImageNet Large Scale Visual Recognition Challenge 2017.”. https://image- net.org/challenges/LSVRC/2017/results
Indeed. No date. “Pros and Cons of Recruiting Chatbots.” https://www.indeed.com/hire/c/info/pros-and-cons-of-recruiting-chatbots#3
IRS. 2022. “Assistance for Eligible Taxpayers in Setting up or Modifying Payment Plans Now Available.” Internal Revenue Service Newsroom, June 17. https://www.irs.gov/newsroom/irs-expands-voice-bot-options-for-faster-service-less- wait-time
Jumper, J., R. Evans, A. Pritzel, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596: 583-589. https://www.nature.com/articles/s41586-021-03819-2
Katz, L., J. Roth, R. Hendra, and K. Schaberg. 2020. “Why do Sectoral Employment Programs Work? Lessons from WorkAdvance.” NBER Working Paper 28248. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w28248
              
51
Kerry, C. 2020. “Protecting Privacy in an AI-driven World.” The Brookings Institution, Washington DC. https://www.brookings.edu/research/protecting-privacy-in-an-ai-driven- world/
Kleinberg, J., H. Lakkaraju, J. Leskovec, et al. 2018. “Human Decisions and Machine Predictions.” The Quarterly Journal of Economics 133, no. 1: 237-293. https://doi.org/10.1093/qje/qjx032
Koenecke, A., A. Nam, E. Lake, J. Nudell et al. 2020. “Racial disparities in automated speech recognition.” PNAS 117, no. 14: 7684-7689. https://doi.org/10.1073/pnas.1915768117
Krayewski, K. 2022. “How NLP Chatbots Work.” Ultimate AI Blog, April 13, 2022. https://www.ultimate.ai/blog/ai-automation/how-nlp-text-based-chatbots-work
Lambrecht, A., C. Tucker. 2019. “Algorithmic Bias? An Empirical Study of Apparent Gender- Based Discrimination in the Display of STEM Career Ads.” Management Science. https://doi.org/10.1287/mnsc.2018.3093
Martinez, E. and L. Kirchner. 2021. “The Secret Bias Hidden in Mortgage-Approval Algorithms.” The Markup, August 25, 2021. https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval- algorithms
Meta. 2020. “Good Questions, Real Answers: How Does Facebook Use Machine Learning to Deliver Ads?” Announcements, June 11, 2020. https://www.facebook.com/business/news/good-questions-real-answers-how-does- facebook-use-machine-learning-to-deliver-ads
Mitchell, M. 2021. “Why AI is Harder Than We Think.” Working Paper, Santa Fe Institute.
https://arxiv.org/pdf/2104.12871.pdf
Mokyr, J. 2005. “Long-term Economic Growth and the History of Technology”. Handbook of Economic Growth, Volume 1, Part B, 2005, Pages 1113-1180. https://www.sciencedirect.com/science/article/abs/pii/S1574068405010178
National Artificial Intelligence Initiative Office. No Date. “Strategic Pillars: Applications.” https://www.ai.gov/strategic-pillars/applications/
The National Security Commission on Artificial Intelligence. “The National Security Commission on Artificial Intelligence.” https://www.nscai.gov/
             
52
Obermeyer Z., and B. Powers, C. Vogelli, and S. Mullainathan. 2019. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science, 366: 447-453. https://www.science.org/doi/10.1126/science.aax2342
Organization for Economic Cooperation and Development (OECD). 2019. “OECD AI Principles Overview.” https://oecd.ai/en/ai-principles
Organization for Economic Cooperation and Development (OECD). “STAN STructural ANalysis Database.” OECD.
https://www.oecd.org/sti/ind/stanstructuralanalysisdatabase.htm
Organization for Economic Cooperation and Development (OECD). 2022. “Trends and Data Overview.” https://oecd.ai/en/trends-and-data
Olmstead, K. 2017. “Nearly half of Americans Use Digital Voice Assistants mostly on their smartphones.” Pew Research Center Fact-Tank, December 12, 2017. https://www.pewresearch.org/fact-tank/2017/12/12/nearly-half-of-americans-use-digital- voice-assistants-mostly-on-their-smartphones/
Olson, C., and K. Kemery. 2019. “Voice report: From answers to action: customer adoption of voice technology and digital assistants.” Microsoft. https://advertiseonbing- blob.azureedge.net/blob/bingads/media/insight/whitepapers/2019/04%20apr/voice- report/bingads_2019_voicereport.pdf
OpenAI “Overview.”. https://openai.com/api/ Pethokoukis, J. 2016. “What the Story of ATMs and Bank Tellers Reveals About the ‘Rise of the
Robots’ and Jobs.” AEIdeas, June 6, 2016. https://www.aei.org/economics/what-atms-
bank-tellers-rise-robots-and-jobs/
Quillian, L., D. Pager, O. Hexel, and A. Midtboen. 2017. “Meta-analysis of Field Experiments Shows No Change in Racial Discrimination in Hiring Over Time.” Proceedings of the National Academy of Scienecs, 114 no. 41:10870-10875. https://doi.org/10.1073/pnas.1706255114
Raghavan, M., S. Barocas. 2019. “Challenges for Mitigating Bias in Algorithmic Hiring.” The Brookings Institution, Washington DC. https://www.brookings.edu/research/challenges- for-mitigating-bias-in-algorithmic-hiring/
Ramesh, A., M. Pavlov, G. Goh, et al. 2021. “Zero-Shot Text-to-Image Generation.” Working Paper, Cornell University Arxiv. https://arxiv.org/abs/2102.12092
            
53
Ramesh, A., M. Pavlov, G. Goh, and S. Gray. 2021 “Creating Images from Text.”. Open AI Blog, January 25, 2021. https://openai.com/blog/dall-e/
Russakovsky, O., J. Deng, H. Su, et al. 2015. “ImageNet Large Scale Visual Recognition Challenge.” J Comput Vis 115: 211-252. https://doi.org/10.1007/s11263-015-0816-y
Silberg, J., J. Manyika. 2019. “Tackling Bias in Artificial Intelligence (and in Humans).” McKinsey Global Institute, June 6. https://www.mckinsey.com/featured- insights/artificial-intelligence/tackling-bias-in-artificial-intelligence-and-in-humans
Silver, D., J. Schrittwieser, K. Simonyan, et al. 2017. “Mastering the Game of Go Without Human Knowledge.” Nature 550: 354-359. https://doi.org/10.1038/nature24270
Stansbury, A., L. Summers. 2020. “The Declining Worker Power Hypothesis: An Explanation for the Recent Evolution of the American Economy.” The Brookings Institution, Washington DC.https://www.brookings.edu/wp- content/uploads/2020/12/StansburySummers-Final-web.pdf
Textio. “Inclusive Talent Attraction.”. https://textio.com/products/recruiting U.S. Bureau of the Census. 2019. “Annual Business Survey.” https://www.census.gov/programs-surveys/abs/data/tables.html U.S. Department of Transportation. 2022. “The Economic Costs of Freight Transportation.” https://ops.fhwa.dot.gov/freight/freight_analysis/freight_story/costs.htm
Webb, M. 2020. “The Impact of Artificial Intelligence on the Labor Market.” Working Paper, Stanford University. https://www.michaelwebb.co/webb_ai.pdf
Weil, D. 2019. “Preparing for the Future of Work Through Understanding the Present of Work: A Fissured Workplace Perspective.” Testimony before the U.S. House of Representatives Committee on Education and Labor and Subcommittee on Workforce Protections and Subcommittee on Health, Employment, Labor, and Pensions. https://edlabor.house.gov/imo/media/doc/WeilTestimony102319.pdf
Weldon, S. and Liang, Y. 2021. “Translation API Advanced can Translate Business Documents Across 100+ Languages.” Google Cloud Blog, May 12, 2021. https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-ai-translation- services-works-for-documents
The White House. 2022. “Blueprint for an AI Bill of Rights.” The United States Government. https://www.whitehouse.gov/ostp/ai-bill-of-rights/
         
54
The White House. 2022. “Fiscal 2023 Budget Chapter 18: Research And Development”
https://www.whitehouse.gov/wp-content/uploads/2022/04/ap_18_research_fy2023.pdf
The White House. 2021. “U.S. – EU Trade and Technology Council Inaugural Joint Statement.” Briefing Room, September 29, 2021. https://www.whitehouse.gov/briefing- room/statements-releases/2021/09/29/u-s-eu-trade-and-technology-council-inaugural- joint-statement/
Wood, A. 2021. “Algorithmic Management.” European Commission, JRC Technical Report.
https://joint-research-centre.ec.europa.eu/system/files/2021-05/jrc124874.pdf
Zolas N., Z. Kroff, E. Brynjolfsson et al. 2020. “Advanced Technologies Adoption and Use by Firms: Evidence from the Annual Business Survey.” NBER Working Paper 28290. Cambridge, MA: National Bureau of Economic Research. https://www.nber.org/papers/w28290
     
55
",https://www.whitehouse.gov/wp-content/uploads/2022/12/TTC-EC-CEA-AI-Report-12052022-1.pdf
The Role of Artificial Intelligence in Shaping Modern Business Strategies,Rushford Business School,Arbab Khan,"ARTICLE In today’s economy, data is changing the way companies perceive businesses. People and AI are working together to strengthen the capabilities of what humans can do to create value in the current business landscape. Experts are working continuously to predict the use of Artificial Intelligence (AI) at a large scale and add its importance to the global economy.",2023,"In today’s economy, data is changing the way companies perceive businesses. People and AI are working together to strengthen the capabilities of what humans can do to create value in the current business landscape. Experts are working continuously to predict the use of Artificial Intelligence (AI) at a large scale and add its importance to the global economy.
But can AI replace human workers? Intelligent AI systems are already being used in industries and have started to replace human employees. As such systems are being recruited frequently within the organizations, displacement in manufacturing, service, financial, content, and other industries can be seen. As smarter versions of AI are in the phase of development, implementation must consider ethical considerations in job displacement and security aspects. At the Yale CEO Summit, a survey poll was conducted in which 119 CEOs participated. It concluded that 42% of CEOs believe that AI poses a threat to humanity within the next 5-10 years.
While AI can be an intelligent choice for automating repetitive tasks, it must leave enough room for professionals to concentrate more on complex and creative aspects. Instead of focusing on whether AI will replace humans, the discussion should surround how AI and humans should work together to create meaningful and workable business solutions. These two should collaborate to create a long-term symbiotic relationship that not only adds value to businesses but also enriches human experiences, personally and professionally. For instance, in education, AI can streamline admin tasks, allowing educators to concentrate more on creating innovative student-centered approaches and understanding human capabilities where a machine would struggle. The combination of humans and machines will more likely deliver the best results for students.
According to a Pew Research Report on AI and employability, 71 percent of US citizens do not like the use of AI in the financial hiring processes as they think technology is worse than humans. Another report confirms that 62 percent of Americans think that the use of AI affects workers over a period of twenty years. It affects them personally as well.
The growing weaponization of AI has been in the news as there are almost no rules applicable to its usage. It is offering promising opportunities to AI-driven business organizations by allowing them to manage their cyberspace and other data spaces. If AI is absent in large data-driven organizations, then their business models are certainly at risk.
AI systems learn to adapt over time and support aids in strategic planning, enhancing logistics and inventory levels at the workplace.
Why should businesses opt for AI-driven Systems?
AI is finally getting the attention it deserves, but what is the fuss about it? Most of us are aware of the fundamental algorithms that tweaked social media and other content platforms. AI, being the operator, automatically adjusts to the requirements of the modern business and delivers results in no time.
AI is already in many places and transforming businesses in many other areas of the world. Bringing about potential adjustments and improvements across various industries, AI is capable of detecting and preventing threats and vulnerabilities within organizations. 79% of CEOs believe that AI investments significantly enhance business efficiencies.
Businesses can now explore AI-powered connections to upgrade their operations and welcome intellectual gains. It assures stronger privacy and cybersecurity regulations with limited effort. AI is capable of helping humans work more like a super-human in many ways. It empowers a continuous learning approach to accomplish things at a much faster pace than no one can expect.
How far can AI go to influence modern business strategies?
Marketing core activities are well understood by AI, enabling it to work effectively on customer needs and provide them with choices for the right products and services. AI contributes the most incredible value in identifying patterns and generating actional insights for organizations to drive their strategic actions.
AI’s current potential and capabilities are applicable in optimizing supply chain management and improving risk assessment options. There are operational efficiencies that drive sustainable growth and embrace the competitive advantage in the business market.
For example, ChatGPT has made headlines as one of the latest advancements of Open AI that is designed for more interactive and dynamic conversations to engage in modern developments of business strategies.
As AI is contributing to transforming every aspect of modern business strategies, companies have started to embrace its use in unlocking several untapped business opportunities.
A survey conducted by Deloitte confirms that more than half of CEOs in the global business landscape have been experimenting with generative AI, and more than four-fifths say that it helps them increase the efficiency of their operations. They are able to create text, images, animations, 3D and 2D content, medical imaging analysis, fraud detection, and more.
Well, another study by McKinsey further reveals that AI adoption has doubled since 2017 and leveled off at just over fifty percent in the past five years.
The transformational impact of AI
Be it production or marketing, AI is set to build relationships as a part of emerging technology businesses across the globe.
Business models are evolving and even undergoing a transformational change like never before. In the journey, AI can create a profound impact on the relative balance of strategies and decision-making approaches of entrepreneurs. Its capability has spread across the horizons of developed to developing nations and continues to apply to bring unique possibilities.
AI is a wide-ranging tool that allows organizations to think before integrating with unknown information and analyze it to use its insights for strategic decision-making.
The modern business approach is nowadays regulated as per broad AI principles rather than just updating only the specific algorithms. It encourages the new models of digital education and AI workforce to have the skills required and enact them according to them.
AI systems have the ability to self-learn and make decisions.
In the finance sector, AI is predominantly used to make investments. AI machines are used for spot trading inefficiencies or market differentials to make money according to the desires of the investor. AI application in fraud detection is helping financial institutions find defaulters and fraudulent activities happening in large organizations.
Well, this is just the tip of the iceberg. AI is everywhere, be it education, hospitality, healthcare, tourism, and more. Still, the essence is the same- it should not replace, but collaborate with humans to create a win-win condition.
Future of AI
It seems AI will truly help humans to augment their power of knowledge, but no one knows if it is for good or bad. In business organizations, upcoming AI platforms will indeed leverage the choice and benefits toward profit maximization. With competition, it will be developed to a much better form and rate to help make strategic decisions in the modern business world.
AI is a powerful tool to have interactive capabilities required for refinement and testing, and CEOs will never look back at the business the same way after realizing its potential at full scale. For example, in a healthcare organization, AI can be used as a diagnostic intelligent machine to organize patient portfolios into segments and deploy hundreds of ways to tackle the problem. Companies that have deep data on their portfolio to analyze must look forward to AI as a solution to gain granular insights.
The use of AI is embedded in our daily lives, and it will continue to gain importance regarding the strategic business environment. It is a fundamental component of business profits and the automation of valuable decisions to offer great strategic potential. AI shall be used as a powerful tool to execute the best possible actions based on strategic decision-making at the business organizations.
As a final word…   
CEOs continue to explore the use of AI in analyzing their strategic decisions to ensure prompt success in their organization. Despite a variety of challenges and uncertainties in the economic world, AI is capable of analyzing it all and offering opportunities for growth to business organizations. Modern businesses remain focused and continue to explore and invest in emerging technologies, but uncertainties and economic concerns continue to disrupt them. However, the rapid adoption of AI-enabled decision-making and strategic approaches are influencing them continuously. The majority of organizations are now confident and able to sustain the level of growth with the help of AI-powered opportunities.
Businesses are beginning to understand the implications of AI-powered systems that go far beyond the complicated and complex challenges. A different level of AI strategy is helping organizations improve their intelligence capabilities to engage with customers and offer them trust and resilience. Innovation in the digital infrastructure is changing the course of business opportunities and transforming current business models.
With this, it is plausible to imagine surprising transformations in business processes, but researching the pros and cons and managing risks and ethical concerns should also be a priority",https://rushford.ch/insights/the-role-of-artificial-intelligence-in-shaping-modern-business-strategies/
Do Strategic Human Resources and Artificial Intelligence Help to Make Organisations More Sustainable? Evidence from Non-Governmental Organisations,Journal of Sustainability ,"Amal Alnamrouti, Husam Rjoub, Hale Ozgit","ARTICLE This study explores how organizational learning and corporate social responsibility impact the sustainability of non-governmental organizations' performance, using data from United Nations and Jordanian NGOs, and finds that strategic resource management practices and artificial intelligence significantly mediate this relationship, offering practical guidelines for enhancing NGO efficiency and sustainability.",2022,"Uncertainty and a lack of stability are among the difficulties non-governmental organisations face. However, certain strategies for ensuring their performance’s sustainability have not been empirically demonstrated in the literature. Using strategic resource management practises and artificial intelligence, this study examines the effect of organisational learning and corporate social responsibility on the sustainability of non-governmental organisations’ performance. The survey gathered data from 171 participants representing 21 United Nations organisations and 70 non-governmental organisations in Jordan to accomplish this goal. The data were analysed using WarpPLS and PLS-SEM. The study demonstrates that organisational learning, artificial intelligence, strategic human resource management practises, and corporate social responsibility all contribute to the long-term viability of non-governmental organisations. Furthermore, the study discovered that strategic resource management practises and artificial intelligence significantly mediate the relationship between organisational learning and sustainable organisational performance on the one hand, and corporate social responsibility on the other. Finally, the study provides theoretical and practical guidance on how to apply the findings to assist non-profit organisations’ management in utilising organisational learning, corporate social responsibility, artificial intelligence, and strategic resource management practices to help them run their internal operations in a more efficient and sustainable manner over time.
Keywords: artificial intelligence; corporate social responsibility; sustainability; non-governmental organisations; sustainable performance; PLS-SEM; Jordan


1. Introduction
According to [1] the dynamic and competitive nature of today’s business environment has elevated organisational learning (OL) to a core capability of high-performing organisations and a primary component of corporate strategy. The literature indicates that promoting OL has a significant impact on upgrading and transforming the national economy, as OL demonstrates efforts to develop knowledge assets and proposes practical methods for managing them [2]. Additionally, Refs. [3,4] suggested that learning is the “next competitive advantage source” or “only competitive advantage source” [4,5], and, as [6] suggested, “the cornerstone to a company’s future success”, making it imperative and critical for both scholars. Meanwhile, society has expressed concern about the devastation caused by commercial activities [7], and some businesses have expressed concern about the extent to which all actors are treated ethically and responsibly, as well as the impact of the “stakeholder theory” on organisational performance [6]. Despite the obvious financial costs associated with balancing the economic interests of stakeholders with the ethical, environmental, and social concerns of other stakeholders, some leading corporate executives are increasing their corporate social responsibility (CSR) investments [8,9]).
Corporate executives believed that the benefits of CSR practises such as increased employee morale, increased customer loyalty, and other forms of social capital outweighed the costs of socially responsible actions, according to [10]. Nonetheless, several researchers assert that a firm’s performance in today’s complex, irregular, and discontinuous business environment has necessitated the search for a strategy to ensure a “competitive edge” that encompasses not only product, innovation, or resources, but also the ability to generate novel and useful knowledge [9,11]. Additionally, Ref. [12] stated that “to survive and grow, modern businesses must learn faster and more efficiently than competitors.” According to [13], organisational learning (OL) is “a process that lays the groundwork for the development of a knowledge-based economy”. Meanwhile, studies indicate that “intellectual capital and knowledge” are required in the modern world, rather than “physical capital” [14,15], as are “information technologies” (ITs), which have permeated professional activities, causing disruption and influencing all critical operations and procedures [16]. Ref. [17] asserts that by integrating business processes and information technology, a visible effect on the business ecosystem, particularly the nexus between firms and their prospects, customers, and partners, can be achieved. Additionally, information technology is critical for advancing a business’s processes and operations. According to [18,19], artificial intelligence (AI) is still the most important application of information technology in the modern world due to its rapid growth.
As revealed by the literature review (see Table 1), numerous previous studies attempted to determine the effect of OL on organisational performance [20,21,22,23]. Meanwhile, the findings have been inconsistent, emphasising the importance of identifying the mediating variables to ensure more effective OL promotion. This position is consistent with [24] conclusion that the mechanism by which “organisational learning” contributes to “organisational performance” (OP) requires additional theory. Additionally, internal procedures have been implemented to improve the performance of “firm-level resources” and “capacity”. In contrast to [25], the employee is viewed as a critical source of information for achieving sustainable organisational performance (SOP), which can be accomplished only through “strategic human resource management” (SHRM). Therefore, according to this research, SHRM should be included in the study because it may collaborate with OL and CSR to achieve SOP, making it a possible moderator of the effect of OL and CSR on SOP.
According to [25], “SHRM is a management strategy that contributes to an organization’s performance improvement through the systematic integration of several best human resource practises”. Meanwhile, while successful implementation of OL and CSR requires the support of an internal managerial approach, the importance of this approach has not been fully recognised [4,25]. The majority of research has concentrated on the factors that contribute to the success of for-profit businesses, with less attention paid to non-governmental organisations (NGOs).
On the other hand, Refs. [12,26] define NGOs as autonomous organisations or institutions whose primary mission is to volunteer in civil society in order to counteract the state’s failures. Non-governmental organisations face a number of obstacles to success, including a lack of stability and uncertainty [27]. NGOs must effectively manage their resources [28]. These are the organisational structures that enable two distinct goals to be pursued: individual (self-realisation) and social (satisfaction and responsibility) [29]. This approach places a premium on sustainability because it necessitates a delicate balance of resources: funding, new partnerships, new revenue streams, professional human resources, and evaluation and control systems [30]. Non-governmental organisations experiment in novel ways [31]. The relationship between organisational innovation and sustainability is critical for ensuring sustainable development. Concerns about sustainability serve as a catalyst for novel, innovative approaches [32]. Thus, the literature indicates that achieving organisational goals for sustainable development requires innovation [32], which is also true for NGOs. Given the preceding, the increased emphasis on organisational performance by various stakeholders in order to maintain a sustainable competitive edge in the markets in which they operate, and the fact that various determinants of organisational performance have been investigated in the literature, the purpose of this research is to examine OL and CSR as drivers of standard operating procedures (SOP) in non-governmental organisations (NGOs) through the lens of strategic human resource management (SHRM) and artificial intelligence (AI), both of which have received little attention in the literature. Thus, the study contributes to the body of knowledge by examining the role of organisational learning, artificial intelligence, SHRM practises, and CRS in promoting sustainable organisational performance in non-governmental organisations. Additionally, the study’s findings have theoretical and practical implications for non-governmental organisation (NGO) management in terms of enhancing sustainability performance.
The following sections comprise the remainder of the paper: the next section discusses how the study’s major variables were conceptualised and the pertinent prior research that resulted in the development of study hypotheses. Next, the methodology section delves into the data collection and source and the analysis method. Finally, the data analysis and the conclusion discuss the findings and implications.
Table 1. Prior research findings related to the constructs observed in this study.


2. Literature Review and Hypotheses Development
2.1. Organisational Learning (OL)
In recent years, the OL concept has garnered considerable attention from researchers and practitioners seeking to improve their organisations [35]. Its application in theory is regarded as a dynamic concept, emphasising the continuous changes that characterise firms [35]. The term “organisational learning” was coined in 1978 by [36], who initially defined it as “the practise of enhancing actions by gaining a better grasp of the situation”. Other authors later defined it as “the practise of enhancing actions by gaining a better grasp of the situation” [37]. Ref. [38] define OL as “the capacity of an organization’s capability or procedures to sustain or increase performance based on past performance”, while another study defines it as “the processes by which firms develop, supplement, and organise the knowledge and procedures that surround their activities, as well as adapting or developing firm efficiency through improved utilisation of their workforce’s general skills” [39]. Additionally, Ref. [40] defined OL as “an organization’s capacity to process knowledge—that is, to create, absorb, transmit, and integrate knowledge—as well as to adapt its behaviour to the current cognitive environment in order to improve performance”. In contrast to Sanzo et al. (2012), this article views OL as a highly active process in which individuals create, consume, and integrate knowledge based on information obtained from external sources in order to improve the firm’s resources and capabilities.
According to [23,41], OL is more than the sum of individual knowledge. According [41], the organisation creates a cohesive system and establishes organisational routines by allowing members to acquire, interpret, and distribute information. As a result, OL has been recognised as a critical component of enabling organisations to achieve competitive advantages and improve performance [5]. Additionally, Ref. [42] asserted that OL can help an organisation broaden its knowledge base and strengthen the talents and skills that promote creative thinking and behaviour.
2.2. Corporate Social Responsibility (CSR)
CSR is regarded as a significant concept in the literature from theoretical and practical perspectives, prompting several attempts by scholars to define it [43]. Numerous arguments have been advanced, and certain factors have been agreed upon as defining CSR. Refs. [44,45] defined CSR as a business practise or activity that ensures an organisation’s compliance with multiple obligations to various stakeholders, including employees, shareholders, customers, the environment, and local communities regarding the firm’s business activities and procedures. Refs. [44,46] asserts that this obligation encompasses both the philanthropic and ethical, economic, and legal dimensions of the social expectations that an organisation must meet. There is no doubt that CSR as a concept is a non-starter among academics and practitioners, as a consensus definition appears to be lacking in the literature. Ref. [47] note that the term “corporate social responsibility” is used differently in different organisations, including “corporate social responsiveness”, “ethical business practises”, “corporate citizenship”, “corporate sustainable business practises”, and “stakeholder management”. According to [48], CSR is “business’s commitment to sustainable economic development through collaboration with employees, their families, the local community, and society to improve people’s quality of life”.
Additionally, CRS was defined as a concept that enables businesses to incorporate social and environmental concerns into their operations and interactions with other stakeholders [49]. Similar to this definition, the “World Business Council for Sustainable Development” (CSR) defines CSR as a business’s ongoing emphasis on ethical conduct in order to advance economic growth while improving the living standards of employees, dependents, the local community, and society as a whole [50]. Similarly, according to [51], CSR is defined as “a comprehensive spectrum of fundamentals that firms are expected to recognise and embody in their operations”. This includes “regard for life rights, suppliers, fair worker treatment, consumers, being a responsible corporate citizen in the communities in which they operate, and environmental protection”.
From a global perspective, Ref. [52] definition, which implies a multidimensional approach to CSR practises, suggests that CSR is related to the expectations of society’s constituent groups regarding the organisational behaviour that the firm must identify and attempt to conform to. Furthermore, according to [53], successful company management now requires high attention to several aspects of business performance and strategic engagement of internal and external stakeholders. In keeping with this perspective, this research considers the internal stakeholder, as we believe that CSR activities focused on employees have the greatest potential to significantly impact the firm. Internally, CSR is concerned with health and safety, social capital investment, and fair and empowered human resource management [53].
2.3. Sustainable Organisational Performance (SOP)
It is impossible to overstate an organisation’s contribution to a nation’s wealth. As a result, successful managers constantly look for ways to grow, enhance, and sustain their business, particularly in a developing economy. Successful managers have mastered the art of problem solving in order for their organisations to overcome and survive obstacles and progress toward long-term viability, profitability, and progress. As a result, Refs. [54,55] observed an increasing interest among academics and practitioners in examining organisational performance aspects, taking into account the critical determinant factors [7,15,39,56,57]. Meanwhile, some studies examine the effects of human resources, strategy, and operations on traditional organisational performance, with relatively few examining organisational performance [55,58]. According to [55], performance can be defined in several ways: “the ability to accomplish something with a specific goal in mind; the outcome of an activity; the capacity to accomplish or the possibility of accomplishing a goal”. This position is consistent with [59] findings, who concluded that performance can be interpreted differently depending on the individual. This can be accomplished in various ways when assessing a business’s performance. Ref. [60] defines performance as the sum of an individual’s actions and accomplishments compared to competitors.
Based on this performance description, an organisation’s performance can be evaluated using non-financial and financial factors [61]. According to the literature, Ref. [62] identified five major performance classification factors: market/customer, financial performance, human resource development, process, and years to come. Nonetheless, product market, shareholder, and financial results are the most frequently used metrics for assessing an organisation’s performance and ability to meet its goals and objectives [54]. Despite this [63] identified three distinct outcomes for organisational performance: organisational outcomes such as quality, efficiency, and production; human resource outcomes such as satisfaction, behaviours, commitment, and attitudes; and financial outcomes such as profit and market share. Given these factors, Ref. [10] define sustainable organisational performance (SOP) as “the foundation’s capacity to meet the needs of its stakeholders while also enhancing investment and management plans and strategies to ensure future earnings, a sustainable environment, and social welfare”. As a result, Ref. [64] asserted that sustainable organisational performance occurs when management can develop strategies to increase market share, talent, stakeholder profit, and so on, while simultaneously lowering operational costs and employee turnover. The study concludes by stating that an organisation is deemed to be sustainable when it strives to maintain a low level of external danger and internal change [64].
2.4. Strategic Human Resource Management (SHRM)
In today’s business world, human capital is regarded as a significant resource due to the employee’s possession of tacit knowledge, which an organisation can use to achieve a competitive edge in the market in which they operate [65,66]. As proposed in the literature, the “valuable, rare, imitable, and non-substitutable” (VRIN) requirements of a competitive resource (Barney, 1991) were met by the described attribute of human capital, thereby establishing SHRM as a critical characteristic of today’s competitive enterprises. Ref. [67] define SHRM as “a management attitude that ensures human resources are used to add value to the business by providing a competitive edge, thereby achieving the organization’s goals, purpose, and vision”. SHRM as a concept dates back to the 1990s, with an emphasis on an integrative, value-driven strategy and proactive approach to human resource management (HRM) that emphasises issues such as aligning HRM practises with firm strategic goals and integrating HR processes into senior leadership [68]; performance evaluation and the value added to firm performance by HRM [67,68]. Additionally, SHRM practise is defined as “the structure of plans that the firm’s human resources department implements to accomplish the organization’s objectives” [69]. According to some studies, SHRM practises establish a link between business requirements and firm activity through their application approach [70,71]. Additionally, the practise unifies and guides employees who are not aligned with the organisation’s business strategies [72], while also assisting the organisation in achieving a competitive advantage [73,74]. Similarly, the literature suggests that when SHRM practises are implemented in a business, they assist the company in gaining an unmatched competitive edge over competitors.
According to the description of SHRM practises, additional internal factors can affect a firm’s performance. This position is consistent with the findings of [70], who asserted that SHRM should be viewed as a strategy implemented to motivate, enhance, and reduce employee turnover to ensure effective implementation and success of both the employee and the organisation. This position is consistent with [75] study, which demonstrates that human resource strategies significantly impact significant positive organisational outcomes. Additionally, the literature indicates that adopting and integrating strategy decisions into human resource management systems is a significant distinction between SHRM and HRM [55,76,77]. Based on a description of SHRM and SHRM practises, this study argues that SHRM is a critical and valuable asset to every organisation, which is believed to be scarce, unique, and occasionally difficult to replicate or substitute, and one that an organisation can leverage to achieve sustainable performance.
2.5. Artificial Intelligence (AI)
Without a doubt, AI is the most amazing application of “information technology” (IT) [15], a technology believed to have advanced tremendously over the last decades [15,18,19]. Ref. [15] define AI as “a collection of beliefs and strategies for developing robots that mimic human intelligence”. By comparison, Ref. [78] discovered that it is a frequently used term to refer to the use of a computer to mimic intelligent behaviour with minimal human intervention. In summary, technological devices to replicate human cognitive abilities to achieve a firm objective autonomously, despite any obstacles encountered, are best described as artificial intelligence [15,78]). Refs. [79,80] The rapid growth of AI is due to significant advancements in computer computational capability and access to massive data sets [79,80]. These studies discovered that AI and its technologies significantly impacted how businesses and firms operate. To put it succinctly, the entire structure of artificial intelligence has altered the way businesses operate and interact with their environments. According to [19,33,81], AI takes a novel approach to information management, which represents both a challenge and a tremendous opportunity for businesses; however, realising this potential requires a shift in culture, mindset, and capabilities. According to these findings, Refs. [79,82] recommend deploying AI throughout an organisation’s value chain, allowing for the integration of all elements such as research and development, preservation, marketing and sales, operations, production scheduling, demand and prediction, and services. With AI as a primary driver of growth, the literature highlights several notable accomplishments that the deployment of AI can enable an organisation to achieve. Refs. [79,82], for example, identified improvements in operational efficiency, maintenance and supply chain operations, customer experience optimisation and enhancement, product and service development, and the item recommendation process. Similarly, Refs. [79,82] believe that AI will enable faster and more automatic adaptation to changing market conditions, develop new business models, and optimise the supply–demand nexus and more efficient forecasting and planning capacity.
Additionally, according to [79,80], deploying AI enables fraud detection, automates threat intelligence, information systems, and sales process optimisation. Refs. [83,84]), in addition to pharmacological vigilance, suggested the diagnosis and treatment of pathologies, the prediction of disease and its evolution, the promotion of individualised treatments, and assisting people in making decisions about diagnosis and prevention through epidemic anticipation. Finally, Refs. [33,81] concluded that deploying AI within an organisation would aid the organisation in automating inquiries and advice related to quality management, supply chain management, and fleet asset management.
2.6. Development of Hypotheses
2.6.1. Organisational Learning and Sustainable Organisational Performance
According to [12], organisational learning (OL) remains a significant source of competitive advantage in the context of strategic management, which is believed to be necessary for a firm to ensure the sustainability of its performance. According to [85], “the ability to learn faster than your competitors may be your only competitive advantage”. “The rate at which individuals and groups acquire knowledge may become the sole source of sustained competitive advantage, especially in knowledge-intensive industries” [86]. Numerous interpretations of OL imply a theoretical connection between OL and firm performance [38,87,88,89]. According to these studies’ definitions, OL directly and indirectly contributes to organisational performance (OP). The relationship between OL and OP has been investigated empirically in a few studies [35,90,91,92,93,94,95]. For example, in their study on the impact of OL innovation on firm financial performance, Ref. [90] discovered a positive, significant relationship between the two variables. The finding is consistent with [94], who established that cohesion of teamwork and OL improves OP in Spanish firms. This finding is consistent with the findings of [92], who established a positive and significant effect of OL on OP in Spanish firms, as well as [96]), who discovered a significant relationship between OL and organisational trust, continuous improvement, and OP. Recent studies, such as [35,95], confirmed a positive and statistically significant relationship between OL and OP. Meanwhile, some studies reported no relationship between OP and OL [97], while others reported a tenuous relationship between the two variables [93,97,98]. According to the literature, previous studies have concentrated on the contribution of OL to conventional organisational performance, with an absence of research on the possible contribution of OL to organisational sustainability, particularly in an NGO. Given these findings, the current study hypothesised a direct link between OL and an NGO’s long-term viability.
Hypothesis 1 (H1).  There is a direct interrelationship between OL and SOP.
2.6.2. Organisational Learning, Strategic Human Resource Management, Artificial Intelligence, and Sustainable Organisational Performance
According to [3] learning is viewed as “the next source of sustainable competitive advantage” or “the sole source of competitive advantage” and as the foundation for an organisation’s future success [4]. Meanwhile, Ref. [99] suggested that OL implementation could be successful if internal managerial approaches were utilised. This internal management mechanism includes a systematic training process, establishing firm procedures, and a non-rigid work design that entails using a holistic approach to develop human resource strategies that are vertically linked to the company plan. SHRM is a human resource management transformation strategy that focuses on the efficiency of human resource management systems within a business, emphasising human resource collaboration to boost competitive advantage [100,101]. Effective implementation of this strategy requires some input, such as organisational learning, particularly in today’s world of increasing competition. Ref. [4] assert that promoting OL demonstrates an effort to create and manage knowledge assets. This demonstrates that OL has the potential to facilitate effective strategic human resource management. Although studies have established the effect of OL on OP [20,23], Ref. [24] asserts that the mechanism by which OL promotes OP requires further exploration. As a result, it becomes critical to investigate possible mediating variables between OL and OP in response to [21]) contention that their relationship is indirect. Given the growing emphasis on internal operations to enhance the outcomes of firm-level resources and capabilities such as OL, we propose that SHRM, a popular human resources management strategy approach, be included because it integrates with OL and has a synergetic effect on the organisation’s long-term performance. Additionally, artificial intelligence is argued to be a mediating variable in this study due to its potential to indispensable factors in firms’ development processes, optimisation, and operational flexibility [102]. According to [103], AI increases data processing speed, reducing bottlenecks, and increasing overall operational efficiency by decreasing the time required to process data.
The theoretical understanding of OL’s influence on OP appears to be imprecise. However, empirical evidence indicates that OL has a significant effect on OP. Meanwhile, the findings in the literature are inconsistent, particularly regarding the strength of the relationship and the OL and a few selected performance indicators. For example, Refs. [104,105] demonstrated some mediating factors in the relationship between OL and OP. Additionally, Refs. [106,107] demonstrated that OL indirectly affects business performance. This indicates that certain variables may influence the strength of the relationship; thus, we hypothesised the mediating effect of SHRM and AI and developed the following hypotheses:
Hypothesis 2 (H2).  OL is directly related to SHRM.
Hypothesis 3 (H3).  There is a direct relationship between OL and AI.
Hypothesis 4 (H4).  The relationship between OL and SOP is partially mediated by (a) SHRM and (b) artificial intelligence.
2.6.3. Artificial Intelligence and Sustainable Organisational Performance
Furthermore, Refs. [78,108] observed that in today’s competitive business environment, every organisation requires the competence of its employees in developing, managing, and implementing intelligent technology that will assist the firm’s technical processes in pursuing green initiatives. Similarly, Ref. [109] opined that a firm’s growth is contingent upon its ability to acquire and implement AI knowledge in today’s contemporary business world. Similarly, Refs. [110,111] observed that numerous task processes can be conceptualised, promoted, and supplemented digitally by collaborative teams working to advance the firm’s goals and environmental sustainability objectives. On the other hand, Ref. [108] advocates for a continuous disregard for the digital relevance of any firm’s nature of work. As a result, such a business may eventually become irrelevant.
Meanwhile, the literature demonstrates that digitisation has become a global phenomenon promoting shifting labour needs [111]. Notably, with the assistance of AI, several traditionally difficult jobs can be advanced digitally, and [112] observed that the primary digital tasks handled by unique interdependent employees can be nurtured more effectively and efficiently. Furthermore, according to studies, with the assistance of AI, employees’ interconnectedness on digital tasks may influence job outcomes and increase productivity [113,114]. In light of this, we propose the following hypothesis:
Hypothesis 5 (H5).  There is a direct interrelationship between AI and SOP.
2.6.4. Strategic Human Resource Management and Sustainable Organisational Performance
SHRM practises are defined as those decisions and actions on the management of employees across all levels of the organisation and those pertaining to implementing strategies aimed at achieving a sustainable competitive advantage [67,115]). This has resulted in an increased focus on the concept by researchers to better understand both the antecedents and outcomes. Researchers have attempted to explain how SHRM affects form performance. For example, Ref. [116] established a positive and significant relationship between SHRM practises and financial and operational performance of an organisation. Ref. [117] discovered that training and compensation practises result in improved employee performance, resulting in improved organisational performance. Ref. [118] conducted a study in Kenya that established a positive and significant relationship between human resource management and organisational performance. This finding is consistent with several other studies, which have established similar findings in their studies [119,120].
Meanwhile, these studies have been primarily focused on for-profit organisations, leaving scant research on non-profit organisations. Nonetheless, a recent study by [67] examined the impact of SHRM practises on the perceived financial performance of NGOs in Bangladesh and discovered that SHRM practises have a positive effect on perceived financial performance. Given the preceding literature and the fact that NGOs are not profit-driven, it is reasonable to assume that their strategic management must be designed so that the organisation’s goals are achieved. As a result, we hypothesised a direct interrelationship between SHRM and SOP.
Hypothesis 6 (H6).  There is a direct interrelationship between SHRM and SOP.
2.6.5. Corporate Social Responsibility, Strategic Human Resource Management, Artificial Intelligence, and Sustainable Organisational Performance
Additionally, CSR is identified as affecting organisational performance [43], owing to its theoretical and practical significance. CSR is generally defined as organisational practises or activities that ensure the fulfilment of a firm’s multiple obligations to various stakeholders, including shareholders, employees, customers, the environment, and the local community [44,45]. CSR has become a requirement in recent years due to the goodwill generated by CSR and the interconnectedness of corporate firms and the environment in which they operate. As a result, several attempts have been made to explain the relationship between CSR and OP in terms of achieving competitive advantage [34,57,87,121,122,123,124,125]. For example, Ref. [124] examined the impact of CSR on firm performance and discovered a significant effect, which [125] corroborate.
Similarly, Ref. [121] examined the effect of CSR on OP and established that a firm’s public image contributes to CSR, which increases profitability and performance. This position was bolstered by [122], who examined the impact of CSR on the profitability of Nigerian banks and discovered a significant correlation between bank profitability and CSR expenditure. Furthermore, Ref. [121] examined the relationship between CSR and Nigerian firm performance and discovered a positive and significant correlation between the two variables, while [34]) examined the relationship between the two variables in the Jordanian telecommunications sector and discovered a positive effect of CSR on the non-financial and financial performance of telecommunication firms in Jordan. This conclusion was confirmed by [57], who conducted a similar study in the Iranian petroleum industry, and Singh (2021), who examined the Malaysian public sector. Meanwhile, Ref. [87] discovered a strong correlation between CSR, corporate strategy, and competitive advantage in their study.
Additionally, some studies demonstrated the potential benefits of CSR initiatives on organisational outcomes such as commitment, motivation, employee morale, recruitment, loyalty, and turnover, as well as attractiveness to current and prospective employees [43,53,126,127]. According to the reviewed literature, numerous attempts have been made to explain the relationship between CSR and OP. However, in today’s new normal, when competition is unpredictable, this is no longer true, and CSR has the potential to influence an organisation’s growth, visibility, and sustainability.
Meanwhile, the mechanism by which CSR can ensure the long-term effectiveness of organisational performance has not been thoroughly investigated. As a result, we argue for the inclusion of SHRM and AI in discussing the relationship between CSR and long-term organisational performance. As a result, we propose the following hypotheses:
Hypothesis 7 (H7).  There is a direct interrelationship between CSR and SOP.
Hypothesis 8 (H8).  CSR is directly related to SHRM.
Hypothesis 9 (H9).  CSR is directly related to artificial intelligence.
Hypothesis 10 (H10).  The relationship between CSR and SOP is partially mediated by (a) SHRM and (b) artificial intelligence.
3. Research Methodology
As illustrated in Figure 1, this study’s conceptual research model demonstrates the relationship between organisational learning (OL), corporate social responsibility (CSR), strategic human resource management (SHRM), artificial intelligence (AI), and sustained organisational performance (SOP). The model establishes a direct relationship between OL and SOP, SHRM, and AI; it also evaluates the direct relationship between AI and SOP, SHRM, and SOP; and it concludes by examining the impact of CSR on SOP, SHRM, and AI. Additionally, this study hypothesised that SHRM and AI plays a moderating role in the relationship between OL and SOP on the one hand, and the relationship between CSR and SOP on the other.


Figure 1. Conceptual research model.
3.1. Items Measurement
The questionnaire consists of five constructs, each measured by 59 items and graded on a five-point Likert scale ranging from strongly disagree to strongly agree. The materials for this study were obtained through previous research. A total of 6 questions adapted and modified from [25] were used to assess strategic human resource management, 6 items were used to assess artificial intelligence [15], 16 items were used to assess organisational learning [40], 20 items were used to assess CSR [28], and 11 items were used to assess organisational performance [128].
3.2. Data Collection and Sample Size
The study’s sample included human resource professionals from Jordan’s 21 United Nations agencies and 70 non-governmental organisations (NGOs). Thus, a sample size of five human resources employees from each of the 21 UN organisations in Jordan, 105 responses, and 70 from non-governmental organisations was determined. Meanwhile, due to the global pandemic currently ravaging the world, many organisations could not distribute questionnaires in person. Instead, an online survey was used to distribute the questionnaires. Before distributing the questionnaire, an official letter was sent to the heads of all UN organisations and non-governmental organisations in Jordan outlining the study’s purpose and requesting permission to collect data.
Additionally, the current investigation adheres to all widely accepted and recognised ethical and approved research protocols. To begin, they ensured the participants’ confidentiality and informed them that participation in the study was voluntary but encouraged them to do so. They were then sent a link to the questionnaire, which they were instructed to distribute to the target employees.
4. Results and Discussions
SPSS was used to prepare and analyse the data for this study in order to determine the significance of the proposed connections between constructs. The statistical tool “WarpPLS 7.0” was chosen to analyse the structure of this research model in light of [129] research, which stated that the statistical tool is “effective for concurrently analysing non-linear and linear associations”. Additionally, PLS-SEM is ideal for ensuring efficacy in assessing the relationship between constructs and producing findings that accurately reflect real-world complexity. Additionally, it is effective when the sample size is small, as this study’s sample size was. Finally, the approach takes the non-normality of the data distribution into account.
4.1. Model Measurements Assessment
Table 2 summarises the results of the model measure evaluation. According to Table 1, the loadings for items about organisational learning (OL), strategic human resource management (SHRM), artificial intelligence (AI), corporate social responsibility (CSR), and standard operating procedures (SOP) were all greater than the 0.5 thresholds, except for a few items with loadings less than 0.40 that were excluded from further estimation. Additionally, at the 0.001 level of significance, the p values associated with these loadings are significant. This finding is consistent with numerous studies [130,131]) demonstrating a high degree of convergent validity for the instrument used to measure the constructs. Additionally, both the Cronbach alpha and composite reliability values for SHRM practises (0.918 and 0.934), AI (0.868 and 0.899), OL (0.890 and 0.913), CSR (0.907 and 0.924), and SOP (0.867 and 0.898) were greater than the suggested threshold value of 0.7, indicating that the instrument of measurement is reliable (Kock, 2014). Additionally, the average variance extracted values for SHRM practises (0.670), AI (0.562), OL (0.572), CSR (0.555), and SOP (0.562) are all greater than the recommended threshold value of 0.5 [130,131], indicating that the measurement has a satisfactory level of internal consistency. Finally, the FVIF values for SHRM (1.926), AI (1.865), OL (2.344), CSR (2.391), and SOP (2.810) are all less than the suggested threshold of 3.3. According to [132], the FVIF coefficient is a “model-wide assessment of multicollinearity that is estimated by combining the variations in the model’s other indicators and allows us to determine whether participants perceive our constructs as conceptually distinct from all other constructs”.
Table 2. Measurements’ properties assessment.


After examining the measurement instrument’s reliability, we examined the variables’ discriminant validity. According to Table 3, the “square root of the average variance extracted and presented in diagonal” for each construct must be greater than the correlations between that construct and the other constructs [133]. Our findings indicate that SHRM practice, AI, OL, CSR, and SOP all have a high level of discriminant validity in our model.
Table 3. Correlations among 1.vs with sq. rts. of AVEs.


4.2. Common Bias Method (CMB)
According to Kock (2015), the coefficients of complete collinearity VIF are highly susceptible to “pathological common variations” in all variables in methodological settings, which is consistent with the findings in this work. Therefore, even if a model passes discriminant and convergent validity tests, it is necessary to examine the CMB due to its susceptibility to common pathological variations. Given this, the literature indicates that a threshold value of 5 is acceptable and that a value of 3.3 is optimal for the full collinearity VIF value [131,132]). As a result, when the entire set of VIF values shown in Table 1 is used, all contrasts exceed the permissible threshold.
4.3. Testing of Hypotheses
We examined the model fit indices summarised and presented in Table 4 prior to interpreting the path coefficients of our model’s proposed relationships. When the indices are significant or satisfy the applicable criterion, the structural model is of sufficient quality [134,135]). Table 4 demonstrates that the structural model used in this study is both fit and adequate in its entirety.
Table 4. Structural model fit.


Following validation of our model’s fitness, we examined the significance of our constructs’ linear and non-linear interconnections. As shown in Figure 2, OL and CSR account for approximately 2% of the explained variation in SHRM, accounting for approximately 4% of the explained variation in artificial intelligence. OL, CSR, SHRM, and AI account for approximately 9% of the variation in the SOP’s explanations. These R2 values, according to [136], indicate that the formative constructs account for a small proportion of the variance. As shown in Table 5, the model testing results indicate that both the coefficient path and p-value for H1 (= 0.16, p = 0.02) and H3 (= 0.17, p = 0.01) are positive and significant. As a result of accepting H1 and H3, we conclude that organisational learning has a direct and positive relationship with long-term organisational performance and artificial intelligence that is statistically significant at the less than 5% level. Meanwhile, as shown in Table 5, the coefficient and p-value for H2 (=0.11, p = 0.08) indicate that the p-value exceeds the 5% confidence level, and thus the hypothesis is rejected. Additionally, the p values for the H4a and H4b coefficients indicate that H4a (=0.24, p = 0.01) and H4b (=0.56, p = 0.03) are statistically significant.


Figure 2. Model Testing results.
Table 5. Coefficients and p values Path.


As a result, we accept H4a and H4b and conclude that strategic human resource management and artificial intelligence are mediators between OL and SOP. This finding demonstrates that OL affects SOP both indirectly and directly via strategic human resource practises and artificial intelligence.
In addition, results of the influence of artificial intelligence, SHRM, and CSR on SOP as hypothesised in H5 (β = 0.13, p = 0.04), H6 (β = 0.14, p = 0.03), and H7 (β = 0.13, p = 0.05), respectively, shows that they all have a significant influence on SOP. Thus, we accept H5, H6, and H7 and conclude that artificial intelligence, strategic human resources management practices, and corporate social responsibility significantly influence sustainable organisational performance with less than 5% significant level. However, the direct relationship between CSR and SHRM (H8: β = 0.09, p = 0.11) on the one hand, and CSR and AI (H9: β = 0.07, p = 0.19) on the other hand were found not to be statistically significant; hence we reject H8 and H9. However, the mediating role of SHRM and AI in the interrelationship between CSR and SOP has been hypothesised in H10a (β = 0.26, p < 0.01) and H10b (β = 0.17, p < 0.01) were found to be statistically significant. As a result, we accept H10a and H10b and conclude that SHRM and AI serve as a conduit for the interaction of CSR and SOP. This implies that CSR affects SOP both indirectly and directly.
5. Discussions and Conclusions
Significant emerging concepts, OL and CSR, were examined within the context of SOP to position them in terms of ecological quality while achieving a sustainable competitive edge through SHRM practises and AI. This was done to investigate OL and CSR as predictors of long-term organisational performance and to shed light on how SHRM practises and AI can aid in SOP implementation. This study demonstrates that OL and CSR have a positive and significant impact on Jordan’s standard operating procedures. The positive correlation between OL and SOP observed in this study is consistent with previous research [90,91,92,96]. On the other hand, this study examined the effect of OL on organisational performance and sampled only profit-oriented organisations.
Meanwhile, the results contradict the findings of [97,99] who concluded in their studies that no relationship exists between the two variables. According to the literature, organisational learning is the “ability of an organisation to process knowledge—that is, to create, acquire, transmit, and integrate information—and to adapt its behaviour to the current cognitive environment to maximise performance”. Our findings support the notion that organisations can achieve sustainable performance by implementing OL effectively. Similarly, the present study’s finding of a significant and positive relationship between CSR and SOP is consistent with findings from a number of prior studies on profit-driven organisations [34,57,123,124,125] This demonstrates that an NGO’s performance can be sustained by adhering to ethical behaviour aimed at economic advancement while also promoting employees’ quality of life, their families, the local community, and society in general.
Considering the literature’s evidence of SHRM’s contribution to organisational performance, this study demonstrates how SHRM practises can assist organisations in maintaining their performance. This finding is consistent with several previous studies [9,116,118,119,120,137], though in for-profit organisations, as well as with [67], who conducted a similar study using Bangladeshi NGOs. Additionally, we discovered that artificial intelligence is critical for sustaining organisational performance over the long term. This finding is consistent with [112,138,139], who posited that allowing employees to perform certain tasks digitally enables more effective communication and knowledge exchange, which is capable of promoting innovative work behaviours that result in the firm gaining an advantage over the competition and thus contributing to the sustainability of their performance.
Furthermore, it was discovered that the moderating effects of SHRM and AI were beneficial in enhancing the positive influence of OL on SOP on the one hand and the relationship between corporate social responsibility and SOP on the other. This finding corroborates the findings of [104,105,106,108], who all suggested that a variety of factors mediate the relationship between OL and SOP in their respective studies. Effectively implementing SHRM, which defines the structure of the plan by which the firm’s human resources intend to accomplish organisational goals, will ensure the organisation’s performance is sustainable. Additionally, as demonstrated in this study, AI can amplify the effect of OL on SOP by enabling effective communication and knowledge exchange between employees [112,138].
While numerous studies have examined both the outcomes and antecedents of generic organisational performance, our research is the first to examine the antecedents of SOP in non-governmental organisations and the moderating effects of SHRM practises and artificial intelligence. This study sheds new light on how OL and CSR can assist Jordan’s non-governmental organisations achieve sustainable performance. This study was conducted in a developing economic context (Jordan), with a special emphasis on NGOs, which have been underrepresented in previous studies to generate substantial and original theory and practice knowledge. While prior research on OL and CSR has yielded important findings, none has examined how OL and CSR contribute to the sustainability of NGO performance. This study aimed to fill a void in the literature by situating OL and CSR within the context of contemporary environmental sustainability. Additionally, the empirical findings that SHRM practises and AI are effective for organisational performance were strengthened by demonstrating in this study that these variables amplify the positive effect of OL and CSR on organisational sustainability. Significant insights have been gleaned from rapidly developing areas of the literature due to our discoveries, allowing us to build on the scholars’ innovative discoveries and contribute to contemporary understanding of OL, CSR, SHRM, AI, and sustainable development organisational performance in the context of NGOs.
Our study’s findings also have significant implications for practitioners and industry policymakers in developing guidelines to ensure the effective implementation of OL and CSR within their respective organisations to achieve the organisation’s sustainability goals. To ensure the long-term viability of an organisation’s performance, such as an NGO, it is necessary to establish an effective mechanism for ensuring the development of their knowledge assets and putting forward practical methods for managing them. Additionally, they should ensure that they are fulfilling their numerous responsibilities to various stakeholders, including employees, shareholders, customers, the environment, and local communities. This becomes critical when one considers the organisational structure of NGOs, the primary mission of which is to volunteer in civil society in response to state failures. To accomplish this goal, NGOs must manage their resources efficiently to ensure their sustainability [28].
Additionally, NGOs should ensure that SHRM is implemented effectively to maximise the impact of organisational learning and corporate social responsibility on their performance sustainability. Similarly, the evidence for artificial intelligence’s importance as a predictor and moderator of long-term organisational performance demonstrates the importance of AI in an organisation. Finally, an organisation’s strategy must be crystal clear to achieve its objectives, which can be accomplished only when its managers mobilise the necessary human, technological, and financial resources. The majority of challenges can be addressed by integrating AI into an organisation by optimising processes and organisational performance. As a result, integrating AI into an organisation necessitates training employees to ensure the quality of future jobs in a world where humans and machines coexist. To maintain confidence, businesses should establish an “external and internal control tower” for data ethics, recruit and retain technologically savvy employees, and adapt training tools to accommodate increasing training volume and variety.
Limitations and Future Research
Although several experts have argued in favour of OL as a counterbalance to the myriad challenges confronting organisations, managers have yet to completely grasp the concept. However, continuous learning is necessary for organisational improvement, and a lack of understanding has made it difficult to spread and implement OL and other critical factors affecting organisational performance sustainability. This has posed a significant challenge for businesses, particularly non-profit organisations, which pursue growth and development in a manner distinct from profit-oriented organisations to maintain a competitive edge in dynamic environmental conditions. Considering these factors, this study established that OL, CSR, SHRM practises, and AI are all significant predictors of long-term organisational performance. Additionally, this study discovered that SHRM practises and artificial intelligence can potentially mitigate the effect of OL and CSR on long-term organisational performance. By incorporating additional relevant indicators, the model developed in this study paves the way for researchers and practitioners to develop more complex, holistic, and comprehensive models to investigate CRS and OL’s outcomes or other SOP antecedents. Meanwhile, the study’s limitations include using a non-probability sampling technique and a narrow focus on a single industry, limiting the findings’ generalisability. As a result, replicating the model in various industries will be fascinating.
",https://www.mdpi.com/2071-1050/14/12/7327
National Artificial Intelligence Research and Development Strategic Plan 2023 Update,White House,VARIOUS ,US STRATEGIC PLAN ,2023,"Artificial intelligence (AI)1 is one of the most powerful technologies of our time. In order to seize the opportunities that AI presents, the Nation must first work to manage its risks. The federal government plays a critical role in this effort, including through smart investments in research and development (R&D) that promote responsible innovation and advance solutions to the challenges that other sectors will not address on their own. This includes R&D to leverage AI to tackle large societal challenges and develop new approaches to mitigate AI risks. The federal government must place people and communities at the center by investing in responsible R&D that serves the public good, protects people’s rights and safety, and advances democratic values. This update to the National AI R&D Strategic Plan is a roadmap for driving progress toward that goal.
This plan defines the major research challenges in AI to coordinate and focus federal R&D investments. It will ensure continued U.S. leadership in the development and use of trustworthy AI systems, prepare the current and future U.S. workforce for the integration of AI systems across all sectors, and coordinate ongoing AI activities across all federal agencies.2
This plan, which follows national AI R&D strategic plans issued in 2016 and 2019, reaffirms eight strategies and adds a ninth to underscore a principled and coordinated approach to international collaboration in AI research:
Strategy 1: Make long-term investments in fundamental and responsible AI research. Prioritize investments in the next generation of AI to drive responsible innovation that will serve the public good and enable the United States to remain a world leader in AI. This includes advancing foundational AI capabilities such as perception, representation, learning, and reasoning, as well as focused efforts to make AI easier to use and more reliable and to measure and manage risks associated with generative AI.
Strategy 2: Develop effective methods for human-AI collaboration. Increase understanding of how to create AI systems that effectively complement and augment human capabilities. Open research areas include the attributes and requirements of successful human-AI teams; methods to measure the efficiency, effectiveness, and performance of AI-teaming applications; and mitigating the risk of human misuse of AI-enabled applications that lead to harmful outcomes.
Strategy 3: Understand and address the ethical, legal, and societal implications of AI. Develop approaches to understand and mitigate the ethical, legal, and social risks posed by AI to ensure that AI systems reflect our Nation’s values and promote equity. This includes interdisciplinary research to protect and support values through technical processes and design, as well as to advance areas such as AI explainability and privacy-preserving design and analysis. Efforts to develop metrics and frameworks for verifiable accountability, fairness, privacy, and bias are also essential.
Strategy 4: Ensure the safety and security of AI systems. Advance knowledge of how to design AI systems that are trustworthy, reliable, dependable, and safe. This includes research to advance the ability to test, validate, and verify the functionality and accuracy of AI systems, and secure AI systems from cybersecurity and data vulnerabilities.
Strategy 5: Develop shared public datasets and environments for AI training and testing. Develop and enable access to high-quality datasets and environments, as well as to testing and training resources. A broader, more diverse community engaging with the best data and tools for conducting AI research increases the potential for more innovative and equitable results.
- vii -
The National Artificial Intelligence R&D Strategic Plan

Strategy 6: Measure and evaluate AI systems through standards and benchmarks. Develop a broad spectrum of evaluative techniques for AI, including technical standards and benchmarks, informed by the Administration’s Blueprint for an AI Bill of Rights and AI Risk Management Framework (RMF).
Strategy 7: Better understand the national AI R&D workforce needs. Improve opportunities for R&D workforce development to strategically foster an AI-ready workforce in America. This includes R&D to improve understanding of the limits and possibilities of AI and AI-related work, and the education and fluency needed to effectively interact with AI systems.
Strategy 8: Expand public-private partnerships to accelerate advances in AI. Promote opportunities for sustained investment in responsible AI R&D and for transitioning advances into practical capabilities, in collaboration with academia, industry, international partners, and other non-federal entities.
Strategy 9: Establish a principled and coordinated approach to international collaboration in AI research. Prioritize international collaborations in AI R&D to address global challenges, such as environmental sustainability, healthcare, and manufacturing. Strategic international partnerships will help support responsible progress in AI R&D and the development and implementation of international guidelines and standards for AI.
The federal government plays a critical role in ensuring that technologies like AI are developed responsibly, and to serve the American people. Federal investments over many decades have facilitated many key discoveries in AI innovations that power industry and society today, and federally funded research has sustained progress in AI throughout the field’s evolution. Federal investments in basic and applied research3 have driven breakthroughs enabled by emerging technologies like AI across the board, including in climate, agriculture, energy, public health, and healthcare. Strategic federal investments in responsible AI R&D will advance a comprehensive approach to AI-related risks and opportunities in support of the public good.
 
- viii -
The National Artificial Intelligence R&D Strategic Plan

Introduction to the National AI R&D Strategic Plan: 2023 Update
Advances in generating, collecting, processing, and storing data have enabled innovation in AI, allowing this technology to become ubiquitous in modern life and touch nearly every facet of daily activities, directly or indirectly. Besides the AI-enabled applications in smartphones and personal computers, applications of AI have streamlined logistics, accelerated scientific discovery, enabled more efficient design and manufacturing, and aided in detecting financial fraud. However, realizing AI’s potential social and economic benefits and aligning it with American values requires considerable research investments, pursued in accordance with the principles of scientific integrity.
In February 2022, the Office of Science and Technology Policy (OSTP) issued a Request for Information (RFI)4 requesting input from all interested parties on the development of this plan. Over 60 responses were submitted by researchers, research organizations, professional societies, civil society organizations, and individuals; these responses are available online.5
Many of the RFI responses reaffirmed the analysis, organization, and approach originally outlined in the 2016 and 2019 strategic plans. It is noteworthy that a majority of the RFI responses referred to aspects of ethical, legal, and societal implications of AI (Strategy 3) or safety and security of AI systems (Strategy 4). These responses underscore a heightened priority across academia, industry, and the public for developing and deploying AI systems that are safe, transparent, and improve equity, and that do not violate privacy. Responses to the RFI also emphasized the importance of supporting AI R&D that will develop systems capable of helping to address some of the foremost challenges and opportunities before the Nation today, including advancing personalized medicine; improving cybersecurity; addressing inequities; bringing efficiencies to manufacturing, transportation, and other critical sectors of the economy; ensuring environmental sustainability; and enabling the scientific discovery and innovation that will power the next generation of technological breakthroughs.
AI as a National Priority
The Biden-Harris Administration is committed to advancing responsible AI systems that are ethical, trustworthy, and safe, and serve the public good. The fiscal year (FY) 2023 President’s Budget Request included substantial and specific funding requests for AI R&D, as part of a broad expansion of federally funded R&D to advance key technologies and address societal challenges.6 The CHIPS and Science Act of 20227 and Consolidated Appropriations Act, 20238 reflect Administration and Congressional support for an expansion of federally funded R&D, including AI R&D.9
The memorandum on Multi-Agency Research and Development Priorities for the FY 2024 Budget10 issued jointly by the Office of Management and Budget and OSTP likewise calls for agencies to prioritize R&D funding toward advancing national security and technological competitiveness, including trustworthy AI, among other critical and emerging technologies of national interest. This plan pursues the advancement of fundamental and translational AI research to make AI trustworthy, equitable, and both rights- and privacy-preserving.
The National AI Initiative Act (NAIIA) of 2020 established the National AI Initiative Office (NAIIO) to coordinate key AI activities across the federal government. This office, based in the White House OSTP, is the central point of contact for technical and programmatic information exchange on activities related to the National AI Initiative across the federal government, academia, industry, nonprofit organizations, professional societies, civil society, and state, local, and tribal governments. In addition, the NAIIO helps advance progress on the priorities outlined in this plan and implements a comprehensive approach to AI- related risks and opportunities in support of the public good.

While R&D activities and outputs inform governance and regulatory approaches, this plan leaves discussions of regulation or governance to other federal documents, such as the Blueprint for an AI Bill of Rights and the AI Risk Management Framework. In addition, issues related to scientific integrity and public access, while directly relevant to AI R&D, are largely left to other federal government documents as well

Strategy 1: Make Long-Term Investments in Fundamental and Responsible AI Research
The United States has maintained its leadership in AI in large part because of continued and consistent investment in long-term, fundamental AI research. For example, many of today’s AI-enabled products and services have their roots in federally funded fundamental research dating back decades. This trend has continued since the release of the 2019 Strategic Plan, with a notable increase in AI R&D funded by the federal government. For example, the AI Initiative and companion efforts funded by the Department of Energy (DOE) Office of Science have enabled groundbreaking discoveries in areas from fusion energy to SARS-CoV-2 understanding. The Nation must continue to foster long-term, fundamental, and responsible research in AI to allow for further discoveries and innovations with long-term benefits.
Investments in fundamental AI R&D span the spectrum from foundational to use-inspired research. For example, foundational investment in AI R&D drives forward learning, reasoning, planning, knowledge representation, computer vision, and beyond, with potential for scale-up and adoption in practice. Use- inspired AI research, meanwhile, contributes to advances in AI while also advancing areas such as agriculture, healthcare, manufacturing, economics, critical infrastructure, and sustainability, with the goal of engaging and improving all of society while respecting individual freedoms.
Of particular importance is the investment in the development of assurance and trust in AI systems, as reflected in Strategy 3 and Strategy 4. Research in these areas is essential for using AI in all fields, but it is particularly important in systems that involve safety or applications in which AI decisions affect individuals, groups, communities, and the environment. Most AI R&D thus far has focused on the advancement of AI for individual tasks. Additional work is needed to solve increasingly difficult science and technology challenges covering multiple domains and applications, moving toward the vision of general-purpose AI. AI R&D increasingly attempts to consider how various areas of AI work can fit together into an integrated system. As a result, this strategy includes priorities that continue to advance AI for individual tasks but also aim toward the vision of general-purpose AI systems. The priorities involve using the significant amount of available data for machine learning (ML) and knowledge discovery, improving the abilities of AI to perceive and act, and developing scalable, general-purpose systems to work in real and virtual environments.
Finally, developing a theoretical understanding of the capabilities and limitations of AI systems can inform what R&D should be done and is critical for enabling safe use of AI. For example, a better understanding of how deep networks construct effective representations could lead to new network designs that can reason about uncertainty more directly and without requiring as much data to train.
This strategy is divided along ten lines of effort: Advancing Data-Focused Methodologies for Knowledge Discovery; Fostering Federated ML Approaches; Understanding Theoretical Capabilities and Limitations of AI; Pursuing Research on Scalable General-Purpose AI Systems; Developing AI Systems and Simulations Across Real and Virtual Environments; Enhancing the Perceptual Capabilities of AI Systems; Developing More Capable and Reliable Robots; Advancing Hardware for Improved AI; Creating AI for Improved Hardware; and Embracing Sustainable AI and Computing Systems.
Advancing Data-Focused Methodologies for Knowledge Discovery
As discussed in the Federal Big Data Research and Development Strategic Plan from 2016,11 new tools and technologies are needed to achieve intelligent data understanding and knowledge discovery. For example, progress on the development of more advanced AI systems will help identify useful information hidden in big data. Many open research questions revolve around the creation and use of data, including its veracity
 
-3-

and appropriateness for AI system training and its role in creating interpretable, reproducible algorithms. While much research has dealt with veracity through data quality assurance methods to perform data cleaning and knowledge discovery, further study is needed to improve the efficiency of data cleaning and labeling techniques, to create methods for discovering inconsistencies and anomalies in the data, to address privacy considerations, and to develop approaches for incorporating human feedback. Researchers also need to explore new methods to enable data and associated metadata to be mined simultaneously. Another major issue is the lack of adequate and representative data in many domains, such as healthcare. Techniques need to be developed to deal with the generation and curation of redacted data to facilitate ML for domains with sensitive data. These and other data concerns are addressed in Strategy 5.
Many AI applications are interdisciplinary in nature and involve heterogeneous data. Further investigation of multimodal ML is needed to enable knowledge discovery from a wide range of heterogeneous data types (e.g., discrete, continuous, text, spatial, temporal, spatiotemporal, graphs).
In addition to data, one of the fundamental challenges in current AI systems is the lack of a standard infrastructure to encode knowledge AI systems must process and interpret significant amounts of data to approximate human-like responses. Hence, it is important to have different kinds of data (e.g., causal, temporal, heuristic) encoded in a form that is open and accessible. As an example, an Open Knowledge Network12 is one concept for making this knowledge accessible,13 but there is a need for considerable research, including developing domain-specific knowledge repositories in standardized formats.14
Fostering Federated ML Approaches
New federated approaches to ML will be important in an increasingly interconnected world and amid growing concerns around data privacy and security.15 Federated learning allows multiple computers or devices to collaborate in building a shared global ML model based on the data that is locally stored on each device. The overall process is a back-and-forth iteration that involves each device training a local model on its own data and then sharing only the model updates (not the data) to improve the global model. The global model is distributed back to the devices for further local training until the global model reaches a specified level of accuracy. Federated learning can improve the accuracy and fairness of such global ML models by including locally-protected data from a diverse and more representative range of users, devices, and other sources that may have data-sharing restrictions due to competitive, regulatory, or privacy concerns. The ability to process confidential information is critical to industries such as healthcare, finance, and telecommunications. Federated learning is one among a range of approaches for privacy-preserving data sharing and analytics.16 Major research challenges arise in dealing with the heterogeneous characteristics of devices (memory capacity, computing power, network connectivity) and data (skewed data samples, different modalities such as images, video, text). Improved efficiency in ML model communication and updating from multiple devices into a shared global model, as well as better data protection and security approaches, are areas for continuing research focus.17
Understanding Theoretical Capabilities and Limitations of AI
While the goal for many AI algorithms is to address open challenges with general-purpose systems, there is not yet a good understanding of the theoretical capabilities and limitations for AI, nor of the extent to which such solutions are even possible with AI algorithms. Theoretical work is needed to better understand how some AI techniques, especially generative AI, work and their emerging properties. Building this understanding of what advanced systems can and cannot do is important for enabling safe and responsible use of AI. While different disciplines (including mathematics, control sciences, and computer science) are studying this issue, the field currently lacks unified theoretical models or

-4-

frameworks to understand AI system performance. Additional research is needed on computational solvability, which is an understanding of the classes of problems that AI algorithms are theoretically capable of solving, and likewise, those that they are not capable of solving. This understanding must be developed in the context of existing hardware, to see how the hardware affects the performance of these algorithms. Understanding which problems are theoretically unsolvable can lead researchers to develop approximate solutions to these problems, or even open new lines of research on new hardware for future AI systems.
Pursuing Research on Scalable General-Purpose AI Systems
A development toward scalable general-purpose AI is the emergence of so-called foundation models that are trained on large amounts of unlabeled data, usually using self-supervised learning, and can be adapted to many application domains such as law, healthcare, and science. Innovations continue to advance the frontiers of what foundation models can do on language and image tasks. Familiar examples of large pre- trained language models include BERT (Bidirectional Encoder Representations from Transformers), GPT-4 (Generative Pre-trained Transformer), and other AI systems with skills that might begin to resemble intelligence within certain domains. Additional R&D is necessary to minimize unwanted fabrications and harmful biases in generative AI. These models are prone to “hallucinate” and to recapitulate biases derived from unfiltered data from the internet used to train them. Further research is needed to enhance the validity and reliability as well as security and resilience of these large models, especially in response to adversarial attacks. Further research is also needed to develop techniques for explaining and interpreting model outputs. Additional work is needed to address privacy concerns related to training models on such large corpuses of data. Finally, appropriate safeguards will need to be conceptualized and designed into these systems.
Developing AI Systems and Simulations Across Real and Virtual Environments
An emerging trend in modeling and simulation is the development of “digital twins.” A digital twin is a virtual representation or model that serves as the real-time digital counterpart of a physical object or process. Real-world applications include predictive maintenance of aircraft engines, urban planning and the management of smart cities, and additive manufacturing. A key requirement is that the physical system is instrumented so that the collected data is interactively shared with the digital or computational model of itself. The digital-twin approach enables smart automation of physical systems across real and virtual environments. Challenges specific to various applications, such as data completeness, quality, latency, and privacy, and the varying accuracies with which different phenomena can be modeled, are likely to lead to additional challenges for digital twins.18
Enhancing the Perceptual Capabilities of AI Systems
Perception is an intelligent system’s window into the world. Perception begins with sensor data, which come in diverse modalities and forms, such as the status of the system itself or information about the environment. Sensor data are processed and fused, often along with a priori knowledge and models, to extract information relevant to the AI system’s task, such as geometric features, brightness, velocity or vibration. Integrated data from perception forms situational awareness to provide AI systems with the comprehensive knowledge and a model of the state of the world necessary to plan and execute tasks effectively and safely. AI systems would greatly benefit from advancements in hardware and algorithms to enable more robust and reliable perception. Sensors must be able to capture data at long distances with high fidelity, often in real time. Systems for perception need to be able to integrate data from a variety of sensors and other sources, including edge devices and cloud systems, to determine what the AI system is currently perceiving and to allow the prediction of future states. Detection, classification,
-5-

identification, and recognition of objects remains challenging, especially under cluttered and dynamic conditions, and privacy considerations add additional complexity to designing systems for real-world applications. In addition, the perception of humans, including the states of their attention and emotion, must be greatly improved by using an appropriate combination of sensors and algorithms so that AI systems can work more effectively with people,19 and as discussed in Strategy 2. Methods and techniques for calculating and propagating uncertainty throughout the perception process are needed to quantify the confidence levels that AI systems have in their situational awareness and to improve overall accuracy.
Developing More Capable and Reliable Robots
Robotics continues to harness most fields of AI, with special emphasis on perception, physical manipulation, and navigation. Significant advances in robotic technologies over the last decade are leading to potential impacts applications including manufacturing, logistics, medicine, healthcare, defense and national security, agriculture, and consumer products. One noteworthy development involves the introduction of AI-controlled robots into the research environment, yielding “autonomous laboratories” that can enable closed-loop synthesis characterization and testing systems capable of designing new drugs, chemicals, advanced electronic materials, and countless other materials far faster and with greater variety and precision than previously possible. Introducing autonomy into manufacturing can further accelerate the efficiency of product design coupled to product performance, while in biological systems, it can drive evolution of organisms to act as living sensors of specified environmental signals. While robots were historically deployed in static industrial R&D environments, recent advances involve close collaborations between robots and humans. Robotic technologies are now showing promise in their ability to complement, augment, enhance, or emulate human physical capabilities or human intelligence. However, scientists and engineers need to make these robotic systems more capable, reliable, easy-to- use, and safe.
Researchers need to improve robot perception to better extract information from a variety of sensors to provide robots with real-time situational awareness to inform decision-making. Progress is needed in cognition and reasoning to allow robots to better understand and interact with the physical world. An improved ability to adapt and learn, building abstract representations of low-level physical tasks, will allow robots to generalize their skills, self-assess their current performance, and learn a repertoire of physical movements from human teachers. Mobility and manipulation, especially when dealing with heavy objects, are areas for further investigation so that robots can move across rugged and uncertain terrain and handle a variety of objects dexterously. Robots need to learn to team together in a seamless fashion and collaborate with humans in a way that is trustworthy and predictable. Robotic systems must safely and cooperatively interact with humans and other actors in complex built and natural environments. Research is also needed to deal with adversarial systems, or systems that operate in disguise to collect data or interfere with legitimate operations. In general, robotic systems require research advances that will make them more capable and reliable, easier to use, and safer.
Advancing Hardware for Improved AI
While AI research is often outwardly associated with advances in software, the performance of AI systems has been heavily dependent on the hardware on which they run. The current renaissance in deep learning and generative AI is directly tied to progress in graphics processing unit (GPU)-based20 and accelerator- based hardware technology and the associated improved memory, input/output, clock speeds, parallelism, and energy efficiency.

-6-

Developing hardware optimized for AI algorithms will enable even higher levels of performance than those of GPUs. One example is “neuromorphic” processors that are inspired by the organization of the brain and, in some cases, optimized for the operation of neural networks.21
Hardware advances can also improve the performance of AI methods that are highly data intensive. Advances in storage technology would also benefit the deployment of AI systems. Continued research is also needed to allow ML algorithms to efficiently learn from high-velocity data, including distributed ML algorithms that simultaneously learn from multiple data pipelines. More advanced ML-based feedback methods will allow AI systems to intelligently sample or prioritize data from large-scale simulations, experimental instruments, and distributed sensor systems (e.g., smart buildings and the Internet of Things). Such methods may require advances in input hardware, including dynamic input or output decision-making, in which choices are made in real time to store data based on importance or significance, rather than simply storing data at fixed frequencies.
Creating AI for Improved Hardware
Just as improved hardware can lead to more capable AI systems, AI systems can also improve the performance and resource (e.g., energy) usage of hardware.22 This reciprocity will lead to further advances in hardware performance, since physical limits on computing require novel approaches to hardware designs.23 One example is where AI is being used to predict high-performance computing (HPC) performance and resource usage and to make online optimization decisions that increase efficiency; more advanced AI techniques could further enhance system performance. AI can also be used to create self- reconfigurable HPC systems that can manage system faults when they occur, without human intervention.24
Improved AI algorithms can increase the performance of multicore systems by reducing data movements between processors and memory. In practice, the configurations of processes in HPC systems are never the same, and different applications are executed concurrently, with the state of each different software application evolving independently over time. AI algorithms need to be designed to operate online and at scale for HPC systems. HPC systems are governed by physical and mathematical laws, which both determine and constrain their performance, and AI algorithms that incorporate these laws into their design will be able to more efficiently optimize AI hardware design in a virtuous loop, leading to even more powerful AI implementations.
-7-

CHIPS and Science Act of 202225
Passed by Congress and signed into law by President Biden to boost U.S. semiconductor production and the Nation’s research and innovation enterprise,26 the bipartisan and bicameral CHIPS and Science Act of 2022 provides a generational opportunity to advance U.S. leadership in semiconductor design and manufacturing, as well as in accelerating breakthroughs in emerging technologies such as AI. AI is an essential element of microelectronics research, development, and manufacturing, and is intimately tied to the Nation’s ability to sustain leadership in this industry. Advances in AI R&D will power future microelectronics devices and systems. Of particular interest is the timely opportunity to develop an overall co-design framework for all stages of microelectronics design, development, fabrication, and application.27 Co-design involves multidisciplinary collaboration that accounts for the interdependencies among materials design, device physics, computer architectures, memory, interconnects, and the software for developing next-generation computing and networking systems. At the same time, a strong microelectronics innovation ecosystem resulting in next- generation computing and network systems will lay the foundation for the breakthroughs and innovations that will ensure that the United States remains in the vanguard of competitiveness across a wide range of fields and sectors, including AI R&D.
Embracing Sustainable AI and Computing Systems
The rising computational cost of developing and operating state-of-the-art AI systems warrants significant attention. The proliferation of data-intensive AI is expected to dramatically increase computational demands and the associated environmental impacts. There is an urgent need to design resource-aware AI algorithms, systems, and applications that consider broader notions of sustainability beyond simply energy consumption. Sustainable AI also depends on research in environmental sustainability within and across all layers of the computing stack and the data management and use lifecycle. This requires a shift in research toward embracing design for sustainability that treats sustainability impacts as first-order metrics and on equal standing with performance, reliability, usability, and operational energy efficiency.
-8-

Strategy 2: Develop Effective Methods for Human-AI Collaboration
Effective methods for human-AI collaboration have become an increasingly important priority as AI becomes more prevalent throughout society. Fully autonomous systems that involve little or no human interaction will continue to be crucial for applications in industry (e.g., automated factories, control of energy systems), hazardous domains (e.g., deep space, radioactive environments), and other areas. However, other applications, ranging from disaster recovery to scientific discovery, are most effectively addressed by a combination of humans and AI systems working together in a way that leverages their respective strengths and mitigates risk. Indeed, the promise of future AI applications requires fully understanding human-AI teaming and collaboration.
This strategy recognizes the growing importance of sociotechnical and human factors and addresses the need for multidisciplinary research in enabling effective human-AI collaboration. It is divided along five lines of effort: Developing the Science of Human-AI Teaming; Seeking Improved Models and Metrics of Performance; Cultivating Trust in Human-AI Interactions; Pursuing Greater Understanding of Human-AI Systems; and Developing New Paradigms for AI Interactions and Collaborations.
Developing the Science of Human-AI Teaming
Teaming is a complex relationship requiring a deep understanding of human decision-making processes and their interactions. Human-human teaming is supported by a substantial body of knowledge, models, and methods for enhancing team performance. The relevance of this body of work for enabling more effective human-AI teams is unclear.28 Research is needed to understand the human side of human- machine interactions. Studies are needed to gain an understanding of the attributes and requirements of successful human-machine teams for efficient and effective task performance. These studies will involve understanding the additional capabilities that a machine needs in order to become an effective teammate for the relevant tasks and environments and includes the modeling of human interactions. The first National AI R&D Strategic Plan defined three functional roles for AI systems in teaming contexts:29
AI performs functions alongside the human: AI systems perform peripheral tasks that support the human decision-maker. For example, AI can assist humans with working memory, short- or long-term memory retrieval, and prediction tasks.
AI performs functions when the human encounters high cognitive overload: AI systems perform complex monitoring functions (such as ground proximity warning systems in aircraft), decision- making, and automated medical diagnoses when humans need assistance.
AI performs functions in lieu of a human: AI systems perform tasks for which humans have very limited capabilities, such as for complex mathematical operations, control guidance for dynamic systems in contested operational environments, aspects of control for automated systems in harmful or toxic environments, and in situations to which a system should respond very rapidly (e.g., in nuclear reactor control rooms).
Fully understanding human-AI teaming requires moving beyond these three functional roles, or today’s models of humans as operators, and on to the idea of teammate relationships. To become true teammates, machines will need to be flexible and adaptive to the states of their human counterparts, as well as to the environment—to intelligently anticipate their human teammates’ capabilities and intentions, and to generalize specific learning experiences to entirely new situations.30 Each of these capabilities represents a research challenge. Other open questions that impact human-AI teaming include team composition, management of situational awareness, and interaction paradigms that govern the
-9-

amount of control given to the AI system, when that control is granted, and how that control is distributed and transitioned.31
Seeking Improved Models and Metrics of Performance
A traditional approach for building effective human-AI teams is to consider the capabilities of the humans and AI systems separately, and then to investigate how the team can be brought together in an optimal fashion. Qualitative and descriptive models of human-AI performance will need to develop into predictive computational models that can assess the relative value of teaming compositions, processes, interface mechanisms, and other characteristics. Human-AI team collaborations are difficult to model well. Ensuring that the team’s collective abilities are significantly better is a grand multidisciplinary challenge across such areas as psychology, decision sciences, economics, and human factors engineering, among others. The challenges are compounded when accounting for unexpected events and the issues of situational awareness, trust, and the potential for human and AI biases. The collaboration types of human-AI teaming models will also differ among human-assisted AI decision-making, AI-assisted human decision-making,32 pure AI decision-making, and AI-assisted machine decision-making. Significant amounts of research are required on the theories, models, data, and computational tools needed for measuring, modeling, simulating, analyzing, and understanding the effectiveness of human-AI teams.
Cultivating Trust in Human-AI Interactions
The opaque nature of the programming and decision processes within AI systems is a potential barrier to the trust needed for effective human-AI teaming. One key challenge for humans is an expectation that mechanical and automated systems will behave in a deterministic way. Given similar conditions and inputs, the system should respond in the same way as before. However, AI systems may behave in non- deterministic, or unpredictable, ways in response to imperfect, noisy, and complex real-world information or even simply because they are stochastic by design. Furthermore, continuous learning systems will evolve over time. Another challenge is related to the accuracy of AI systems and appropriately calibrating understanding of system outputs that could be incorrect. Trust is recognized as a key factor associated with the use of AI systems.33 Research is needed on how to establish and maintain appropriately calibrated trust among teammates in uncertain conditions and environments.34
Pursuing Greater Understanding of Human-AI Systems
Greater trust in and overall success of human-AI teaming will stem from the lessons learned from failures that can be replicated and studied to determine what went wrong. “Recorders” are important in all AI applications, and diagnosing failures in human-AI teams is a particularly acute need. As the science of teaming evolves, the need for testbeds and methodologies to measure the effectiveness of human-AI teaming in settings that replicate the complexity of the operational environments also becomes critically important. Pursuing research in virtual environments and developing testing methodologies that measure human teaming components and the user experience are important next steps for the deployment of successful systems that provide assurance.35
Developing New Paradigms for AI Interactions and Collaborations
Usability and human-centered design research demonstrate that interaction mechanisms, designs, and strategies highly influence user performance. Similar research is required to understand the usability and impact of interaction design in human-AI teaming. Specifically, research is needed to understand the influence of interaction design on decision-making, skill retention, training requirements, job satisfaction, and overall human-AI team performance and resilience. Research should also include the development of new paradigms for human-AI interaction to facilitate collaboration, decision-making actions, human
-10-

oversight, accountability, and control. A particular challenge is conveying enough information to the user while avoiding cognitive overload. Other interaction challenges include enabling the user and the machine to understand when to pass control back and forth, and how to maintain user engagement for proper situational awareness. Early research has shown that relying on a “human in the loop” is not a universally effective method for catching errors or ensuring sound decision-making, even though these human-in- the-loop applications may give the impression of a more robust or fair system. Finally, research into human-AI interactions and paradigms requires controlled experiments with end users. There is currently little research on the application of usability, human factors, and human-centered design to the development of AI-teaming applications.36 Open research areas include understanding user needs and user requirements; the role of context in AI-teaming application use; the use of task analysis and iterative design methods; and ways to measure efficiency, effectiveness, and performance of AI-teaming applications. A research focus that includes end users, including the public where appropriate, provides a lens for studying how best to address existing structural inequalities in human-AI collaboration, promote the development of tools for safe and effective human-AI collaboration, and effectively train the human in human-AI collaborative situations.
-11-

Strategy 3: Understand and Address the Ethical, Legal, and Societal Implications of AI
AI technologies hold significant opportunity, but they also pose risks that can negatively impact individuals, groups, organizations, communities, society, the environment, and the planet. Like risks for other types of technology, AI risks can emerge in a variety of ways and can be characterized as long- or short-term, high or low-probability, systemic or localized, and high- or low-impact.37 Without proper controls, AI systems can amplify, perpetuate, or exacerbate inequitable or undesirable outcomes for individuals and communities.
Since the National AI R&D Strategic Plan: 2019 Update, investment in AI and public awareness of the technology have increased. This has been accompanied by an increasing focus on the ethical, legal, and societal implications of responsible AI. According to the 2022 AI Index Report, publications addressing AI fairness and transparency have quintupled over the past decade.38
As a step toward addressing concerns related to the use of AI in society, the White House issued A Blueprint for an AI Bill of Rights that lays out five core protections to which everyone in America should be entitled when interacting with AI and automated systems: Safe and Effective Systems; Algorithmic Discrimination Protections; Data Privacy; Notice and Explanation; and Human Alternatives, Consideration, and Fallback.39 In January 2023, NIST published a framework to better manage risks to individuals, organizations, and society associated with AI.40 These complementary frameworks provide useful guidance to researchers as well as important avenues for further research.
This strategy identifies R&D priorities that can help to instantiate these principles—viewing them as design objectives, system properties, or requirements. Centering these principles in the development process is key to ensuring that AI broadly benefits the American people. The interdisciplinary field of values in design develops methods and approaches to build support and protection for rights and values into sociotechnical systems, or systems that integrate social and technical aspects. The study of ethical, legal, and societal aspects of AI is critical because decisions about the use and design of AI can require trade-offs between competing values, such as equity, fairness, privacy, and autonomy. These issues are challenging, even outside the realm of AI. But AI systems bring these concerns to the fore because they often do not attempt to model the decision-making processes, including the ethical and legal constraints on them, of humans or organizations, but rather analyze the results of such decision-making processes to develop their own heuristics for decision making.
The extent of work in developing AI principles and guidelines highlights growing concerns about the ethical, legal, and societal implications of AI. Ensuring that AI can be developed and used in accord with these principles will require an expansive R&D program. This strategy divides this R&D program along four lines of effort: Making Investments in Fundamental Research to Advance Core Values Through Sociotechnical Systems Design and on the Ethical, Legal, and Societal Implications of AI; Understanding and Mitigating Social and Ethical Risks of AI; Using AI to Address Ethical, Legal, and Societal Issues; and Understanding the Broader Impacts of AI.
Making Investments in Fundamental Research to Advance Core Values Through Sociotechnical Systems Design and on the Ethical, Legal, and Societal Implications of AI
There are several areas in which fundamental research is needed to advance our ability to design values- aligned AI systems and to understand the ethical, legal, and societal implications of AI. The use of design, in addition to policy, to protect security, accessibility, privacy, and accountability is an active area of research and practice. It moves beyond the retroactive analysis of impacts, developing the tools and
-12-

methodologies to reason about how best to protect values through mixed technical and policy choices. Research that supports values-aligned design approaches that consider multiple values, rather than one at a time, are essential to support the development of safe, equitable, and accountable AI systems. Technical work on issues such as explainability and interpretability are important to this work, as is technical work on privacy, harmful bias mitigation, and accountable design. For example, with many types of AI, such as deep learning models, explainability, and effective auditing of the model are difficult technical problems. Resolving the technical problems is only part of the challenge. Ensuring that users can make sense of system behavior in context (i.e., interpretability) is also essential. This is a sociotechnical problem that requires understanding the context in which the model will be operating, the needs and capabilities of the people who require the explanation, and the most effective methods of communicating the explanation. Research into communications and psychology finds that individuals generally overestimate how well they understand others’ perspectives and how well their communications are understood.41 Given this reality, interpretability will require fundamental research into communications.
There is also a need for technical research to develop metrics and frameworks for accountability, fairness, privacy, and bias. This includes research into language models and other generative AI systems to mitigate the production of harmful and biased outputs.42 This must be accompanied by basic social science research into AI governance, which will include understanding how to engage stakeholders most effectively on AI issues throughout the AI life cycle, establishing legitimacy for AI development and implementation decisions, and performing intersectional research into how different people and communities understand, interact with, and are impacted by technology.
This work must be accompanied by research examining the potential implications of AI and developing evaluation and mitigation strategies. This research is needed to inform policy and governance approaches.
Understanding and Mitigating Social and Ethical Risks of AI
There is an immediate need for research to identify effective AI governance structures that can mitigate risks, build systems and implement AI worthy of public trust, and foster appropriately calibrated public trust in it through effective engagement. One possibility is to study and adapt approaches from other fields, such as medicine, that have robust governance and regulatory ecosystems. For example, institutional review boards for AI research to consider AI R&D’s potential harm could be explored. Such an Ethics, Scientific Integrity, and Society Review Board could help steer the research community away from research questions that pose risks of downstream harm without any clear benefits, and could learn from past engagement with nuanced questions of harm and value tradeoffs. Similarly, the random control trials, validation, and ongoing monitoring used for drugs and medical devices may provide models for AI governance more generally. However, the governance of AI will vary depending on the context of use and approaches to validating efficacy and safety vary across sectors in relation to risk. The need for robust governance and oversight structures appropriate to domains of use, which is relevant to all fields of scientific endeavor, is particularly acute in AI as the pathways from ideas to impacts have become especially short.
Social science research exploring the introduction of AI systems into organizations, professions, and fields is necessary to develop a richer understanding of how AI alters the production of knowledge, shapes understandings of professional responsibility, shifts accountability across institutional actors, and shapes the relationships between organizations and the populations they serve.
Stakeholder engagement can be advanced by studying how to adapt deliberative civic engagement processes to AI governance and develop new methods to elicit stakeholder feedback. These social science and regulatory tools can empower communities to weigh in on AI’s public- and private-sector uses, legal
-13-

and ethical issues, and societal implications. Broadened participation can also promote diversity and equity in shaping data collection, storage, and management practices; developing regulatory oversight and guidance; and creating equitable policy solutions.43
Finally, R&D can determine how best to teach and communicate about AI governance structures and sociotechnical approaches for various audiences, be they researchers, research subjects, technologists, policymakers, other stakeholders, or the public. As previously stated, a fundamental truth in the field of communications44 is that people overestimate how well they are understood by others. There are ethics and scientific integrity requirements in some technology curricula, but it is important to systematically identify and promulgate the most effective ways to integrate these concepts into the learning process to ensure that people have the tools to engage with these issues effectively and consider their actions and decisions in broader contexts. There are other urgent technology issues that require ongoing R&D as well. For example, the use of personal data in AI systems raises privacy concerns, highlighting the importance of privacy-enhancing technologies such as homomorphic encryption, differential privacy, and secure multiparty computation to mitigate these concerns. There is also a need for tools to identify and mitigate harmful bias across datasets, particularly in new training data. Overall, mechanisms to develop, assess, and maintain AI systems that mitigate risk and maximize benefit are keenly needed.
Using AI to Address Ethical, Legal, and Societal Issues
AI system development, when approached in a manner that mitigates bias and harm and is done in accordance with the civil rights, civil liberties, and interests of those affected by the system, can help address complex societal challenges. Properly developed, AI can help provide data-driven inputs as society tries to address issues in domains that advance equity, climate change adaptation and mitigation, employment, and healthcare, especially for those traditionally underserved. AI often exacerbates bias, but ongoing research has shown that it can also be used to identify and mitigate harmful bias in current practice.45 Different AI tools need to be developed and adapted to face the challenges in different domains: the AI capabilities needed to optimize healthcare will differ from those needed to address environmental sustainability. There is also reason for caution in these endeavors, as technology solutionism, where technological solutions are advocated for challenges for which they may be inappropriate or ineffective, has been problematic in a number of scenarios.
A few general capabilities are needed for AI to better be able to help address broader societal issues. First, as noted above, AI can be used to counter harmful bias. Understanding how AI can reduce inequities stemming from systemic, structural, and individual bias is an important area of research. This would enable a range of analyses of the use of AI in managing harmful bias.46 Existing research has shown that some well-known mathematical definitions of bias47 make inherently conflicting recommendations, so an ongoing challenge is developing sociotechnical mechanisms to resolve conflicts in the decision-making pipeline. Indeed, research in this area must be sociotechnical, focusing on real world implementations, in particular institutional and regulatory contexts, and account for the policies, professional and organizational obligations that structure interactions and reliance between humans and AI results.
Second, research is needed to ensure that use of AI capabilities advance equity rather than exacerbating inequity. For example, if only wealthy hospitals can take advantage of AI systems, the benefits of these technologies will not be equitably distributed. Research to make beneficial AI accessible in historically underrepresented communities will help ensure that those in greatest need of these capabilities can use them. This research will include making AI capabilities affordable and ensuring that AI is understood and can be integrated into existing systems.
-14-

Many historically underrepresented communities may not be represented in datasets typically used to train AI systems, nor included in development processes. This limits the ability of these communities to benefit from the AI systems. While noteworthy efforts are being made to connect with a broader set of communities, additional research is needed to identify these types of gaps and address them more fully.
Finally, there is an international dimension to these challenges. For truly global concerns (e.g., pandemics), international approaches are needed, as discussed in Strategy 9. In addition to the concerns about access and serving the underserved, AI that can be adapted to societies with different legal, ethical, and political commitments while respecting human rights and democratic values is essential.
Understanding the Broader Impacts of AI
AI promises to bring vast changes to society. While many of those changes will be positive, there are likely to be negative consequences, and these impacts are also likely to be unequally distributed. R&D in the ethical, legal, and societal implications of AI is needed to understand, anticipate, and mitigate harm as well as understanding the distribution of likely benefits. Large-scale research into sociotechnical feedback loops, using the tools of systems engineering and complexity theory, is needed to understand how AI interacts with society. This includes the systematic study of the tradeoffs in societal benefits and risks of using, using in different permutations, or not using AI in each context.
One specific area that requires this approach is the future of work.48 There has been some attention to the future of work, the potential for AI to displace workers, and the need to retrain workers for a rapidly changing economy.49 There is also a need to understand what AI does to workplaces and how it impacts work safety and overall well-being.50 This is especially needed with the growing popularity and abilities of generative AI systems. Similar inquiry is needed across social institutions, such as research into how AI will change how patients experience the healthcare system and how students are educated.
Finally, R&D is needed to identify means to counter malicious uses of AI, for example the generation of deep fakes and manipulation of social media. Here, too, there may be technical responses, but sociotechnical study is needed as well. The Information Integrity Research and Development Interagency Working Group (IWG) recently published recommendations, which will in turn require innovative approaches to implement in future AI systems.51
-15-

Strategy 4: Ensure the Safety and Security of AI Systems
While AI systems offer promise in providing performance improvements in several different applications, their increased complexity, rapidly evolving technology base, and significant data needs can lead to increased risks derived from their deployment. The result is an emerging emphasis on the safety and security of AI systems, which requires an inherently interdisciplinary approach. For the purposes of this strategy, to appropriately discuss the needs surrounding these risks, the terms “safety” and “security” will carry the definitions laid out in the “Assessing and Improving AI Trustworthiness: Current Contexts and Concerns” workshop report,52 which defines safety as mitigating against a system producing new harm, and security as monitoring a system’s integrity. This usage is consistent with the NIST AI RMF.
Critical areas for research focus include the development of testing methods that can scale with the increasing demands of modern AI systems and complex systems-of-systems, and improved methods for ensuring the security of AI systems against input data manipulation, model inversion, and other forms of adversarial attack. Ultimately, a combination of additional investment in standards, systems, and research is needed to calibrate trust in the performance of deployed AI systems.
Key to this strategy is addressing the fundamental question of what level of testing is sufficient to ensure the safety and security of non-deterministic and/or not fully explainable systems before their deployment. The process of securing and making safe AI, as discussed in the first National AI R&D Strategic Plan and the 2019 update, must be incorporated in all stages of the AI system life cycle, from the initial design and data/model building to verification and validation, deployment, operation, and monitoring. “Safety by Design” must therefore be an important part of the AI R&D portfolio, particularly as models are increasingly used by non-technical users and incorporated across a broad range of platforms and applications. Adopting AI systems that are unsafe or insecure will potentially lead to harm, and uncertainty about the safety and security of these systems will stymie AI adoption (as discussed in Strategy 3).
Standards setting (discussed in depth in Strategy 6) is critical in the effort to develop safe and secure AI; it requires research into how effective and meaningful standards can be developed and adapted to the broad array of applications in which AI will be used.
This strategy divides the safety and security R&D program along two lines of effort: Building Safe AI and Securing AI.
Building Safe AI
As AI becomes commonplace and its applications proliferate, the need for a national approach to research on AI and safety becomes increasingly urgent. This research includes developing methods for creating, evaluating, deploying, and monitoring AI that are focused on safety.
With datasets and models growing larger and more complex, there is an urgent need for solutions that can scale with these larger systems. Additionally, there is a need for a national innovation ecosystem53 that can democratize the tools for accessing AI models at this scale, making analysis of such large models accessible to the broader community and beyond the groups that are capable of investing in the infrastructure to develop and deploy them. This approach would enable a larger field of researchers to address safety and security concerns relating to these larger models, including those related to bias, accuracy, and functionality.
More research is needed to develop safe human-machine interactions. Exploration of new formal methods could characterize boundaries of behavior and bring much-needed rigor to safety-critical AI algorithms and applications. These techniques include novel programming languages and compilers to
-16-

develop more robust AI, formal verification techniques for AI systems that could provide assurances of safety, and neurosymbolic programming that could bridge the areas of deep learning and program synthesis. Addressing AI systems-of-systems, in which the AI system is only one component of a larger system, or a large system composed of many AI and classical subsystems, is one of the most pressing challenges in testing systems at scale. Methods and approaches need to be developed to independently verify subsystems within the context of their operating framework and to evaluate the performance of the overall construct to ensure that the ensemble will operate safely, and that security of the overall system is not harmed by subsystem interactions.54 New testbeds and prototyping facilities could enable this area of research.
Long-term risks remain, including the existential risk associated with the development of artificial general intelligence through self-modifying AI or other means. Other long-term risks are related to the possible deep entangling of AI systems into all parts of daily life and systems, which may cause large-scale societal or environmental issues that are currently difficult or impossible to predict; or specification gaming, whereby an AI system gradually learns to achieve numerical requirements but somehow avoids accomplishing the desired task. These risks are difficult to quantify currently and need additional research.
Securing AI
The national need for secure AI is growing as software and systems are growing more complex but also increasing our collective vulnerability to cybersecurity threats.55 This is echoed in both the desire for additional training among practitioners within government agencies, and in the recognition of AI security as an independent field of study adjacent to cybersecurity and AI. Such a field is needed to address the many open questions still surrounding the multifaceted issues of AI security, such as the need for appropriate metrics for goal alignment, protection against adversarial attack, scalable methods, and trade-offs between interpretability and accuracy.56
Adversarial AI includes “data poisoning,” in which AI training or input data are manipulated, and other forms of adversarial attacks against AI, such as targeting systems linked to the AI or manipulating objects in the physical world. Changes to audio or visual data that cannot be perceived by humans can change how an AI system processes data. This is particularly salient for ML systems. Some of these risks can be identified through red teaming, where trusted partners act as adversaries in a simulated compromise attempt, and other risks can be mitigated through mathematical modeling. Research is needed to better enable both approaches.
An additional threat to AI systems is the existing AI-development supply chain. As only a few tools are currently used for AI system development and deployment, there is a risk that these tools could present a vehicle for systems to be compromised. Efforts should be made to protect these tools from manipulation, and to develop a more robust toolset to protect the AI-development supply chain.57
Research into improved methods for ensuring the security of AI systems is critical, including work on improving the capability of systems against input data manipulation, model inversion, and other forms of adversarial attack. The many-against-one nature of an AI system’s vulnerability needs to be addressed, as AI systems open more pathways for disruption that most systems. Only one needs to be successful, while the AI system must protect against all.58
-17-

Strategy 5: Develop Shared Public Datasets and Environments for AI Training and Testing
Progress in AI is increasingly linked to data and computation. The availability of well-purposed (i.e., legally, and ethically collected and managed) data for AI training and testing enables research applications, scientific discovery, and operational efficiencies. A well-designed cyberinfrastructure can aid data federation, support metadata, track provenance, and enable reproducibility. Similarly, access to advanced computing, including HPC, edge computing, cloud resources, traditional desktop computing, and emerging computing paradigms, drives AI innovation. At the same time, the challenges for researchers to access at-scale data and computing resources continue to pose significant obstacles for the field. For example, many AI researchers are departing academia for industry settings where such resources are more readily available. Similarly, with resources concentrated in large technology companies and well- resourced universities, the divide between those with access and those without has the potential to adversely skew AI research. Researchers who lack access to rigorous data and computation will simply not be competitive.
With the goal of democratizing access to at-scale AI data and compute resources, the NAIRR Task Force has published a roadmap and implementation plan for a national research cyberinfrastructure that would connect researchers to data, computation, testbeds, and associated training.59 The work of the NAIRR Task Force builds on other efforts to enhance access to these diverse resources. For example, since their introduction in 2016, the Findable, Accessible, Interoperable, and Reusable (FAIR) Guiding Principles60 for data have seen tremendous acceptance by the scientific research community. In 2019, the OPEN Government Data Act61 mandated that the federal government, through collaboration and coordination, provide open data, engage in evidence-building activities, enhance statistical efficiency, uphold confidential information protection, and, where data is about humans, respect privacy.
This strategy is divided into four lines of effort: Developing and Making Accessible Datasets to Meet the Needs of a Diverse Spectrum of AI Applications; Developing Shared Large-Scale and Specialized Advanced Computing and Hardware Resources; Making Testing Resources Responsive to Commercial and Public Interests; and Developing Open-Source Software Libraries and Toolkits.
Developing and Making Accessible Datasets to Meet the Needs of a Diverse Spectrum of AI Applications
Sustaining access to well-purposed training and testing datasets is crucial for ensuring scientifically reliable, reproducible, ethical, and equitable results. While there is value in simplified and synthetic datasets for algorithm research, development, and testing, other datasets must be sufficiently representative to effectively tackle challenging, real-world problems. Dataset documentation must include data provenance and references to previous work with the data. These will facilitate the ability of researchers to compare multiple datasets generated by the same system or process and clearly describe changes in the system that yield any differences in the data. The technical and sociotechnical infrastructure necessary to support reproducible research has been recognized as an important challenge—and is essential to AI systems as well. The current infrastructure and the level of documentation and curation of datasets are mostly inadequate and vary significantly by research area.
-18-

Many machine learning applications need their training data to be integrated, cleaned, and refined in order to be usable. Specific, detailed user and system requirements, methods by which the data were collected, and any factors (e.g., sensor noise) that lead to noise or other artifacts in the data will drive how the data can be made “ready” for use in AI applications (Strategy 6). As with computational infrastructure and testbeds, data infrastructure needs to be designed to meet the specific demands of AI applications. The infrastructure should be developed with community input and continually re-evaluated and updated as technology advances and the research problems and endeavors evolve. Many government datasets are already available to researchers and students on various websites and platforms (e.g., substantial National Aeronautics and Space Administration [NASA] Earth Science datasets are available through NASA’s Distributed Active Archive Centers62), though not all are well known, easy to find, or easy to use.63, 64 For example, the different processes by which government data and government-funded data are publicly available or licensed for external use can be confusing and time consuming to navigate.
Easing access to federal government data,65 when appropriate, can increase the use of existing resources for developing and studying AI.66 This includes potential benefits from creating agreements, templates, or processes for data access that can be shared across agencies so that researchers and students no longer face an array of different requirements to gain access to different datasets. Government data are often suitable for inclusion in standardized training datasets and benchmarks within the AI research community. When appropriate, agencies may identify opportunities to share data across agency boundaries or contribute agency data to standardized resources that have the potential for widespread usage by researchers and practitioners. Likewise, the development of better methods for moving data and computing closer together is important, as these methods decrease the costs and time associated with analysis. For example, the National Institutes of Health (NIH) S&T Research Infrastructure for Discovery, Experimentation, and Sustainability (STRIDES) Initiative67 enables access to rich datasets and advanced computational infrastructure, tools, and services. This can involve investment in federated and distributed learning approaches and supporting infrastructure (a potential crosscut with Strategy 8) and support of federation of data archives that increasingly make use of cloud services. This includes improving data access and facilitating use of ML and other data analytics methods.
As needed, because of privacy or security, the federal government can provide secure platforms for access to government data, as the National Geospatial-Intelligence Agency and the Department of Defense’s Chief Data and Artificial Intelligence Office have done, and as is envisioned for the recently authorized National Secure Data Service demonstration.68 Data providers also may be able to make otherwise confidential data available through removal of some information not critical to analytics, as is done with NASA’s commercial aviation data.69 Such data access also requires having Identity Access Management in place.
Alongside easing access to data is making data more discoverable and usable. There is a research need for exploring rational methods for linking related datasets, such as the development of open knowledge70 graphs for data discovery and compilation across sources and users.
In addition, increasing capabilities for and public access to synthetic data generation can be helpful when corresponding real data cannot be made available because of privacy concerns or because they are available only in small quantities. Other means by which to increase the amount of useful data, such as crowdsourcing and active learning to increase the number of labels, should also be considered. Thoughtful investment may be necessary to supplement existing datasets through the careful collection of more representative data (for example, the All of Us Research Program at NIH, which has focused participant enrollment to ensure a large percentage includes individuals who have been traditionally under- represented in biomedical research71). These resources can be made available more easily and in greater

-19-

quantity through the use of public-private partnerships. This requires investment in additional mechanisms to facilitate collaborations with the private sector through funding and novel mechanisms that allow academia, non-government organizations (NGOs), and other researchers to use private-sector and public-sector resources. Additionally, government or government-funded data should account for historically underrepresented communities and groups being underrepresented in data. For example, support is needed to generate natural language processing tools for underrepresented languages and those that do not have a written form.
Datasets also need to document context (e.g., labeling process and sample bias) to be useful, in part by encouraging the capture of metadata that can be computationally queried and assessed. Depending on the problem, additional effort may be needed to capture sociological and contextual information and to enhance secure and privacy-preserving data linkages between informative data assets. Computer science and data experts in government may need to engage social scientists and other relevant experts in this process.
An important part of the government provision of data for AI is ensuring their use in a manner that reflects American values (a crosscut with Strategy 3). For example, it is critical to advance both technological and governance methods that preserve privacy, including protecting against revelations of personally identifiable information when publicly available government data are combined with other data. There also is a need for research on effective data governance that allows releasing data under secure platforms that control access to or removal of content.
Beyond ensuring that AI does not result in harm from the release of personally identifiable information, there is also a public need to demonstrate how datasets can help overcome inequities. This could include creating curated datasets for analysis of past inequities, such as digitizing “redline” maps originally developed by the Home Owners Loan Corporation in the 1930s. This analysis can be used to avoid replicating disparities, and can help increase access to safe and sanitary housing combined with flood maps.
Developing Shared Large-Scale and Specialized Advanced Computing and Hardware Resources
Innovation in AI is dependent not just on data, but also on access to advanced computing. Large universities, federal laboratories, and private-sector firms often have access to such capability, which can take the form of HPC, cloud, hybrid, and/or emerging systems. But many researchers and students at smaller institutions of higher education, minority-serving institutions, community colleges, secondary schools, and startups and small businesses may have less access or fewer resources to purchase the computing needed to conduct AI R&D.
To lower barriers to entry into AI R&D, enhanced access to advanced computational resources is necessary, particularly for the variety of new users who otherwise would face financial, logistical, or capacity challenges to engaging in the AI research ecosystem. Expanded access should be provided by leveraging existing resources in all sectors, augmenting the capacity of federally provided resources as appropriate, creating new research computing infrastructure to serve the AI R&D community, and providing financial support where needed.
To this end, the NAIRR Task Force has put forward a roadmap and implementation plan leading to a mix of computational resources (i.e., on-premises and commercial cloud, dedicated, and shared resources) with a range of central processing unit (CPU) and graphics processing unit (GPU) options with multiple accelerators per node, high-speed networking, and sufficient memory capacity.
-20-

Making Testing Resources Responsive to Commercial and Public Interests
The growing complexity of AI systems has created a need for equally robust AI testing resources. In many cases, these resources are developed alongside the technology itself by private industry or the research community at large. However, this approach to AI testing leaves certain concerns unaddressed. First, novel AI research often experiences limited testing because of difficulties with replication.72 Second, AI systems developed by private industry often do not have mechanisms for public qualitative evaluation and testing.73 Finally, for research institutions or private industry, certain areas of testing, especially surrounding large-scale AI models, are not economical to pursue in isolation, and these areas are left underexplored as a result.
Federal AI testing resources, primarily in the form of testbeds and testing frameworks, can address the limitations of existing testing paradigms. For example, the NIST Facial Recognition Vendor Test (FRVT) helps provide insight into the accuracy of otherwise private facial recognition algorithms,74 and the Guaranteeing AI Robustness against Deception program at the Defense Advanced Research Projects Agency supports novel testing mechanisms in ML security by means of a virtual testbed, toolbox, and benchmarking dataset.75 Similar approaches could be employed for other common AI applications, including voice-assistant software and recommender systems.
Expanding the scope of federal testing resources is critical to the healthy adoption of emerging AI systems. As agencies develop new testbeds, both foundational AI and application-specific AI should be considered. In addition, new testing efforts may also inform (or conversely, be contingent on) emerging AI standards and benchmarks. Awareness and coordination between these efforts is likely to improve the efficacy of both. Finally, because of the rapid rate of AI R&D, test framework designers should pay close attention to changing trends in software, hardware, and research focus to plan for the longevity of their work.
Developing Open-Source Software Libraries and Toolkits
Another area for government investment involves providing access to and support for open AI software libraries. Access to and continued support for libraries and toolkits can accelerate R&D, from conducting fundamental research through facilitating technology translation, as the same libraries may be used for a wide range of services, including commercial ones. The growth in open software libraries and toolkits has enabled a corresponding growth in AI applications and skills. Researchers and students across sectors use open-source tool sets. Government agencies also develop open software libraries and toolsets specific to mission needs in which industry lacks market incentives to develop the products for the government or other sectors. Many agencies and agency-funded researchers make code available through GitHub or other commonly used commercial platforms that provide resources for researchers and students. Also, prior to commercial interest, the federal government may need to incentivize continued development, maintenance, and curation of software and tools to prevent them from becoming outdated. As an example, NSF’s Pathways to Enable Open-Source Ecosystems program aims to harness the power of open- source development for the creation of new technology solutions to problems of national and societal importance.
-21-

Strategy 6: Measure and Evaluate AI Systems through Standards and Benchmarks
Standards, benchmarks, testbeds, and their adoption by the AI community are essential for guiding and promoting R&D on AI systems, and the recognition of this role continues to rise in the United States and globally. Both the 2019 Executive Order on Maintaining American Leadership in Artificial Intelligence76 and the NAIIA77 explicitly call out the importance of standards. In addition, the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) have convened a joint technical subcommittee on AI (ISO/IEC Joint Technical Committee 1,78 Subcommittee 42 on Artificial Intelligence79) to develop standards and associated considerations for AI systems. The surge in AI-related standards activities has outpaced the launch of new AI-focused benchmarks and evaluations, particularly as related to the trustworthiness of AI systems. Considerations of fairness and bias in benchmark datasets have become increasingly important. Much more plentiful are benchmarks that test the application-level performance of AI algorithms (e.g., false-positive or false-negative rates for classification algorithms) and benchmarks that quantify the compute-level performance of AI software and hardware systems. These efforts need to be expanded to sociotechnical evaluations of AI systems and assessing the broader impact of AI technologies.
Assessing, promoting, and providing assurances on all aspects of AI trustworthiness require measuring and evaluating AI technology performance through benchmarks and standards. Beyond being safe, secure, reliable, resilient, interpretable, and transparent, trustworthy AI must preserve privacy while detecting and avoiding inappropriate bias. Claims of trustworthiness and compliance must also be verifiable and certifiable. As AI systems evolve, so will the need to develop new metrics and testing requirements for validation of these essential characteristics.
The evaluation, standards, and benchmarks of AI systems need to acknowledge underrepresentation of certain communities, and approaches are required to solve this issue theoretically and practically. There is a need to proactively identify underrepresented communities, including Indigenous groups, and to include diverse stakeholders and domain experts from government, academia, the private sector, and civil society, including representatives from differently sized organizations and lower- and middle-income socioeconomic groups and countries, to ensure fairness and prevent bias in the development of standards and benchmarks.
It is necessary to develop standard ways to measure, test, and report the potential societal effects of commonly used datasets such as ImageNet.80 A user-friendly acknowledgment of privacy considerations regarding an organization’s collection, use, and sharing of personal information as well as a label of ethical assurance could help promote trustworthy AI systems. A potential example of a voluntary program that effectively fosters trust based on standards-based assessments is the Leadership in Energy and Environmental Design program for building certifications.81
This strategy divides the areas in which additional progress must be made along five lines: Developing a Broad Spectrum of AI Standards; Establishing AI Technology Benchmarks; Increasing the Availability of AI Testbeds; Engaging the AI Community in Standards and Benchmarks; and Developing Standards for Auditing and Monitoring of AI Systems.
Developing a Broad Spectrum of AI Standards
The development of standards must be hastened to keep pace with the rapidly evolving capabilities and expanding domains of AI applications. Standards provide requirements, specifications, guidelines, or characteristics that can be used consistently to ensure that AI systems meet critical objectives for
-22-

functionality and interoperability, and that they perform reliably and safely throughout their operational lifecycle. There is a need to achieve consensus-based provision of precise definitions of technical terms and consistent terminology (e.g., AI, autonomy, transparency, explainability, and interpretability) within the domain of safety and security.
Adoption of standards brings credibility to technological advancements and facilitates an expanded interoperable marketplace. One example of an AI-relevant standard that has been developed—by the Institute of Electrical and Electronics Engineers—is P1872-2015 (Standard Ontologies for Robotics and Automation). This standard provides a systematic way of representing knowledge and a common set of terms and definitions. These allow for unambiguous knowledge transfer among humans, robots, and other artificial systems. Another example of an AI-relevant standard is ISO/IEC 22989:2022 (Information technology—Artificial intelligence—Artificial intelligence concepts and terminology), developed within ISO/IEC Joint Technical Committee 1 SC 42,82 which defines terminology and concepts related to AI. Additional work in AI standards development is needed across all subdomains of AI. For example, there should be support for an Open Knowledge Network standard to address the limitations of current deep learning systems.83,84
As part of the NAIIA, Congress directed NIST to develop an AI risk management framework, a voluntary tool that organizations can use to evaluate, assess, and manage risks that may result from the use of AI. The Framework leverages standards and best practices that organizations can use to achieve stated outcomes. Further support is needed for research initiatives that tackle questions related to understanding and operationalizing the risks and harms of applications of AI systems so that risk ratings, certifications, and insurance become feasible for AI systems.
One of the key challenges to developing standards in new areas is achieving a sufficient shared understanding of the underlying issues necessary for the standards to serve useful purposes. Additional efforts are needed to inform and create standards that support the following:
Software engineering to manage system complexity, sustainment, and security, and to monitor and control emergent behaviors.
Functionality and trustworthiness to assess an AI system’s validity and reliability, safety, security and resilience, privacy, interpretability, and bias as well as the tradeoff among the mentioned trustworthiness characteristics.
Metrics to quantify factors impacting performance and compliance with standards. Safety to evaluate risk management and hazard analysis of systems, human-computer interactions,
control systems, and regulatory compliance. Usability to ensure that interfaces and controls are effective, efficient, and intuitive.
Interoperability to define interchangeable components, data, and transaction models via standard and compatible interfaces.
Security to address the confidentiality, integrity, and availability of information, and cybersecurity. Privacy to control for the protection of information while being processed, when in transit, or while
being stored or used.
Fairness and interpretability to ensure that AI systems’ harmful bias is managed and able to help humans understand their operation and outputs.
-23-

Flexibility to avoid a rigid lock-in that may lead to workarounds, lack of compliance, and other harmful spillover effects.
International collaborations to that can support responsible AI development and thoughtful policy both domestically and abroad.
Traceability to provide a record of events (their implementation, testing, and completion), and to curate data.
Domains to define use-inspired standard lexicons and corresponding frameworks.
For example, consider the domains of healthcare and manufacturing. In the United States, nearly $4 trillion is spent on healthcare each year, and healthcare data today are fragmented, often incomplete, and difficult to access. This limits AI capabilities in healthcare. A significant improvement to AI algorithms—for safety, reliability, and trust—can be obtained through improvements to data access, standards for metadata that capture important social characteristics, and a balance that achieves privacy for the individual and enables ethical, legal, and societal validation. Manufacturing is a major contributor to the U.S. economy, and research is needed on data standards for AI in manufacturing.85 With the passage of the CHIPS and Science Act of 2022,86 there will be an expanded role for AI in semiconductor design and manufacturing, where standards will aid in further innovation.
Impact assessments can expose preventable harm, encourage consultation with affected communities, and standardize the information available for further research about which AI systems are used in which contexts and for what purposes. The development of methodological standards for these assessments is especially critical for ensuring that impact assessments are done in the public interest, and for preventing the proliferation of assessments that manipulate or obscure harmful impacts of applications of AI systems.
Finally, the real-world performance and energy efficiency of AI models remain poorly quantified. One recent study found that the carbon footprint of a large language model nearly doubled when equipment manufacturing and idle consumption during training were taken into account.87 Development and adoption of standards, including the documentation of hardware and training details, may allow better management of the nuances of the environmental performance of AI, which in turn, informs its responsible use.
Establishing AI Technology Benchmarks
Benchmarks, comprising tests and evaluations, provide quantitative, qualitative, or mixed method measures for developing standards and assessing compliance to standards. Benchmarks drive innovation by promoting advancements aimed at addressing strategically selected scenarios; they additionally provide objective data to track the evolution of AI science and technologies. To effectively evaluate AI systems, relevant and effective testing methodologies and metrics must be developed and standardized. Standard testing methods will prescribe protocols and procedures for assessing, comparing, and managing the functionality and trustworthiness of AI systems. Standard metrics are needed to define measures to characterize AI systems, including, but not limited to accuracy, complexity, trust and competency, risk and uncertainty, explainability and interpretability, unintended bias, comparison to human performance, and economic impact. It is important to note that benchmarks are driven by data. Research needs to be done on how to construct benchmarks that test for more than accuracy under assumptions that data are independent and identically distributed. Strategy 5 discusses the importance of datasets for training and testing.
-24-

Frequently, AI performance is evaluated using only a handful of typical metrics (e.g., accuracy, precision). While these metrics are useful for development, they do not provide end-to-end contextual information. For instance, for AI systems developed to improve maintenance, metrics associated with repair times and overall system availability will be more informative than the accuracy of a maintenance action prediction. Hence, testing should also use metrics that are operationally relevant to the use context for an AI system. Further, datasets used should be dynamic in the sense that they should be enhanced by new data and connected to domain problems with human committees and evaluators, not just provide metrics numbers.
While prior efforts provide a strong foundation for driving AI benchmarking forward, they are limited by being domain-specific. Additional standards, testbeds, and benchmarks are needed across a broader range of domains to ensure that AI solutions are broadly applicable and widely adopted. The federal government should validate and collate evaluations created by independent researchers to create a catalog of approved tests for deployed models and those in development. It is useful to emphasize characterizing performance across use conditions; an AI system can be deployed with constraints that limit it from working in conditions where its performance is degraded, or it is more likely to do harm.
Increasing the Availability of AI Testbeds
As noted in one recent report: “Testbeds are essential so that researchers can use actual operational data to model and run experiments on real-world system[s] ... and scenarios in good test environments.”88 While some AI testbeds exist,89 adequate testbeds are needed across all areas of AI. As an example, although the federal government has massive amounts of unique and mission-sensitive data, many of these data cannot be distributed to the extramural research community. Appropriate programs should be established for academic and industrial researchers to conduct research within secured and curated testbed environments established by federal agencies. AI models and experimental methods can be shared and validated by researchers if they have access to these test environments, affording AI scientists, engineers, and students unique research opportunities not otherwise available. It is necessary to create standardized testing frameworks and benchmarks that allow for effective evaluation of AI systems to ensure that they are performing appropriately for a given use case in a way that is fair, safe, secure, and reliable, as well as to develop new tools for test, evaluation, validation, verification, and monitoring—and to assure the reliability of AI systems over their full domain of use and life cycle. A NAIRR, as outlined by the NAIRR Task Force, would support this goal.
Engaging the AI Community in Standards and Benchmarks
Government leadership and coordination are needed to support standardization and encourage its widespread use in government, academia, and industry. The AI community—comprising government, academia, industry, and civil society, including end users—must be energized to participate in developing standards and benchmark programs. As each government agency engages the community in different ways based on its role and mission, community interactions can be leveraged through coordination to strengthen their impact. This coordination is needed to collectively gather user-driven requirements, anticipate developer-driven standards, marshal the expertise of the AI R&D community, and promote educational opportunities. User-driven requirements shape the objectives and design of challenge problems and enable technology evaluation. Community benchmarks allow R&D to define progress, close gaps, and drive innovative solutions for specific problems. These benchmarks must include methods for defining and assigning ground truth. The creation of benchmark simulation and analysis tools will also accelerate AI developments. The results of these benchmarks will help match the right technology to the
-25-

user’s need, forming objective criteria for standards compliance, qualified product lists, and potential source selection.
Industry and academia are the primary sources for emerging AI systems. Promoting and coordinating R&D subject matter expert participation in standards and benchmarking activities are critical. As solutions emerge, opportunities abound for anticipating developer- and user-driven standards through sharing common visions for technical architectures, developing reference implementations of emerging standards to show feasibility, and conducting precompetitive testing to ensure high-quality and interoperable solutions, and to develop best practices for technology applications.
AI practitioners carry critical domain expertise on testbeds for AI, and their expectations can play a major role in developing AI systems. As a result, it is crucial to broaden AI education to a variety of industries and encourage the AI community to further engage in standards development for evaluating AI systems. Furthermore, it is even more crucial to bridge the gap between practitioners’ expectations and AI researchers to achieve a harmonious development cycle between AI technology developers and users. It is also important to collaborate with industry consortia and affected communities.
Developing and adopting standards, as well as participating in benchmark activities, comes with a cost, and R&D organizations engage in these activities when they see significant benefit. Updating acquisition processes across agencies to include specific requirements for AI standards in requests for proposals will encourage communities to further engage in standards development and adoption. Community-based benchmarks such as the Text Retrieval Conference90 and FRVT91 also lower barriers and strengthen incentives by providing types of training and testing data otherwise inaccessible, fostering healthy competition between technology developers to drive best-of-breed algorithms, and enabling objective and comparative performance metrics for relevant source selections. There is also a need for improved testing methodologies and resources that would allow agencies to directly evaluate cloud-hosted AI capabilities.
Developing Standards for Auditing and Monitoring of AI Systems
AI systems will need to be properly audited and regularly monitored to identify and mitigate risks, both technical (e.g., accuracy, reliability, and robustness) and sociotechnical (e.g., bias and privacy). There are many unresolved research questions about how to effectively audit and monitor AI systems, and the scalability of auditing is emerging as a significant practical challenge. As AI systems proliferate and find their way into more realms of human activity, it is imperative to develop scalable auditing techniques, create new types of qualitative analysis tools, train enough people to carry them out, receive feedback from humans in the loop, and build institutional capacity in government and industry to undertake, oversee, and respond to audits.
-26-

Strategy 7: Better Understand the National AI R&D Workforce Needs
Rapid advancements in AI continually impact the workforce by creating a growing demand for qualified computer and information science professionals and for new skills in the broader workforce now or soon using AI systems daily.
Within the United States, computer and information science positions are projected to grow by 22 percent between 2020 and 2030.92 Private industry is expected to lead this demand with its sustained financial support and access to advanced computing facilities and datasets.93 The resulting economic growth could be large: AI research is expected to contribute as much as $11.5 trillion in cumulative growth across G20 countries alone over the same period.94
Fortunately, interest in AI study and careers remains high. However, U.S. academic institutions are struggling to keep pace with the explosive growth in student interest and enrollment in AI.95 Furthermore, while booming enrollments are common at the undergraduate level in AI-related fields such as computer science, doctoral enrollment trends show steady decreases in U.S. citizen and permanent resident enrollments. This has impacts on the AI workforce, particularly in government positions such as those requiring security clearances. Overall, these trends put an onus on government to better understand workforce needs and take steps to develop and support AI talent, with the goal of creating a sustainable AI workforce for government, academia, and industry. Moreover, the trends in computing and information science need to be complemented by those in other disciplinary areas that also contribute to AI discovery and innovation, such as the social and behavioral sciences, economics, and systems engineering.
This strategy is divided into ten lines of effort: Describing and Evaluating the AI Workforce; Developing Strategies for AI Instructional Material at All Levels; Supporting AI Higher Education Staff; Training/Retraining the Workforce; Exploring the Impact of Diverse and Multidisciplinary Expertise; Identifying and Attracting the World’s Best Talent; Developing Regional AI Expertise; Investigating Options to Strengthen the Federal AI Workforce; Incorporating Ethical, Legal, and Societal Implications into AI Education and Training; and Communicating Federal Workforce Priorities to External Stakeholders.
Describing and Evaluating the AI Workforce
Data on the current AI workforce, including its participants, their roles and tasks, and the knowledge and skills required to perform these tasks, is critical to properly understanding the workforce’s abilities, gaps, and needs. With extensive, properly prepared, and well-ordered data, the United States can gain reliable clarity on the status quo of the AI workforce. Clarifying the understanding and priorities for a strong AI workforce in the United States will help focus efforts and investments across sectors. Further, illuminating the demographic disparities and gaps in the AI workforce will provide policymakers and human resource professionals with information necessary to address these disparities and increase equity and diversity. Facilitating this work could provide incentives for employers in various sectors to improve their data collection methods, consolidate existing workforce datasets, and support the creation of a modernized labor database. Research necessary to facilitate and reinforce this effort should focus on developing
-27-

proper data, knowledge and skills, and workforce curation and analysis techniques, including the enterprise and architectural needs of a modernized workforce.
The CHIPS and Science Act of 2022 takes a step in this direction by authorizing NSF to generate a study of U.S. universities that conduct high-impact AI research to better understand what factors enable AI progress. In particular, the report should contain information about university computing power, dataset availability, specialized curricula, faculty and graduate students, sources of federal and non-federal research funding, and industry and other partnerships, with the intention of implementing successful practices across the academic ecosystem. Such a study could help ensure that AI workforce needs such as adequate resources and institutional support are well-understood and integrated with complementary workforce needs such as beneficial training and skills.
Developing Strategies for AI Instructional Material at All Levels
The United States would benefit from making AI research accessible to a wide range of Americans. Moreover, exposing students at all levels, starting at the primary and secondary levels, to AI and data science prepares them for successful integration into a world that is rapidly adopting AI.
High-quality, domain-specific, and appropriately challenging lessons are needed for introducing students to critical thinking skills that will help them understand and evaluate AI systems. The research required to properly identify and curate the right content for a given area and level of study requires considerable effort. Further research is needed to sort out the best pedagogy and media through which to convey this content, as well as to identify and curate best practices for training instructors. It is important to facilitate the engagement of other public- and private-sector entities in this research and ensure demographic and cultural equity in that engagement.
Additionally, it is important that any AI materials, training programs, or systems are accessible, equitably promulgated, and broadly representative, especially given current inequalities among students’ and educators’ access to resources.
Supporting AI Higher Education Staff
At the most advanced levels, some AI researchers in university positions (e.g., tenured or tenure-track faculty) are moving toward industry R&D. Workforce efforts should also study opportunities to ensure a sufficient university workforce to educate future generations of the AI workforce in two-year and four- year colleges and universities, spanning associate’s, bachelor’s, master’s, and doctoral degree programs. These efforts could include joint appointments enabling faculty to engage across sectors.
Training/Retraining the Workforce
Similarly, there are opportunities to upskill individuals who will be using AI systems in their current lines of work. To do so, the federal government must prioritize developing programs and systems that support the identification and recruitment of AI talent and the assessment, training, and validation of AI skills and knowledge. These programs and systems should leverage AI to maximize their relevance and impact. They should instill standardization, interoperability, and democratization. Once developed, these programs and systems will continue to foster the development of AI-competent workforce and support personnel displaced by AI deployment.
Pursuant to this, research partnerships among government, academia, and industry must be cultivated. These partnerships should prioritize creating on-demand courses that benefit from the best pedagogy and oversight available to a diverse workforce. These courses must be equitably and accessibly available to all.
-28-
The National Artificial Intelligence R&D Strategic Plan

Additionally, rapid and well-informed development of grand challenges for worker training and retraining programs and systems should be explored. Grand challenges are an exceptional joint research, development, and acquisition method that allows the government to leverage its partnerships, technologies, and other assets to tackle hard problems such as workforce development.
Exploring the Impact of Diverse and Multidisciplinary Expertise
Safe and equitable AI development and deployment requires a broad understanding of the people and places affected by AI as much as deep technical knowledge of the AI itself. Multidisciplinary education across diverse fields can be beneficial for ensuring fair and equitable access to information and opportunity, democratization of new and emerging technologies, and the development of a diverse a marketplace of ideas around technology use and development. Moreover, AI must be developed and managed from a holistic perspective that integrates knowledge from various disciplines and backgrounds to foster an interdisciplinary and transdisciplinary approach that considers the needs of all Americans. As such, hiring for teams that make and/or use AI should emphasize diversity from academic, professional, and experiential perspectives.98
To facilitate this approach, federal researchers should leverage their unique position and perspective to spearhead research into the roles and impacts of different areas of study on the realities and future of AI. As a result, researchers will understand how to engage diverse perspectives and align their efforts and resources with national needs and priorities, as well as across all sectors.
These actions should be taken in addition to other efforts to increase the diversity of communities, identities, races, ethnicities, backgrounds, abilities, cultures, and beliefs involved in AI R&D. The federal research community should prioritize research on the best way to increase demographic and cultural representation in the federal AI workforce.
Identifying and Attracting the World’s Best Talent
The United States is home to an abundance of talent in many areas but has historically relied on foreign- born talent to bolster its technology workforce—especially in R&D in emerging technologies. Half of the current AI experts in U.S. academia and industry were born outside of the United States.99,100,101 Federal resources can support university, industry, and civil society efforts to host visiting students and scholars with pathways to U.S. citizenship.
Fostering international partnerships with foreign governments and universities in support of Strategy 9 also serves to address this strategy.
Developing Regional AI Expertise
The size and diversity of the United States makes it useful to synthesize inputs and expertise from various parts of the country. Leveraging different geographical regions can facilitate equitable and broad dispersion of AI training and the economic opportunities, while also accessing a diversity of represented perspectives for contribution and feedback. In addition, by coordinating geographically concentrated resources such as data and computing infrastructure, a highly skilled local workforce, and complementary industry presence (e.g., cybersecurity, data science), regional synergies could foster local participation in the AI-enabled economy, facilitate high-quality AI training, and accelerate AI research progress at the national level.
In complementary fashion, federal efforts should be directed toward fostering regional efforts that enable access to the AI economy in historically underserved areas, including in rural areas and on tribal lands.
-29-

Such efforts will ensure that opportunities are provided to a broad array of Americans, allowing AI research efforts to draw upon diverse perspectives that may be underrepresented in current efforts.
Investigating Options to Strengthen the Federal AI Workforce
The federal government should fund and execute research efforts to determine the feasibility of different options for strengthening the federal AI workforce. Federal efforts could accelerate and leverage the growing number of AI K-12 education and workforce development programs to build partnerships among government, academia, and industry, helping to recruit and train early-career private-sector professionals and traditional students to engage with federal agencies in the areas of digital transformation, data management, analytics, and AI. Such partnerships could potentially also include rotations and/or work in local, state, and federal government organizations, accelerating and supporting the deployment of AI across the public sector. Efforts to strengthen the federal AI workforce should include a focus on training federal AI professionals so they are able to design systems that support the rights and safety of the public and mitigate the residual risks to them.
Along these lines, the AI Training Act directs the federal government to develop and provide an AI training program for a substantial portion of the federal AI workforce. The CHIPS and Science Act of 2022 authorizes NSF to study and establish a federal AI scholarship-for-service program to recruit and train the next generation of AI professionals across the federal government. It also clarifies that individuals studying AI-related topics are eligible for the existing NSF CyberCorps: Scholarships for Service program, enabling the program to begin specifically recruiting individuals with an interest in applying AI skills to federal projects in the future.
Incorporating Ethical, Legal, and Societal Implications into AI Education and Training
The ethical, legal, and societal implications of AI have become increasingly salient in recent years and will continue to be so. As such, it is vital for those who develop, use, and oversee AI systems to be conversant in these topics and committed to upholding the associated values. Experts are needed who are conversant in these issues and in data science and AI systems, and who can help educate the workforce and inform education and upskilling curricula. Also needed are policy, law, and governance experts who are conversant in the ethical, legal, societal, and technological aspects of AI topics.
Unfortunately, current academic programs that create qualified experts in any one of these three areas are challenged to offer education in the others. To address this challenge, the federal government should support undergraduate and graduate programs, as well as postdoctoral opportunities that designed to build interdisciplinary competencies, and support research into and dissemination of education materials on ethical, legal, and social aspects of AI for integration in AI education and training programs.
Communicating Federal Workforce Priorities to External Stakeholders
Educating private-sector institutions, higher-education institutions, and the public about the federal government’s workforce needs and priorities and how to support fulfilling them is a critical step along the path to intersectoral alignment and optimization. Workforce description, recruitment, and development must be fair, transparent, and accountable, and that expectation should be conveyed consistently to all stakeholders in all lines of effort. Federal agencies can carry out these communications through posting of success stories in the media, outreach to small and minority-owned businesses, representation in talks and booths at industry trade shows, participation in scientific conferences that span the spectrum of disciplines surrounding AI, and program funding announcements. Other opportunities include education and workforce programs intertwined with research, as in the National AI Research Institutes and extant collaborations among university faculty and students, industry representatives, and the federal government.
-30-

Strategy 8: Expand Public-Private Partnerships to Accelerate Advances in AI
American leadership in science and engineering research and innovation is rooted in the U.S. government- university-industry R&D ecosystem. As the American Academy of Arts and Sciences has written, “America’s standing as an innovation leader” relies on “establishing a more robust national Government- University-Industry research partnership.”102 Since the release of the first National AI R&D Strategic Plan, multiple administrations have amplified “the increasing importance of effective partnerships between the federal government and academia, industry, other non-federal entities, and international allies to generate technological breakthroughs in AI and to rapidly transition those breakthroughs into capabilities.”103
Over the last several decades, fundamental research in information technology conducted at universities with federal funding, as well as in industry, has led to new multibillion-dollar sectors of the Nation’s economy. Concurrent advances across government, academia, and industry have been mutually reinforcing and have led to an innovative, vibrant AI sector. The growing importance of public-private partnerships was reflected in the addition of Strategy 8 in 2019, and has become more apparent since then, as highlighted here. The three forward-looking themes of this strategy are: Achieving More from Public-Private Partnership Synergies; Expanding Partnerships to More Diverse Stakeholders; and Improving, Enlarging, and Creating Mechanisms for R&D Partnerships.
Achieving More from Public-Private Partnership Synergies
The private sector often views AI as a high-potential new tool for business and operational interests, whereas public funding in AI research has focused on longer-term impacts and societal good. These complementary perspectives can and should be further integrated into an overall whole.
By leveraging resources, including facilities, datasets, and expertise, the strategists and participants in public-private partnerships will more rapidly advance science and engineering innovations. For example, sharing AI artifacts, models, data, and results serves to reduce resource use and redundancies. Similarly, government-university-industry R&D partnerships bring pressing, real-world challenges faced by industry to university researchers, enabling use-inspired research, and leveraging industry expertise to accelerate the translation of open and published research results into viable products and services in the marketplace for economic growth. Public-private partnerships are especially well served when they build on joint engagements among federal agencies that enable collaboration and better return on investment in areas where agencies’ missions intersect.
Continued support for cross-government efforts104 such as the National AI Research Institutes105 is key to long-term R&D partnership progress. These coordinated investments advance responsible foundational and use-inspired AI research in collaborations that benefit from a range of direct and indirect partnerships among governments, academia, industry, non-profits, communities of practice, and civil society. Researchers trained in these environments are well-prepared to expand on such approaches in years to come.
Expansion and extension of multiple types of programs that provide opportunities for researchers from government, academia, and industry to spend time working in another sector would additionally enable federal funding agencies, academia, and the private sector to work more effectively with one another. The unique perspectives and capabilities of each sector enable mutual benefit. Industry’s commercialization and scale-up of AI systems is assisted by universities’ early-stage R&D and federal laboratories’ focused materials, device, and measurement research, and specialized computing resources.
-31-

Expanding Partnerships to More Diverse Stakeholders
Expanding partnerships between the public and private sectors to include civil society organizations serves to involve those organizations’ unique perspectives in the discussion of future developments regarding the implications of AI research, development, and use. Furthermore, development of R&D approaches that focus on accountability, equity, and respect for democratic values and human rights is critical in additional considerations of AI design, development, and deployment. Equitable access to partnerships, ethical guidelines in charters, early experience with developing technologies by a wider stakeholder community, and diverse insight into the strengths and weaknesses of participant approaches yield a more robust AI infrastructure and ecosystem. Also recommended is a more concerted effort to produce international collaborations with like-minded governments, multinational corporations, and the civil society organizations of other nations, which has the potential to accelerate advances in AI for global benefit, as detailed in Strategy 9.
Translation to practice that emphasizes ethics, safety, and public good is also of high importance. Involvement of civil society and its representative organizations is critical for discussion of equitable access and use, and of trustworthiness issues. Companies of all sizes publish guidelines and focus on reducing their risks in AI product development.106 Small nonprofit organizations are major contributors to societal “AI for Good” efforts, often with substantial volunteer programs that leverage the growing pool of AI talent in the United States. Efforts to increase capacity for advisory services across sectors were recommended by the National Academies107 to help build partnerships for public good.
Collaborations between public-private partnerships and civil society organizations are particularly critical in striving for equitable access to and use of AI, and in addressing concerns about societal implications to the global ecosphere (e.g., climate change, energy security, agricultural challenges, and healthcare). Governments and international bodies play a key role in setting standards for just and responsible use.108, 109 An open-access AI collaboration ecosystem that includes large and small corporations, advanced computing capabilities and other resources only available in government agencies, and a diversity of organizations having varied perspectives, expertise, and capabilities can lead to a more ethical use of AI. These diverse collaborations lead to innovations and support new models such as partnerships between minority-serving institutions and National AI Research Institutes.110
Partnerships can also support the inherently interdisciplinary nature of AI R&D, which requires convergence between computer and information science, cognitive science and psychology, economics and game theory, the physical sciences, engineering and control theory, medicine, ethics, linguistics, mathematics and statistics, and philosophy. Bringing together this wide diversity of disciplines poses a significant research and logistical challenge (for example, in a common taxonomy), but the ultimate outcomes drive the development and evaluation of future AI systems that are fair, transparent, accountable, safe, and secure.
Improving, Enlarging, and Creating Mechanisms for R&D Partnerships
R&D is a team effort, often conducted by diverse groups operating in multiple institutions. Public-private partnerships require institutional arrangements to facilitate the pooling of resources for efficient return on investment of time and funding, faster outcomes, and positive impacts, and avoiding duplication of efforts. An array of potential configurations and mechanisms for public-private partnerships has been developed over the past few decades for a variety of AI applications.111 Expanding the reach of existing mechanisms, improving their functioning and outputs for a more diverse set of participants and application spaces, and creating new forms of public-private partnerships are significant and valuable endeavors. Examples include the following:
-32-

Individual project-based collaborations. In these partnerships, government agencies pool resources and/or expertise with industry, NGOs, foundations, and academics to address a critical issue, such as safety and trustworthiness. This is a flexible and rapid approach, but often challenging to sustain and expand.
Joint programs to advance open, precompetitive, fundamental research. Government has traditionally played a critical role in supporting foundational research through grants and contracts (primarily at universities), for which there is no short-term commercial application, but instead advances the field as a whole.112 Given the massive needs for expanded fundamental and use-inspired research, innovative methods to bring private-sector resources to these ends are critical but often challenging, given the short project timescales that profit-driven companies typically operate on. One example that addresses this challenge is the NSF Industry-University Cooperative Research Centers program,113 which provides an NSF-supported institutional framework for industry to support precompetitive research at universities. In general, non-federal partners contributing research resources can receive intellectual property rights as governed by the Bayh-Dole Act.114
Collaborations to deploy and enhance research infrastructure. Large-scale AI research will require significant research infrastructure, including compute and storage resources. Joint projects between the government and private-sector partners can achieve economies of scale that enable access to necessary resources for all engaged parties. The NAIRR115 is one example of a concept that could transform the national AI research ecosystem by providing researchers with access to computational, data, and training resources. Provision of such resources equitably to a large segment of stakeholders is critical to maximizing the impact of such collaborations.
Collaborations to enhance workforce development, including broadening participation. As discussed in Strategy 7, there is a tremendous demand for workers with AI skills. Every sector is competing for these valued workers. While there are many programs to encourage students to enter science, technology, engineering, and mathematics (STEM) fields, public-private partnerships should explore opportunities to pool resources to broaden the overall pipeline of AI R&D skills. New types of partnerships for curriculum development and new approaches to developing and implementing curricular standards for programs could be especially impactful by building broader capacity for AI education and training.
Federal prize competitions. Organizing competitions to address difficult research challenges has significant advantages for supporting R&D. In this form of partnership, the risks are introduced by the participant, not the government. Prize competitions represent only a tiny fraction of federal R&D spending, but they have proved effective at addressing a host of complex scientific and technical challenges. One difficulty has been getting from research to usable product. Research on how best to maximize impact should be enlarged. For example, competitions that are embedded in a broader structure of public-private partnerships might better enable the transition of the competition winners to deployment.116
Data and model sharing. Creating partnerships with the goal of sharing data and testbeds at scale could make a big difference in the breadth of availability of cutting-edge ML models. There are challenges, however, because trained models are a potential source of income and competitive advantage for the organizations that train them, and partnerships that require the release of these models to the public or other private organizations would likely cause these organizations to withdraw from such an arrangement. Innovation in standards and processes for equitable and responsible data sharing is urgently needed.
In each case, leveraging each partner’s strengths for the benefit of all is vitally important to achieving the greatest impact.
-33-

Strategy 9: Establish a Principled and Coordinated Approach to International Collaboration in AI Research
The 2019 Organization for Economic Cooperation and Development (OECD) Recommendation on Artificial Intelligence included investing in AI R&D as the first recommendation for national policies and international cooperation.117 While the United States leads the world in annual R&D spending, competitors seek to outpace these investments. The National Science Board’s U.S. State of Science & Engineering (S&E) 2022 report118 found that no single nation leads in all aspects of science and engineering in today’s world. In AI, the annual number of publications in the field has doubled between 2010 and 2020, and research production has become increasingly geographically dispersed.119 Ensuring that the United States remains a central hub within the AI R&D ecosystem requires ongoing participation in international programs, infrastructures, datasets, and secure data-sharing mechanisms; continued access to global talent; sustained productive international cooperation; working with existing international structures that may already regulate the data, infrastructure, and talent that the AI R&D ecosystem needs; and effective public-private partnerships. International partnerships play a key role in facilitating efforts in all these areas.
In recognition of the importance of AI to economies across the globe, the U.S. government is working to address the pressing need for better access, sharing, management, standards, and common frameworks for data and computational resources, in addition to building out the design, development, verification, validation, and use of trustworthy AI. To support this, and future AI research, development, and deployment, the AI R&D community can facilitate opportunities for international research and exchange of ideas and expertise in line with Strategy 3, including the mutual cultivation of AI international standards and cross-border frameworks that promote responsible and trustworthy AI.
This strategy is divided into four lines of effort: Cultivating a Global Culture of Developing and Using Trustworthy AI; Supporting Development of Global AI Systems, Standards, and Frameworks; Facilitating International Exchange of Ideas and Expertise; and Encouraging AI Development for Global Benefit.
Cultivating a Global Culture of Developing and Using Trustworthy AI
Groundbreaking scientific research is an inherently collaborative and international activity. Given this, global partnerships for the development and deployment of AI capabilities are integral to advancing the state of the art in AI while ensuring that the full scale of its benefits is realized in a secure, equitable, and ethical way. Around the world, “trustworthy AI” is understood as AI with attributes that conform to various ethical, legal, and societal standards. For the United States, these attributes are lawful and respectful of our Nation’s values; purposeful and performance-driven; accurate, reliable, and effective; safe, secure, and resilient; understandable; responsible and traceable; regularly monitored; transparent; accountable; and advancing equity.120
Federal research and partnership efforts can benefit from international collaboration with likeminded nations to discover and promulgate methods to support AI R&D and innovation that build public trust and confidence and realize shared values and social priorities such as equity, fairness, accountability, transparency, reliability, security, and safety. These collaborations come in many forms and through many mechanisms; examples include MOU10 (2022) with Australia’s Commonwealth Scientific and Industrial Research Organization,121 which has initiated a jointly funded research program that includes equitable and trustworthy AI; and an administrative arrangement between the United States and European Commission to further research on AI in application areas including extreme weather and climate
-34-

forecasting, emergency response management, health and medicine improvements, electric grid optimization, and agriculture optimization.122
U.S. leadership in multilateral fora such as the OECD and the Group of Seven (G7) has resulted in the 2019 OECD Recommendation on AI and the launch of the Global Partnership on AI.123 This work has paved the way for promoting research that aligns with U.S. interests and values, including safe and ethical use of AI and building a global community of practice. The United States should continue to engage and lead in these international organizations and fora to signal an interest in R&D cooperation and to send a clear message about shared interests in supporting AI R&D, innovation, and cooperation that builds public trust and confidence and respects applicable international law, individual privacy, and human rights.
Additionally, U.S. agencies should evaluate the risks of pursuing AI R&D collaboration with partners in countries that might not share democratic values or respect for human rights. When identifying opportunities for dialogue on shared AI concerns and priorities, careful consideration should be given to the benefits and risks of discussions with adversaries and competitors. In partnership with countries that share its core values, the United States should develop strategies to combat nefarious uses of AI, such as political oppression and coercion, criminal activities, violations of applicable international law, or social manipulation. Alignment of activities with the aims stated in Strategy 3 is vital.
Not only does international engagement foster research collaborations, but it also provides opportunities to directly engage international stakeholders to amplify the impact of R&D ties and showcase U.S. leadership. One can look, for example, to recent engagements with the United Kingdom and India.124
U.S. agencies can also consider R&D engagement with nations that currently lack robust AI R&D ecosystems to build research capacity and strengthen ties.
Supporting Development of Global AI Systems, Standards, and Frameworks
International cooperative research is needed to inform the development of shared and best available metrics, test methodologies, quality and security standards, development practices, and standardized tools for the design, development, and effective use of trustworthy AI systems. Of particular value are methods for secure data-sharing and methods for applying AI to areas of importance such as public health and sustainability. Also valuable are systems and environments that provide nations’ domestic enterprises with access to the expertise and infrastructure garnered from increased international collaboration and investments. All of this is ultimately a prerequisite for achieving optimum scale and collaboration with international partners, and critical for bringing about an ecosystem around AI R&D designed from the beginning around principles such as those in the 2020 trustworthy AI executive order.125
Also in need of consideration are effective mechanisms for public-private partnerships and international arrangements, as discussed in Strategy 8. This work is especially complex and intersectional, but small- scale and similarly focused examples could help to guide agencies in pursuit of this research. One such example is the Declaration of the United States and the United Kingdom on Cooperation in AI R&D126 to advance a shared vision of AI and to work toward a mutually supportive AI R&D ecosystem. Another is the recent commitment of the Quad (the United States, India, Australia, and Japan) to establishing various technical standards contact groups,127 including a group for advanced communications and AI focusing on standards-development activities as well as foundational pre-standardization research.128 Other fruitful avenues include investigating and optimizing the potential of joint solicitations for AI R&D with international partners, and of joint international AI research and computing infrastructures.
Along the way, it is critical that international cooperative research also focuses on data management, governance, and sharing. One key area of consideration is how to share data, especially if it is sensitive
-35-

data, in a safe and secure way among countries that have different information security standards and capabilities. Another is research into allowing interoperability among nations’ systems while protecting data and data ownership so that data is treated in a safe and consistent way, leading to the development of trusted and durable mechanisms for cross-border data transfers for AI R&D collaboration. A third consideration could be how best to ensure a culture of transparency and disclosure that aligns with the principles of research integrity, both domestically and with allies and partners. Overall, it is key that U.S. agencies develop and establish appropriately rigorous standards, policies, and procedures for data sharing, data privacy, and the protection of intellectual property to safeguard data, privacy, and national security.
Facilitating International Exchange of Ideas and Expertise
Leading experts and innovators in emerging technologies are spread out over multiple countries and continents. Ensuring that ideas can flow among them and across locations is necessary for a shared global future of effective and trustworthy AI. Agency-to-agency collaborations and broader bilateral and multilateral cooperative arrangements provide an opportunity for the United States to address gaps by leveraging AI research expertise around the world. Such collaboration could be realized through existing programs, such as the Embassy Science Fellows Program,129 U.S. Science Envoy Program,130 Fulbright Program,131 International Visitor Leadership Program,132 and TechCamps,133 through AI-centric tracks.
Agencies should consider how undergraduate and graduate AI R&D internships, international fellowships, and exchange initiatives can help build the U.S. STEM workforce. These international collaborations can expose researchers to diverse ideas, attract and retain top AI R&D talent, and foster long-term partnerships among U.S. AI researchers. Current programs, such as the U.S. Intergovernmental Personnel Act (IPA),134 illustrate what potential partnerships would look like. IPA facilitates temporary exchanges among federal agencies and other organizations, including state, local, and tribal governments, colleges, and universities. Developing similar programs for short-term international exchanges could foster R&D activities and outcomes in the international context.
Additionally, grand challenges are effective and efficient mechanisms for governments to leverage partnerships, technologies, and other assets for the purposes of cooperative research, development, and acquisition that could be more widely used. Grand challenges have provided a leveling platform for multilateral approaches to international collaboration and have enabled highly directed and innovative means of finding solutions to complex societal and industrial challenges of interest to the United States as well as to global partners, such as those related to health and natural disasters as well as food security. Among the strengths of grand challenges is their ability to garner a highly varied set of participants across a diversity of sectors, including academic, industrial, and individual technology enthusiasts, and from all manner of origins and backgrounds. This strength is amplified and realized in an international context. The current U.S.-led series of Grand Challenges on Democracy-Affirming Technologies135 are an example, having already demonstrated success via a U.S.-United Kingdom collaboration on a prize challenge for accelerating the development and adoption of privacy-enhancing technologies.
Encouraging AI Development for Global Benefit
Certain uses of AI run counter to the values and well-being of the United States, especially when AI is utilized for the purposes of political oppression, coercion, criminal activities, violations of international law, and social manipulation. To combat this threat, additional research is needed into the ways in which nefarious usage of AI may be countered. This research presents further opportunities to engage with the international community and leverage bilateral and multilateral partnerships with allies and partners to restrict competitors and adversarial nations from gaining access to or acquiring advanced AI tools and
-36-

associated technologies critical to U.S. national security and other interests. Mutually beneficial alliances and partnerships around AI provide the United States with a durable means of addressing global AI challenges, deterring aggressive behavior, assuring allies and partners, and supporting stability.
Though not created by AI, other existential threats to peace and security could also be countered via AI innovations. For example, as described previously, there is opportunity for co-investment with values- aligned countries in novel AI techniques to solve long-term global challenges such as those related to health, natural disasters, pollution, food production, and sustainability. In addition, investigation of methods of public outreach and engagement with the broader stakeholder community is important to spread awareness regarding capabilities and limitations of AI.
As global interest in and use of AI continues to grow, so does the importance of international cooperation in research and coordination in the field. The United States is already positioned as a leader in AI research and innovation. This existing leadership may be leveraged to realize the aims of safe and secure use of trustworthy AI; standardized effective AI infrastructure, including robust and equitable data-sharing practices; international cooperation and coordination of AI research; and development of AI for global benefit.
-37-

",https://www.whitehouse.gov/wp-content/uploads/2023/05/National-Artificial-Intelligence-Research-and-Development-Strategic-Plan-2023-Update.pdf
Promoting More Accountable AI in the Boardroom Through Smart Governance,ScienceDirect,Jingchen Zhao,ARTICLE This article discusses how AI can impact corporate governance and highlights the importance of accountability in AI application within decision-making.,2024,"This paper focuses on the benefits that accountable artificial intelligence (AI) could bring to corporate boardrooms, and the role and format of regulation to promote more accountable AI. It will investigate the interconnections between AI, accountable decisions made by boards of directors in companies, the associated legal risks, and how to regulate AI to mitigate risks while promoting accountability. In the boardroom, AI is defined as the use of computers to assist, support, collaborate or even duplicate directors’ behaviours. The last few years have seen increased calls for boards to have oversight responsibility at the intersection between AI and the regulatory and policy implications in a corporate setting. AI can be applied to advance the effectiveness and efficiency of corporate governance and to raise awareness of accountable practice in its standardisation and implementation. However, AI can also be a disruptive technology, and the accountable practice of AI needs to be reinforced by regulatory insight to enable its sustainable development. No consensus has yet been reached on the most appropriate regulatory framework to achieve these goals; this article therefore aims to identify the most appropriate regulatory framework to promote sustainable AI by monitoring and mitigating the associated risks in a corporate setting. We investigate the rationale for and advantages of a smart regulatory approach for regulating AI, aiming to achieve the participation of multi-disciplinary teams with members playing particular roles associated with the safe and effective deployment of AI.

Introduction
Artificial intelligence (AI) has been defined in many ways by different scholars .The term was first introduced by the American computer scientist John McCarthy, an AI pioneer, during a workshop at Dartmouth College in 1956 that set out to explore how machines could intelligently think. He defined AI as “the science and engineering of making intelligent machines, especially intelligent computer programs”.1 Marvin Minsky, another leading figure in the history of AI, defines it as “the science of making machines do things that would require intelligence if done by men”.2 It requires high-level mental processes such as perceptual learning, memory, and critical thinking. Russell and Norvig describe an AI application as an “intelligent agent”, since such machines can act as intelligently as humans by imitating human intelligence; this is made possible by feeding the machines with lots of data, and then training and testing them using machine learning models.3
AI has the potential to transform the strategic operations of companies across many sectors of the global economy. As the speed of AI innovation and data-driven business transformation accelerates, boards and executives need to understand how these technologies could impact their businesses, as well as the potential risks involved. AI refers to artificially-created intelligence, software systems that simulate human thinking on computers or other devices,4 but in a corporate setting AI is defined as the use of computers to assist, support, collaborate or even duplicate directors’ behaviours, referring to the effects generated by the realisation of human minds through computers.
AI applications are transforming business processes and strategies, increasingly acting as authoritative devices to boost internal corporate governance processes through innovative analytical models and robust risk-management methods and matrices.5 Used appropriately, AI in the boardroom can allow companies to function competently, successfully, and with foresight in their business environments over the long term. Directors can leverage AI in a variety of ways to enhance their decision-making – for example by allowing data-driven decisions, whereby AI can analyse massive volumes of data and provide insights for directors so that they can make more informed decisions. AI-powered analytics tools can process financial data, market trends and customer feedback to help directors identify growth opportunities, optimise resource allocation and mitigate risks. For instance, data science is central to Netflix's business success because the company uses its trove of data and analytics about international viewing habits to create or purchase programming that will be embraced by large audiences.6
There is no shortage of principles and concepts aiming to support the fair, responsible and accountable application of AI when it comes to managing AI in the boardroom. It is clear that AI can be a double-edged sword. In a corporate setting it may bring many benefits, including reduced costs, efficiency gains, innovation, and improved decision-making supported by data. However, it can also be a disruptive technology impacting a wide range of human rights-related issues. AI is becoming ever more omnipresent in our society and is affecting more aspects of human life in greater ways, and recent years have seen increased calls for board oversight of AI in areas such as sustainability and accountability. Boards of directors will also increasingly be called upon to address an emerging area of oversight responsibility at the intersection of AI and ethics, another issue that is becoming more and more pervasive in our daily life. AI can also generate adverse effects such as infringing on privacy and causing algorithmic bias, meaning that companies should critically analyse their AI algorithm data to ensure that it is making predictions both fairly and ethically. It is vital to investigate the potential hazards of AI applications, so that the application of these technologies can be aligned with human values and beliefs.
With a focus on the outcomes of AI, questions of how to trace and account for responsibility in the application of AI have prompted us to consider not only how to ensure that AI systems are used in an accountable manner, but also to whom they are accountable. Given these current challenges in the field, we will focus on the implications of AI application in the boardroom to promote both board accountability and accountable AI, which means the same thing in self-driving corporations. In the corporate setting, the challenge for boards is to understand how to increase AI automation to promote board competence, board diversity and board accountability, transforming their companies into dynamic, data-centric organisations that are resilient to changes and challenges. There is a gap between perceived ethical values of transparency and trustworthiness and the actual implementation of AI in the boardroom; this article aims to fill this gap by embedding explainability, accountability and transparency in AI systems and regulatory approaches to AI in the corporate context.
In order to address these issues, we need to explore questions such as the accountability challenges AI that could bring, and how we can safeguard accountability in AI application in corporate decision-making. The overarching aim is to unpack the most appropriate regulatory framework to achieve accountable AI in the boardroom in order to promote board accountability. The accountable practice of AI needs to be reinforced by regulatory insight to enable its sustainable development;7 we will discuss the rationale for regulating AI in the boardroom by monitoring and mitigating the associated risks in a corporate setting.
Just as the philosophy of technology has produced two competing approaches for understanding the role of tools in human affairs, namely technological determinism and technological instrumentalism,8 the corporate use of AI also enables technologies to play either an instrumental or a determining role, both of which often result in improved efficiency, accuracy and effectiveness. We aim to achieve research objectives based on the different roles that may be played by AI in the boardroom. First we will investigate the functions of AI in an advisory or assistant role, so as to promote AI-supported and AI-enhanced board accountability. In these situations AI is neutral and instrumental; board members are still in the driving seat, regardless of how the AI is designed or applied, and the focus is still primarily on directors’ behaviours and contexts enhanced by features of AI-enhanced decision-making. Second, we will discuss scenarios where AI has an independent role and acts as an autonomous director. In this situation, AI accountability is equivalent to board accountability. As AI applications are the accountors in both scenarios, we will investigate the feasibility of holding them accountable for corporate decisions.
The article proceeds as follows. Section 2 critically discusses the nature and scope of accountable AI, the implications of accountable AI and the regulation of AI in the boardroom. Section 3 is concerned with the features of an effective regulatory framework for accountable AI. Section 4 looks at the rationale for introducing smart regulation in the boardroom when it comes to regulating AI, involving stakeholder participation and empowered whistleblowers. Finally, there will be some concluding remarks.",https://www.sciencedirect.com/science/article/abs/pii/S0267364924000062
Regulating Artificial Intelligence: Proposal for a Global Solution,Journal of Ethics and Artificial Intelligence,"Erdélyi, O. J., & Goldsmith, J.",ARTICLE An international AI regulatory agency can address legal and ethical challenges.,2018,"Given the ubiquity of artificial intelligence (AI) in modern soci- eties, it is clear that individuals, corporations, and countries will be grappling with the legal and ethical issues of its use. As global problems require global solutions, we propose the establishment of an international AI regulatory agency that — drawing on inter- disciplinary expertise — could create a unified framework for the regulation of AI technologies and inform the development of AI policies around the world. We urge that such an organization be developed with all deliberate haste, as issues such as cryptocur- rencies, personalized political ad hacking, autonomous vehicles and autonomous weaponized agents are already a reality, affecting international trade, politics, and war.
CCS CONCEPTS • Computing methodologies → Artificial intelligence; • So-
cial and professional topics → Governmental regulations; KEYWORDS
Transnational legal ordering; international organizations; hard/soft law; international governance
ACM Reference Format:
Olivia J. Erdélyi and Judy Goldsmith. 2018. Regulating Artificial Intelligence Proposal for a Global Solution. In 2018 AAAI/ACM Conference on AI, Ethics, and Society (AIES’18), February 2–3, 2018, New Orleans, LA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3278721.3278731
1 INTRODUCTION
AI is increasingly affecting our lives. Self-driving cars are being released on the roads; we rely on medical diagnosis tools to catch rare diagnoses; product recommendation systems use pattern recog- nition software to analyze our needs and optimize our shopping experience; we use automated surveillance techniques, killer bots, and other weaponized AI technologies to defend our countries; powerful data mining applications allow us to sift through a wealth of information within the shortest period of time; and AI-enabled
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
AIES ’18, February 2–3, 2018, New Orleans, LA, USA
© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-6012-8/18/02. . . $15.00 https://doi.org/10.1145/3278721.3278731
Regulating Artificial Intelligence Proposal for a Global Solution
Olivia J. Erdélyi School of Law University of Canterbury Christchurch, New Zealand olivia.erdelyi@canterbury.ac.nz
Judy Goldsmith Department of Computer Science University of Kentucky Lexington, KY, USA goldsmit@cs.uky.edu
decision-making systems using predictive analytics are employed in financial services to detect fraud, tax evasion, or money launder- ing, and in the justice system for predictive policing and sentencing. Undoubtedly, some of these technologies can make life a lot eas- ier, but they also present substantial problems. Sometimes these problems result from the imperfection of AI applications, as when AI systems produce discriminatory biases. At other times, issues arise because AI is doing its job far to perfectly, as evidenced by the increasing privacy threat posed by pattern recognition applica- tions [11]. Some instantiations of AI are ethically questionable (e.g., child-like sex bots [17]), potentially dangerous (e.g., autonomous kill-decisions by machines), or raise broader systemic challenges (e.g., inequality and labor displacement issues and impugnment of existing ethical, legal, and social paradigms). For a good overview of the contemporary AI landscape and policy environment around AI, see [4].
Do these applications really make human society more efficient, better, or safer? Or is AI rather a looming menace that will ulti- mately destroy mankind [10]? A lot depends on how we handle the historic opportunity to shape these potent, emerging technologies — a question acquiring increasing practical importance in view of the rapidly growing number of machine-human interactions thanks to recent breakthroughs in AI (in particular in machine learning).
The AI community has long been calling for policy action with respect to AI, and criticism is getting louder on the growing le- gal vacuum in virtually every domain affected by technological advancement [3, 20, 21]. Policymakers around the world are be- ginning to address AI policy challenges. Canada, China, Japan, the UK, the US, and the EU have launched ambitious strategies to pro- mote the development and commercialization of AI with a view to maintain sustained economic competitiveness after the inevitable global transition to an AI-driven economy [13, 19]. There are also many academic, and joint public and private sector venues that support governments in promoting AI R&D — examples include the International Association for Artificial Intelligence and Law, the Partnership on AI, the Artificial Intelligence Forum of New Zealand, and SPARC in the EU, not to mention a small number of tech firms’ (Amazon, Apple, Baidu, Google, Facebook, IBM, and Microsoft) uncontested hegemony over the AI scene. Courts, faced with first AI-related disputes, contribute to clarifying situations, although some decisions reflect a complete lack of technological expertise [5, 12], rather adding to than eliminating confusion and underscoring the need for interdisciplinary cooperation and im- proving policymakers’ expertise on AI [4].
95
Paper
AIES’18, February 2–3, 2018, New Orleans, LA, USA
In short, both the AI revolution and the challenges it presents to society are very real and it is policymakers’ turn to do some- thing about it. However, caution is advised with purely national approaches: Once the emergence of AI technologies is constructed as a problem and the necessity of regulation identified, a number of peculiarities intrinsic to lawmaking should be considered so the newly created legal norms become truly authoritative, that is, accepted as legitimate and institutionalized, as opposed to merely formal or symbolic rules without any impact on normative orienta- tions and behavior [8].
First, whenever the regulation of an issue has externalities that transcend national boundaries — as is the case with AI — differing domestic approaches tend to conflict, raising significant difficul- ties for those affected by more than one regime. Such problems are then typically perceived as transnational in scope, with the consequence that actors increasingly deem national rules inapt to provide suitable solutions. This discrepancy between the transna- tional nature of a problem and the national character of the law governing it creates pressures for transnational regulation. Second, transnational legal ordering is characterized by a set of complex, recursive, multi-directional processes, which follow their own logic and crucially affect norms’ authority [7]. Third, even though the legitimacy of legal norms has predominantly social rather than moral roots, ethical considerations should and will play a pivotal role in shaping the nascent body of law — whether domestic or transnational — concerned with AI.
We therefore hold the view that national efforts to develop AI policies, should, from the very beginning, be coordinated and sup- ported by an international regulatory framework to avoid the risks stemming from the imperfect interaction of fragmented domestic regulatory approaches. Against this background, we propose the establishment of a new intergovernmental organization — which could be named International Artificial Intelligence Organization (IAIO) — to serve as an international forum for discussion and en- gage in standard setting activities. The IAIO should unite a diverse group of stakeholders from public sector, industry, and academic organizations, whose interdisciplinary expertise can support pol- icymakers in the overwhelming and crucially important task of regulating this novel, immensely complex, and largely uncharted area. We hope that such a wide-scale, in-depth cooperation among all interested stakeholders at this early stage will put national and international policymakers in the position to take proactive action instead of lagging behind technological innovation with potentially devastating implications. Recent turbulences in worldwide finan- cial markets can serve as warning examples of the ramifications of regulators not being ready for what the future might bring. The stakes with AI are even higher. We must get it right the first time.
The paper will proceed as follows: first, we present a brief anal- ysis of transnational normmaking processes, followed by our pro- posal on an international AI regulatory framework and a short conclusion.
2 DYNAMICS OF TRANSNATIONAL LAWMAKING
In response to economic and cultural globalization, legal, political science, and sociology scholarship have taken manifold attempts to
capture processes of various forms of transnational social ordering. Examples include the traditional, purely state-centric legal notion of international law with a dichotomous view towards national and international law; global law, which refers to legal norms of univer- sal scope while also acknowledging the role of non-state actors in normmaking; transnational law, which can have several connota- tions in reference to norms with a more than national but less than global purview; the concept regime theory developed by interna- tional relations scholars, which is likewise state-centric and has a sole focus on international political processes without any regard to the impact of domestic politics or law’s normativity; the socio- logical world polity theory, studying the diffusion of legal norms assuming that global conceptual models frame national societies in one-dimensional top-down processes; and transnational or global legal pluralism, which emphasizes the coexistence of different legal orders and normative contestations among them. Giving a compre- hensive overview (including references) of the respective merits and limitations of existing theories, Shaffer [15] and Halliday and Shaffer [7] introduce a further, socio-legal notion termed transna- tional legal order (TLO), which builds on these approaches and is defined as a social order of transnational scope consisting of ”a col- lection of formalized legal norms and associated organizations and actors that authoritatively order the understanding and practice of law across national jurisdictions.” We explain the determinants in- fluencing transnational legal processes through which legal norms are constructed, conveyed, and institutionalized based on the con- cept of TLO, owing to its ability to highlight both the legal and institutional aspects justifying the proposed regulatory framework.
Disaggregating the above definition into two parts — (1) for- malized legal norms produced solely or partially by some type of transnational legal organization or network, which are (2) aimed at inducing changes within nation-states — we will first give account of the bewildering variety of governance arrangements characteriz- ing modern international relations, and then illuminate the complex ways in which legal norms interact and institutionalize. The terms international and transnational shall be used interchangeably, re- ferring to norms and institutions spanning national boundaries (whether global or geographically more restricted in scope).
Turning to the first element of our TLO definition, both the norms and the institutions issuing them come in a diverse array of forms. Norms are contained in various formal texts of softer or harder legal character such as treaties, conventions, codes, model laws, stan- dards, administrative rules and guidelines, and judicial judgments. International institutionalization displays a similar diversity featur- ing public intergovernmental (also called international) organiza- tions (IGOs or IOs) and private non-governmental organizations (NGOs) of varying levels of formality [9, 14, 18]. The widespread use of both hard and soft legalization in international governance begs the questions of what hard and soft law are and what drives actors’ choices between disparate legal and institutional settings.
Note that the existing literature is divided on what constitutes hard and soft law. Some authors concentrate on legal rules’ bind- ing quality either in binary terms or along a continuum between fully binding legal instruments and purely political, non-binding arrangements, while others focus on their ability to impact behav- ior. For a good overview see [1, 6, 16]. Because it includes both the legal norms and the institutional arrangements responsible for their
96
Paper
AIES’18, February 2–3, 2018, New Orleans, LA, USA
development within the scope of its analysis, the most interesting definition for our purposes is the one adopted by Abbott and Snidal. They distinguish hard from soft law along three dimensions, namely (1) the extent of rules’ precision, (2) the degree of legal obligation they establish, and (3) whether or not they delegate authority to a third-party decision-maker for interpreting and implementing the law. Hard law refers to legally binding obligations that are either precise or can be made such by adjudication or further clarifying regulation, and that empower a third party to oversee their inter- pretation and enforcement. Soft law, on the other hand, embodies legal instruments that exhibit some degree of softness along any of these three dimensions.
Guzman and Meyer [6] and Abbott and Snidal [1] provide a very instructive comparison of the relative advantages and disad- vantages of hard and soft legalization and the various factors that determine actors’ preferences towards different types of interna- tional governance arrangements.
Hard legalization is typically characterized by a coherent, estab- lished, and formalized institutional and procedural framework to ensure smooth implementation, elaboration, and enforcement of commitments. These arrangements are generally perceived as legit- imate, resulting in a concomitant enhanced compliance-pull, and backed up by international law that provides international actors readily available mechanisms (e.g., for recognition or enforcement) to order their relations. The combination of these factors enhances the credibility of commitments by constraining opportunistic behav- ior and increasing the costs of reneging; reduces post-contracting transaction costs by restricting/constraining attempts to alter the status quo by way of frequent renegotiation, persuasion, or coer- cive behavior; allows parties to pursue political strategies through legal rather than political channels at low political cost; and solves problems of incomplete contracting by vesting an administrative or judicial institution with power for interpreting and clarifying rules intentionally left imprecise in anticipation of unforeseeable future contingencies. Yet, hard legalization comes at certain costs: it restricts actors’ behavioral freedom, entails potentially severe sovereignty implications, and is less effective in accommodating diversity or adapting to changing circumstances by reason of its relative rigidity.
Thus, in many instances, softer forms of legalization, which offer some of hard law’s perks yet alleviate its intrinsic disadvantages through their flexible, more or less informal cooperation mecha- nisms, may better serve parties’ purposes. By relaxing the level of formality along one or more of the dimensions precision, obligation, and delegation, soft legalization minimizes initial contracting costs and facilitates speedy conclusion of agreements. Bargaining prob- lems become less pronounced, negotiation and drafting requires less scrutiny, and there is no need for potentially challenging approval and ratification processes. Thanks to soft legal commitments’ mal- leable cooperation frameworks, parties retain more control over the overall design and organization of their cooperation, incur lower sovereignty costs, and have an easily adjustable system at their dis- posal to deal with change and uncertainty. Soft law also has a way of evening out power asymmetries by securing and perpetuating powerful actors’ interests at lower sovereignty costs, while at the same time shielding the week from their pressure. Furthermore, soft law is the only directly available instrument to non-state actors
for ordering their interactions. Due to their conciliatory properties, softer forms of legalization leave actors time to acquire sufficient information and expertise to gradually test and develop solutions to problems, encouraging collective learning processes and ever deeper cooperation between them — benefits that plentifully com- pensate soft law’s central weakness: diminished compliance pull.
In conclusion, the choice between harder and softer types of legal- ization involves a context-dependent tradeoff, which actors should carefully consider on a case-by-case basis. Vabulas and Snidal [18] describe the pros and cons of institutional formality and the trade- offs actors face when moving along a broad spectrum of intergov- ernmental organizational formality — especially between formal and informal intergovernmental institutions (FIGOs and IIGOs) — in an analogous fashion.
These three analyses show that, in general, actors opt for hard law/higher institutional formality when they (1) wish to enter into a binding commitment in issue areas subject to a high degree of consensus, because violations are hard to detect, or parties wish to signalize their intention to engage in sincere cooperation; (2) are willing to accept sovereignty costs stemming from delegating decision-making authority to a central body in order to establish stronger collective oversight over issue areas where the probability of violations is high and monitoring and enforcement is important; (3) put more value on collective control of information, for instance, to unveil violations and increase peer pressure to induce univer- sal compliance; (4) aim for lower long-term transaction costs to effectively tackle recurring or clear-cut issues in standard operating procedures; (5) intend to set up a sophisticated centralized adminis- tration to provide legitimacy and stability for supporting complex work processes such as the design and elaboration of norms, coordi- nation involving multiple parties, or judiciary and/or enforcement procedures; (6) are faced with the task of managing routine prob- lems, which is more easily done with established administrative and implementing systems.
Conversely, soft law/lower institutional formality is preferable when actors (1) want to maintain flexibility to deal with uncertainty, distribution problems, diversity, and changing circumstances; (2) prefer to preserve state autonomy and avoid sovereignty intrusions because welfare gains of cooperation outweigh the potential for defection and opportunism so that agreements are self-enforcing once any focal point for discussions has been established, or when external effects elicited by domestic actions are negligible; (3) insist on avoiding formal transparency mechanisms to maintain closer control of information typically among a more homogeneous group; (4) need lower initial contracting costs to speed up negotiations to be able to act fast (e.g., in crisis situations) or because hard law is not available for lack of consensus; (5) find that minimalistic administrative functions are sufficient to support their purposes; (6) must manage high uncertainty (e.g., in initial stages of cooperation or in new/complex issue area) and want to allow themselves time for coordination and establishing common ground without making strong commitments.
Sometimes soft law eventually paves the way towards harder forms of legalization and cooperation becomes increasingly formal- ized, but in many cases soft legalization and institutional informality have their own justification. In practice, both highly institution- alized FIGOs, such as the United Nations (UN) or World Trade
97
Paper
AIES’18, February 2–3, 2018, New Orleans, LA, USA
Hard Law/High Institutional Formality binding commitment delegation/high sovereignty costs collective control of information
low long-term transaction costs complex centralized administration routine management
Soft Law/Low Institutional Formality flexible cooperation arrangements
state autonomy/low sovereignty costs close control of information low initial contracting costs minimalistic administrative functions crisis/uncertainty management
governance models by altering the allocation of functions between the state, the market, and other forms of social ordering. At times, this prompts more state intervention giving birth to new public and public-private hybrid agencies, while at other times it propels deregulatory tendencies resulting in a retreat of state administra- tion and simultaneous engagement in self-regulation by the private sector. Moreover, transnational legal processes are often respon- sible for revamping states’ institutional architecture. They shift power between different branches of government and upset the division of responsibilities among existing state institutions, some- times giving rise to new additions to the institutional landscape. It is not hard to see that domestic systems may starkly differ, and such fragmentation often entails devastating consequences in is- sue areas with cross-border effects. These legal, governance, and institutional changes directly affect individuals by reconfiguring markets for professional expertise, which, in turn, feeds back into the adaptation of governance models by, e.g., a move towards more technocratic forms of governance. This highlights an important, yet admittedly somewhat elusive point, namely that not only in- stitutions but also individuals — acting as conduits facilitating the diffusion of transnational norms — play a crucial role in domestic and transnational lawmaking. The fifth domain of state change con- cerns the modifications in patterns of association and mechanisms of accountability across various national and international sites of governance. These shifts ultimately shape individuals’ legal culture and consciousness, as well as their expectations towards the state, triggering new processes of state change where these views conflict with the prevailing state of affairs.
The extent, location, and timing of state change hinges on three clusters of factors pertaining to the TLO’s nature, its relation to the receiving state, and the receiving state’s peculiarities. First, TLOs are generally better received if perceived legitimate, i.e., norms are adopted by respected actors with preferably similar interests, in a fair (especially non-coercive) process, and they effectively tackle designated target problems. Myriads of international and national, state and non-state actors interact in complementary or conflicting ways in shaping every aspect of transnational lawmaking. They seek to legitimize rules that serve their purposes and delegitimize those running against them. Powerful players typically dictate the outcome of such struggles. TLOs are more likely to have real behav- ioral impact if they consist of accepted, clear, and well-understood norms. As discussed above, binding hard law does not necessarily score better in this respect. In a large part, TLOs’ coherence is a function of the quality and quantity of their horizontal interac- tion, and can be threatened where significantly overlapping TLOs interact in an antagonistic rather than complementary fashion [16].
As far as TLOs’ relation to the receiving state is concerned, pow- erful actors sometimes resort to coercive measures to impose their will on weaker countries. However, because coercion irrevocably destroys norms’ legitimacy, changes forced on states in this manner are at best symbolic and short-lived before they are successfully blocked at the stage of domestic implementation. Another essen- tial prerequisite for the sustainability of transnationally triggered change is the support of intermediaries, who link transnational and national lawmaking processes and are deeply familiar with the interests of both sides. Whether government representatives,
Table 1: Tradeoffs in legalization/institutional formality.
Organization (WTO), IIGOs allowing for laxer cooperation, like the Basel Committee on Banking Supervision (BCBS), private NGOs, for instance the International Chamber of Commerce (ICC), and hybrid forms can be fairly successful and instrumental actors in international lawmaking.
Table 1 gives an overview of the above outlined six tradeoffs actors have to weigh when choosing their desired level of legaliza- tion/institutional formalization.
Moving on to the second part of our TLO definition, transna- tional legal norms directly or indirectly pursue the ultimate goal to induce shifts in countries’ policies and individuals’ normative preferences through various formal or informal channels. This gen- erates convoluted, recursive cycles of international lawmaking pro- cesses across diverse transnational and national fora, until norms eventually settle and institutionalize [15]. Halliday and Shaffer [7] note that transnational norm-making may encounter difficulties in the following situations: First, actors may find themselves caught up in diagnostic struggles over the framing of problems, which favors particular alliances and antagonisms, supporting diagnoses reflecting the respective interests of these groupings. Second, do- mestic implementation of transnationally agreed rules is frequently thwarted and a new cycle of lawmaking is triggered by parties who are influential at the national level, but not represented or unsuc- cessful in international negotiations and therefore refuse to accept such norms as legitimate — a situation referred to as actor mismatch. Third, in their endeavor to reach widely accepted compromises, parties often resort to vague language or leave delicate issues unre- solved in their agreements. The resulting ambiguity and built-in contradictions of transnational norms open avenues for nationally fragmented, likely conflicting implementation, again calling for further transnational lawmaking to eliminate related problems.
Inspired by Shaffer [15], we now describe the recursive pro- cesses of international lawmaking, which encompass mutual inter- actions both vertically between transnational and domestic venues, as well as horizontally among various TLOs. Vertically, transna- tional norms impact states in a process referred to as state change. Their impact can encompass the whole or parts of the state (lo- cation of change), it may occur in a slow, progressive process or abruptly owing to unexpected circumstances (timing of change), and across five interrelated dimensions. The most obvious aspect of state change is the dynamic evolution of domestic legal systems elicited by the formal national enactment of transnational law. For- mal enactment may or may not have a substantial effect on rules’ practical implementation depending on the extent to which the transnationally induced change is viewed as legitimate. In more subtle ways, however, these primary legal changes set much broader systemic transformations in motion with potentially heavy social repercussions. For one thing, they continuously reshape established
98
Paper
AIES’18, February 2–3, 2018, New Orleans, LA, USA
industry specialists, academics, social movement leaders, or profes- sionals employed with various public or private organizations on the national and international platform, these intermediaries are instrumental in coordinating communication, easing tensions, and conveying norms between the national and transnational levels.
Finally, the single most important condition for transantional legal norms’ national acceptance is their conformity with the target country’s existing cultural and institutional settings and pursued reform initiatives. It strongly depends on the receiving country’s prevailing power configurations, institutional capacities, path de- pendencies, and cultural disposition, and tends to decrease as the distance between the transnational and national contexts and in- terests and/or the extent of state change increases.
This concludes our analysis of transnational legal ordering, high- lighting the main factors instrumental in determining transnational legal norms’ efficiency in influencing the behavior of states and their various constituencies. We now turn to our proposal on a consistent international AI regulatory framework.
3 PROPOSAL FOR A NEW INTERNATIONAL ARTIFICIAL INTELLIGENCE ORGANIZATION
International institutions are the prevalent vehicles of international cooperation in our interconnected world. When a critical mass of states and/or non-state actors feel that transnational coopera- tion is necessary to solve a problem that is impossible to tackle by isolated national measures, they establish a new IGO or NGO for that particular purpose. Based on legal and international rela- tions definitions in circulation — see [9, 14, 18] — we define an IGO as a formal entity (1) established by an international agreement governed by international law; (2) with at least three (sometimes two) members — typically states but increasingly also IGOs; and (3) having at least one organ with a will distinct from that of its members. FIGOs’ organizational purpose is laid down in a binding international agreement such as a treaty or a formal legal act of another IGO, their membership is clearly defined in the founding legal act, and they have a permanent and significant institutional- ization in place. By contrast, IIGOs operate based on an explicitly shared, but informal expectation about purpose, their membership is not always clear, as members are explicitly associated but only by non-legal mutual acknowledgment, and they do not possess any significant institutionalization. NGOs differ from IGOs in that they are not created by treaty — meaning they are governed by national rather than international law — and their membership is made up of non-state actors.
Given the severity and global nature of AI’s anticipated impact on humanity, we expect it to join the long line of issue areas requiring interstate cooperation, raising the question of establishing an IGO at some point in the future. Against this background, we propose the creation of the IAIO as a new IGO, which could initially serve as a focal point of policy debates on AI-related matters and — given sufficient international support — acquire increasing role in their regulation over time. We start by determining the degree of desired institutional formalization by examining, in turn, the six above elaborated tradeoffs in relation to AI.
Binding commitment vs. flexible cooperation arrangements: As pointed out earlier, AI will fundamentally transform human so- ciety worldwide. Since this process of transformation is likely to be inescapable for any single state, states will probably wish to cooperate sincerely. Also, violations will be difficult to detect as keeping pace with technological innovation will require consid- erable technical expertise and capacities, presumably exceeding especially weaker countries’ capabilities and evoking severe power asymmetries. While apart from this latter circumstance, these fac- tors speak for hard legal commitments, it must be kept in mind that AI research and AI-human interactions are relatively young phe- nomena and their novelty severely restricts our ability to anticipate the spectrum and extent of the impending changes, let alone the dimension of the problems they will raise. Many AI instantiations encroach on our most basic rights, pose an existential threat, or bring up profound ethical and social questions, not to mention that they will utterly and completely upset our legal system. So, we are looking at heated debates among radically diverse parties over a va- riety of uncertain issues, which may change in rapid and currently unimaginable ways — conditions that, based on past experience, do not exactly favor international consensus. Therefore, we need all the flexibility we can get to acquire familiarity with the issues at hand, sort out differences, and establish common ground, before we can contemplate drawing up a more binding framework for cooperation.
Delegation/high sovereignty costs vs. state autonomy/low sovereign- ty costs: Weaponized AI technologies and certain data mining prac- tices are clearly relevant for national security. As this is a sensitive issue area involving particularly high sovereignty costs, at least initially, states will show reluctance to give up and delegate decision- making authority to the IAIO. On the long run, however, powerful collective oversight and enforcement mechanisms will probably be indispensable in order to curb incentives for violations and op- portunistic behavior, which should otherwise be high in light of the major shifts in international power constellations triggered by changes in countries’ competitive positions. Also, domestic AI poli- cies will produce significant externalities, affecting other countries. Based on this analysis, it is hard to escape the conclusion that a highly institutionalized organization with binding legislative, dis- pute resolution, and enforcement authority would be better suited as new international AI regulator. Nevertheless, the political reality remains that until sufficient clarity is reached on the IAIO’s precise purpose, membership, the issues to regulate, and the broad direc- tions to follow, international consensus supporting such a high degree of institutionalization is off the table.
Collective control of information vs. close control of information: History shows that states are generally cautious about sharing in- formation on fate-changing technologies, which speaks for close control of information with respect to AI. However, if and when we manage to gather consensus for hard legal commitments (e.g., treaty on certain AI applications), we will probably need to be more forthcoming with certain information to ensure compliance with those instruments. This is again a strong argument in favor of start- ing cooperation on AI regulation in a softer institutional framework and using soft law instruments, although a move towards harder legalization seems to be desirable over time.
99
Paper
AIES’18, February 2–3, 2018, New Orleans, LA, USA
Low long-term transaction costs vs. low initial contracting costs: In- ternational discussions on AI are just beginning and powerful states will likely have divergent preferences with respect to the regulation of this high-impact field. Compounded with the difficulties dis- cussed in the context of previous tradeoffs, this makes the prospect of reaching a workable international consensus on the short term rather remote. Yet crucially, swift regulatory response is imperative to prevent proliferating unregulated AI applications from causing social harm and to ensure that the opportunity presented by the rise of AI is harvested to humanity’s benefit rather than detriment — an aim best facilitated by lowering initial contracting costs with soft legalization and low institutional formalization. This is not to say that the idea of setting up a more robust regulatory framework with standard operating procedures should be abandoned. On the contrary, such a step has merit but only at a later stage, in posses- sion of sufficient expertise and political consensus to better assess the implications of various policy options and formulate informed policy recommendations.
Complex centralized administration vs. minimalistic administra- tive functions: Similar considerations apply as far as the level of administrative sophistication of the IAIO is concerned. In the initial stage of determining the purpose of the organization, its member- ship, the issues that need to be regulated, and the backbone of its regulatory agenda, less is probably more. Later, with perhaps binding legal instruments governing selected aspects of AI for a wide membership, work will get more complex, requiring stronger oversight, dispute resolution, and enforcement mechanisms as well as more powerful bureaucratic functions to service them.
Routine management vs. crisis/uncertainty management: In view of AI’s novelty, extreme complexity, unforeseeable evolution, and the controversies it is expected to elicit among a very heterogeneous circle of members, we are up against managing an extraordinarily uncertain issue area. Consequently, we need time and soft legal- ization’s flexibility to establish commonly shared ideas, interests, cooperation mechanisms, and solutions, which can then form the basis of more formalized cooperation arrangements in the future.
In summary, at least initially, the IAIO should start out as an IIGO displaying a relatively low level of institutional formality and using soft law instruments, such as non-binding recommenda- tions, guidelines, and standards, to support national policymakers in the conception and design of AI-related regulatory policies. Its interim goal should be to galvanize international cooperation in this domain as early as possible, before states develop their own, diverging policies, which may be hard to rescind without political damage. Whether the international community wishes to move towards more formalized cooperation at some point in the future, remains to be seen. Sometimes informality turns out to be the key to an organization’s success. This seems to be the case with the Bank of International Settlements especially during its initial years of operation and World War II, or the BCBS and the different G- Groups at present [2]. Another common trajectory is when initially informal arrangements turn into formal frameworks of cooperation. A case in point is the General Agreement on Tariffs and Trade’s (GATT) gradual transformation into the WTO [1]. Finally, there are examples for remarkably successful, sustained, complementary, and mutually beneficial cooperation between several organizations of varying institutional formality in the same issue area. This sort
of relationship is characteristic for the IMF and various G-Groups in financial regulation, or the Australia Group (AG), an IIGO, and the Organization for the Prohibition of Chemical Weapons (OPCW), a FIGO, in the regulation of chemical and biological weapons [18].
Our excursus in the domain of international lawmaking shows that beyond the optimal level of legalization and institutional for- mality, the IAIO must fulfill a number of more subtle requirements to be perceived as a fair and legitimate regulator. While leaving the elaboration of details to future research, we would like to stress the importance of two points: (1) including an interdisciplinary mix of experts (with, e.g., AI, legal, political, and ethics background) in the initial deliberations related to the IAIO’s establishment, modus operandi, and regulatory agenda, and (2) conducting regular, large- scale consultation processes with a diverse spectrum of interested stakeholders from public sector, industry, and academia, to ensure due consideration of all relevant perspectives.
4 CONCLUSION
Given the intensifying worldwide activism in AI regulation and AI’s anticipated substantial and global impact on human society, we pro- pose a consistent international regulatory framework — with a new IGO, the IAIO, as its focal point — to streamline and coordinate na- tional policymaking efforts. Learning from past experience in other regulatory fields, our objective is to offer a viable framework for international regulatory cooperation in the issue area of AI to avoid the development of nationally fragmented AI policies, which may lead to international tensions. Should our proposal find sufficient support in the international community, more concrete steps to- wards https://www.overleaf.com/project/5ba054d847fd0b1cb4ad1159setting up the here advocated regulatory framework, and regulatory poli-
cies on specific AI issues can be elaborated.
",https://dl.acm.org/doi/pdf/10.1145/3278721.3278731
Steering the governance of artificial intelligence: national strategies in perspective,Oxford Academic,Roxana Radu,"STUDY This study proposes the first comprehensive analysis of national approaches to AI from a hybrid governance perspective, reflecting on the dominant regulatory discourses and the (re)definition of the public-private ordering in the making.",2021,"ABSTRACT
As more and more governments release national strategies on artificial intelligence (AI), their priorities and modes of governance become more clear. This study proposes the first comprehensive analysis of national approaches to AI from a hybrid governance perspective, reflecting on the dominant regulatory discourses and the (re)definition of the public-private ordering in the making. It analyses national strategies released between 2017 and 2019, uncovering the plural institutional logics at play and the public-private interaction in the design of AI governance, from the drafting stage to the creation of new oversight institutions. Using qualitative content analysis, the strategies of a dozen countries (as diverse as Canada and China) are explored to determine how a hybrid configuration is set in place. The findings show a predominance of ethics-oriented rather than rule-based systems and a strong preference for functional indetermination as deliberate properties of hybrid AI governance.
AI, national AI strategies, hybrid governance, regulation
Introduction
Artificial intelligence (AI) is the new terrain of contestation in international relations, wrapped in uncertainty about loss of technological control and human oversight. ‘Whoever becomes the leader in this sphere will become the ruler of the world’, Russian President Vladimir Putin famously stated in 2017 (RT, 2017). Since then, a plethora of public and private actors have issued statements on how AI would change society for the better or for the worse, highlighting infrastructural developments, military applications and impact on jobs and human relations. Some of these statements revealed concrete plans to address AI-related challenges, but the majority remained principled positions on limiting risks associated with disruptive technologies (Ulnicane et al., 2020, Jobin et al., 2019). As recognition grows that tools based on algorithmic processing and machine learning bring about as many promises as commotions, governments are under increased pressure to react for the wellbeing of their citizens and for their raison d’être (Taeihagh, 2021). By 2020, more than 30 nations across the globe had started discussions about designing national AI strategies. Seventeen of these were already implementing them.
For a long time, the public discourse on AI was closely linked to concepts such as super-intelligence (Bostrom, 2014), technological Singularity (Shanahan, 2015) and the Fourth Industrial Revolution (Schwab, 2017). As the move away from technological perspectives towards societal transformation begins to consolidate, governance approaches start to come under scrutiny (Jobin et al., 2019, Kind, 2020). To date, in both academic and policy writings, the ethics focus has overshadowed the interest in adopting regulation. This article addresses this gap by exploring the following research question: How do states choose to design AI governance arrangements? Through the lens of hybridity, it is argued here that priorities outlined in governmental strategies form the basis for regulatory configurations and functional assignment of roles and responsibilities in policy-making. The drafting process and the national priorities identified thus reflect the extent to which multiple institutional logics inform a hybrid governance approach to AI.
This analysis is based on a qualitative comparison of a dozen national strategies focused on dominant regulatory approaches and the redefinition of public and private roles. The strategies of Canada, China, France, Finland, Germany, Japan, Singapore, South Korea, Sweden, the United Arab Emirates (UAE), the United Kingdom (UK), the United States (US) are examined, providing an overview of the separate discourses of continuity and change in the governance of AI. These national documents embed and reveal mainstream approaches to enabling new markets, regulating emerging technologies and working with non-state actors.
The conceptual framework and the qualitative content analysis presented here make a threefold contribution to governance and public policy literature and debates. First, they bring back the state as a central actor in a field dominated by private governance arrangements, revealing the extent of hybrid interactions in-the-making. Second, they capture the variety of approaches adopted by governments to respond to AI challenges, based on the discourse embedded in sovereigntist AI projects. Thirdly, they uncover the key elements of hybrid governance, pointing out marketization trends and functional indetermination.
The argument unfolds as follows. The next section situates the national strategies against the broader AI debates at the global level, showing that there is a mismatch between the aspiration to place the state in a leading role and the reality of corporations driving developments in the field. This is followed by an in-depth exploration of the hybrid governance literature, outlining merits and limitations of its main tenets. Two dimensions are derived from the hybridity thesis as a way forward in the analysis of AI governance: (1) regulatory approach and (2) redefinition of roles. The third part discusses the findings across the 12 strategies analysed, highlighting broad trends and instances of variation. The conclusion offers a summary of the argument and points to future research directions.
National AI strategies in a private governance landscape
AI was established as a field of research at the 1956 Dartmouth College workshop. It was around that time that undirected research into machine intelligence and robotics received funding from the American, British and Japanese governments. When public funding was withdrawn in the early 1970s – in what is known as the first ‘AI winter’ – due to the lack of concrete results and applications, companies stepped in to develop ‘expert systems’, replicating if-then reasoning models in highly specialized areas. The private sector has since taken a leading role in AI research and applications across a wide range of domains, from industrial robots to data mining. In many cases, such advances were first applied in technology industry and later on integrated into general applications, gaining societal acceptance based on earlier successes. As attempts to build a general intelligence supercomputer moved to the background, a successful generation of AI advanced deep learning, speech and image recognition, as well as data analytics tools. They are now deployed on a daily basis in banking, e-learning, medical diagnosis, smart vehicles, etc. forcing governments to formulate a response to the challenges they bring about.
The retreat of the state, first from funding AI research and later on from defining market limitations, has not meant a complete withdrawal of public support; universities and government-sponsored programs continues to conduct relevant research on the topic. But AI garnered extensive support as its application became widespread, culminating in geopolitical tensions and a new ‘race to the bottom among powerful nations’ (Scharre, 2019). The talk of a ‘global AI race’ is continuously fuelled by a ‘great powers’ discourse, with the United States, Russia and China competing for supremacy in the field. The governance of AI is often politicized, revealing concerns that AI could be designed to serve the ideologies and interests of a few centres of power (Jobin et al., 2019, Taeihagh, 2021). This article refocuses the attention on the broader state-society-market interactions in the context of AI, showing the extent to which they form the basis for the transformation of public and private authority.
Around the globe, the growing state interest in AI developments coalesced around the need to control the negative effects and the unintended consequences of new technologies, in particular their impact on furthering inequality. In a largely privatized domain of governance, intergovernmental efforts directed at AI governance have addressed limited aspects of public intervention. In May 2019, OECD member states adopted a set of Principles on AI highlighting democratic values and respect for human rights. Other intergovernmental initiatives stressed the need to work with various stakeholders in order to lead developments in the field. In the Group of Seven, agreement was reached on fostering cooperation with international organisations to promote a human-centred society and to reduce AI-related risks. In March 2018, the Government of Quebec proposed the creation of an Organisation mondiale de l’intelligence artificielle as an intergovernmental forum for building consensus on the standards and practices governing the application(s) of AI. While many state-led initiatives have explored the ethical and human rights dimensions of AI (e.g. Council of Europe’s Expert Committee on AI and Human Rights, OECD Principles on AI), providing guidelines for future research, they have come short of reforming public governance frameworks and addressing the needs of developing countries.
What has become clear in recent years is that public authorities have supported the dominant focus on AI ethics by putting forward their own recommendations for administrative bodies. The specialized literature also covers proposals around algorithmic impact assessment, laws on AI and robots, as well as fairness, accountability and transparency frameworks (Cath, 2018). An initial evaluation of 160 sets of AI ethical principles and guideline documents conducted by AlgorithmWatch (2020) showed that the majority of binding agreements and voluntary commitments that exist are proposed by the private sector. A critical take on ethical guidelines designed by companies reveals the oscillation between ‘ethics washing’, or attempts to disguise ethical stances to avoid regulation, and ‘ethics bashing’, resulting in loss of hope in the power of normative discussions:
The word 'ethics' is under siege in technology policy. Weaponized in support of deregulation, self-regulation or hands-off governance, ‘ethics’ is increasingly identified with technology companies’ self-regulatory efforts and with shallow appearances of ethical behavior (Bietti, 2019)
While self-regulation has been the main response to AI-enabled transformation at the global level, regulatory discussions and strategic approaches at the regional and national levels have gained prominence since 2016. Stronger critical stances have started to emerge, in particular around the narrow understanding of the responsibilities of AI developers, the monopolization of research by a few companies and the lack of diversity of perspectives in the field (Kind, 2020; AI Now 2018; Cath, 2018). Some scholars have also deplored the limited contextualization of the plethora of AI principles issued by various actors (Fjeld, Achten, Hilligoss, Nagy, & Srikumar, 2020), while some NGOs pointed out they lacked much-needed enforcement mechanisms (AlgorithmWatch, 2020).
Such concerns resonate with the approach of the European Union (EU) on the matter, materialized in the Ethics Guidelines for Trustworthy AI (April 2019) and the Policy and Investment Recommendations for Trustworthy AI (June 2019). Both documents were issued by the High-Level Expert Group on Artificial Intelligence, a multi-stakeholder group of 52 experts from various sectors. Prominent EU work in this area includes the 2017 European Parliament Resolution on Civil Law Rues on Robotics (non-binding), the 2018 European Commission Communication on Artificial Intelligence for Europe and the 2018 establishment of an advisory European Group on Ethics in Science and New Technologies.
Complementing these efforts are specific approaches to regulating AI-driven innovation – such as autonomous vehicles (Taeihagh & Lim, 2019, Leiman, 2020) – and shaping developments in the field in light of privacy and data protection provisions. In Europe, the General Data Protection Regulation, which imposes significant sanctions for violations, is relevant to AI discussions on two levels: 1) for the collection and storage of data of personal nature and the protection of the rights of data subjects (to access, to object, to rectify, etc.); 2) for enabling the data subject to obtain from the controller information about the logic of the algorithm (Art 15 (1)). The latter is yet to be tested in practice, but creates the basis for a so-called ‘right to explanation’.
Through patchwork legislation, the ‘return of the state’ in the AI field becomes more visible, but only in a fragmented way. All-purpose technology of this magnitude requires broader governance frameworks that restructure basic relations between the public and the private sector, as has been the case with the Internet (Radu, 2019; Radu, Chenou, & Weber, 2014). In this respect, national strategies reveal more clearly the innovations and limits of the public approach taken to govern artificial intelligence.
The countries hosting technology industry giants have taken the lead, with the ambition to dominate AI development at the global level in the next decade. Many other countries, in particular from the developing world, are still debating their national priorities and future AI frameworks. The first country to release a comprehensive national AI strategy in March 2017 was Canada, but the first sector-specific strategy, the one from South Korea, preceded it by one year. At the time of writing (June 2020), AI strategic documents or working group papers were available in more than 17 countries. The mushrooming of strategic initiatives at the national level is likely to continue as more countries discuss their approaches to AI. This article makes a timely contribution to the policy debates in the field, providing a comparison of emerging governance approaches.
So far, states have not imposed strict limitations on AI-related innovation, but that does not mean they have always been passive players. In many cases, they enabled the creation of markets for AI to thrive in. Oftentimes, they funded basic research that led to advancements exploited by businesses. In recent years, governments also started adopting AI technologies for reforming their own administrations. These moves reflect more complex and profound changes in the governance of emerging technologies, that neither the ‘retreat of the state’ thesis, nor the ‘return of the state’ thesis capture adequately. The next section presents the hybrid governance tenets and their merits and limitations in relation to AI. It dissects the key elements of hybridity, distilling two dimensions for the empirical analysis.
Theoretical insights: hybridity and AI governance
Designing governance systems for all-purpose technology is not an easy task. Based on the understanding of governance as a process of patterned and orderly interactions between various institutions and actors (Biersteker 2010), institutional arrangements can be examined and responsibilities across the governance spectrum can be disentangled. Where high interaction between public and private actors exists, the hybridity thesis is particularly useful to unpack relationships of mutual dependency in situations of uncertainty (Ménard, 2004), in particular for their functional continuity in achieving public goals (Hodge & Greve, 2005) or performing public responsibilities (Chenou & Radu, 2017; Radu, Zingales, & Calandro, 2015). Sociological institutionalists like Crouch (2005) have long regarded this as the norm rather than the exception in advanced capitalist economies. Denis et al. (2015) note a growing hybridity trend in the public sector, with governments in steering roles or in reactive mode, responding to external pressures. Offering a more nuanced conceptualisation of the interactions and overlaps among governance actors, hybridity sheds light on interests, roles and shared understandings that acquire new institutional forms.
According to Skelcher and Smith (2015, p. 436), hybridity is a ‘non-exceptional, but not necessarily universal event’. It can be better explained using the institutional logics approach, which connects normative frames and organizational embodiments in order to identify where agency lies. Hybrid governance not only patches together a multitude of institutional logics, but also blurs the boundaries between institutional forms and actors’ identities. Following Friedland and Alford (1991), the organizing principles, material practices and ideological constructions in each society form the basis for institutional logics, understood as ‘symbolically grounded, organizationally structured, politically defended, and technically and materially constrained’ (p. 248–49). The influential trilogy of hierarchy, market and networks has been used frequently to differentiate new modes of governance (Thorelli, 1986). In an ideal form, each of these modes denotes an operating logic, corresponding to: public interest, for-profit drive and a mix of goals. Mapping these interests for the field of AI is particularly helpful as a basis for applying hybrid governance tenets.
Drawing on theoretical advances from organizational studies, institutional logics expose interests and priorities specific to each sector, although public-private interactions remain widespread and difficult to classify. Cashore et al. (2021) distinguish between these complex and diverse forms of public-private interactions, from coordination and collaboration to substitution and co-optation. Structurally-embedded hybridity is thus different from an instance of delegation from public to private actors, or a functional replacement of the latter by the former. The plural logics on which hybrid governance builds ultimately transform the relations of these actors in two respects. First, the negotiation space and their identities become subject to interdependencies (Greenwood, Díaz, Li, & Lorente, 2010; Thornton & Ocasio, 2008). Second, agency itself changes when confronted with a multitude of institutional logics, impacting the positioning of key actors and related accountability frameworks. In the words of Skelcher and Smith, ‘rather than conceptualizing hybrids descriptively as entities that somehow combine different sectoral characteristics or organizational forms, a theoretically richer approach is to propose that they are carriers of multiple institutional logics’ (2015, p. 439).
Adding the institutional logics perspective helps us address the unresolved tensions of hybridity, which continue to be debated, from the fear that everything becomes hybrid (Goodfellow & Lindemann, 2013) to the explanatory power of the concept (Stepputat, 2013). To overcome this limitation, Canclini (1995) suggested starting with the process of hybridization as an ongoing mixing and reconfiguration of sources of authority and power in response to changing political and economic conditions.
If we take the argument further and apply it to AI governance, two dimensions of hybridity need to be examined. The first dimension requires establishing at what stage hybridization emerges and whether it represents a new property of AI governance systems, impacting future policy directions. The comparative analysis below focuses on how hybridity appears, revealing plural institutional logics at work and instances of coherence or incoherence in their underpinning values and ideologies.
A second axis of exploration is the redefinition of actors’ roles and identities in hybrid configurations, in response to one of the critiques of hybrid theory, noting the need for better specification. The blurring boundaries between the public and the private may give rise to new institutions and to redefined functions. The ambiguity and uncertainty related to the future of AI may structure interactions as a way to preserve freedom of action. From this perspective, the process of function indetermination reflects a decision to allow for open-ended possibilities: ‘tasks so far assigned to the polity can be transposed with increasing ease to a web of “authorities” created for the purpose of making decisions on technical and scientific issues’ (Graz, 2006). Whether such changes are entailed in the national strategies is a key part of the analysis.
Methodological considerations
As more and more countries define their approach to AI challenges, strategic documents are issued at various levels, encapsulating the vision of key players. This exploratory analysis of national strategies assesses instances of hybrid governance, with a focus on institutional dynamics and the redefinition of actors’ roles and identities. A wide array of working group papers, consultations, guidelines and reports precede and inform the design of a national strategy, but the analysis in this article is limited to governmental strategies or national programmes in their final form – with or without committed resources -, which tend to be carefully worded and thus appropriate for a qualitative content analysis.
Among the national strategies released to date, some are general guiding documents, while others are prescriptive plans with clear priorities and funding attached (Dutton, 2018a). This article analyses the latter, because strategic documents embed collective thinking across different levels of government. A two-step methodology was implemented to select cases and specific dimensions of analysis. Starting from a comprehensive list of all national1 strategies released by February 2019, those in English and those for which there was official English translation2 were retained for the analysis. Repositories such as the Future of Life and Tim Dutton’s (2018b) Overview of National AI Strategies were consulted in order to define the final list of cases. Among these, only the strategies dedicated to AI exclusively, as opposed to AI being listed alongside other digital technologies (e.g. Australia’s Innovation Strategy, Denmark’s Digital Growth Strategy), were kept.3 In the group of countries selected for the analysis, a first distinction could be made between the countries presenting the strategy as guidance for industrial policy (e.g. South Korea, United Kingdom) and those adopting a comprehensive approach merging socio-political and economic incentives (e.g. China, France). After the case selection, 12 national documents were retained for the analysis, detailed in Table 1.
Table 1.
Open in new tab
National strategies included in the analysis
Date
Name of strategy
Country
Issued by
April 2016
Artificial Intelligence Information Industry Development Strategy
South Korea
Korean Ministry of Science, ICT and Future Planning
March 2017
Pan-Canadian Artificial Intelligence Strategy
Canada
CIFAR, a Canadian-based charitable organization
May 2017
AI Technology Strategy
Japan
Artificial Intelligence Technology Strategy Council, established by the Japanese Prime Minister Shinzō Abe
May 2017
AI Singapore
Singapore
Smart Nation and Digital Government Office
May 2017
Artificial Intelligence Programme
Finland
Steering group appointed by the Minister of Economic Affairs and Employment
July 2017
Next Generation AI Development Plan
China
State Council of China
October 2017
UAE Strategy for AI
UAE
UAE Government
March 2018
AI for Humanity: French Strategy for Artificial Intelligence
France
French government appointed Cédric Villani to lead the process
March 2018
AI Sector Deal
United Kingdom
Department for Business, Energy and Industrial Strategy & Department for Digital, Culture, Media and Sport
May 2018
National Approach to Artificial Intelligence
Sweden
Ministry of Enterprise and Innovation
November 2018
Federal Government’s Artificial Intelligence Strategy
Germany
Federal Ministry for Economic Affairs and Energy, Federal Ministry of Education and Research & Federal Ministry of Labour and Social Affairs
February 2019
Executive Order on Maintaining American Leadership in Artificial Intelligence/American AI Initiative
United States
US President Donald Trump
Table 1.
Open in new tab
National strategies included in the analysis
Date
Name of strategy
Country
Issued by
April 2016
Artificial Intelligence Information Industry Development Strategy
South Korea
Korean Ministry of Science, ICT and Future Planning
March 2017
Pan-Canadian Artificial Intelligence Strategy
Canada
CIFAR, a Canadian-based charitable organization
May 2017
AI Technology Strategy
Japan
Artificial Intelligence Technology Strategy Council, established by the Japanese Prime Minister Shinzō Abe
May 2017
AI Singapore
Singapore
Smart Nation and Digital Government Office
May 2017
Artificial Intelligence Programme
Finland
Steering group appointed by the Minister of Economic Affairs and Employment
July 2017
Next Generation AI Development Plan
China
State Council of China
October 2017
UAE Strategy for AI
UAE
UAE Government
March 2018
AI for Humanity: French Strategy for Artificial Intelligence
France
French government appointed Cédric Villani to lead the process
March 2018
AI Sector Deal
United Kingdom
Department for Business, Energy and Industrial Strategy & Department for Digital, Culture, Media and Sport
May 2018
National Approach to Artificial Intelligence
Sweden
Ministry of Enterprise and Innovation
November 2018
Federal Government’s Artificial Intelligence Strategy
Germany
Federal Ministry for Economic Affairs and Energy, Federal Ministry of Education and Research & Federal Ministry of Labour and Social Affairs
February 2019
Executive Order on Maintaining American Leadership in Artificial Intelligence/American AI Initiative
United States
US President Donald Trump
Second, the selected strategies were included in the qualitative content analysis on the dimensions of interest here. Cybersecurity scholars such as Carr (2016) and Weiss and Jankauskas (2018) have drawn attention to how national interest and various understandings of security shape the relationship between government and private actors, providing a useful starting point for grasping a state’s general position in contested issue areas. An initial search by keywords pointed to relevant sections in the documents analysed. These included references to ‘public’, ‘authorities’, ‘state’, ‘institutions’, ‘government’, ‘public interest’, ‘private sector’, ‘business’, ‘industry’, ‘role’, ‘responsibility’, ‘policy’. However, keywords were not enough, as references to dimensions of interest were generally scattered anywhere between the preamble and the action points of the document, thus requiring an in-depth analysis of the context to understand particular formulations and nuances.
Despite the fact that these strategies were released one after the other, their content and approach varied according to the priorities identified, ranging from academic excellence (Canada) and skills development (South Korea) to technological sovereignty (Germany). Finland adopted an explicit nation-wide education focus and prioritized AI services for its public administration. To this aim, the Finnish Ministry of Finance launched #AuroraAI, an autonomous applications network to help create the ‘conditions for a people-oriented, proactive society’, built around the real-life events of people and business transactions. Collaborations between various AI sub-programmes have also been established, such as in the case of Canada-France-UK partnerships for research, but they are not covered in this study.
Having explained the research design used to assess how configurations of interests and governance decisions come together for defining a national approach to AI, the next section discussed the findings, stressing hybrid governance elements. It delves into broad trends across AI sovereigntist projects and variation in terms of emerging logics, regulatory approaches and newly-created institutions.
Findings
This study set out to explore how states choose to design AI governance arrangements since 2016. Analyses of recent AI strategies released by for-profits and nonprofits globally (Ulnicane et al., 2020) reveal that various forms of tactical engagement begin to replace ad-hoc responses to the disruptive speed and scope of AI transformations. Many national strategies go in the same direction and discuss how to develop and support scientific research, retain AI talent, and how to enhance skills for future work. Additionally, they all propose – to various extents – the industrialization of AI technologies via sectoral programmes and the uptake of AI by start-ups, and small and medium-sized enterprises. A few countries talk about their ambition to become world leaders in the field (China, the UK, the US) from both a technical and a political standpoint. This section sheds light on broad trends and signification variation in the development and content of national AI strategies, starting with the drafting process.
The national AI strategies analysed here reflect authoritative priorities, directions and allocation of resources that governments have completed in a relative short time span. Yet they vary in scope and length, ranging from sectoral visions of AI development to full-fledged industrial strategies and comprehensive governmental approaches, driven by a multitude of actors. When it comes to the drafting process, a first gap can be noticed between the early adopters of AI strategies and countries in the process of designing their strategies. The first tend to be AI leaders and developed countries, rather than developing countries. Among EU member states, there is coherence around the perceived influence of the block and the need to continue the regional work. While the need for international cooperation is recognized by the majority of countries analysed here, this seems to imply – more often than not – exchanges with countries that are more advanced in AI technologies. The relationship with developing countries is rarely mentioned. One exception is Germany, whose national strategy has an action point around ‘building up capacities and knowledge about AI in developing countries in the context of economic cooperation so that economic and social opportunities can be utilized there’.4
A second limitation of the drafting process is visible in the diversity of means employed to write a national strategy, in many cases not informed by broad consensus: while some countries hosted long sectoral consultations, others delegated the creation of the strategy to one person (e.g. France) or to a group of experts (e.g. Finland). These processes affected the institutional logics observed and the type of hybridity emerging in the field. Corporate representatives were often driving in the drafting process, as the following engagements show: Jérôme Pesenti (Facebook) contributing to the UK AI Sector deal, the former Nokia CEO Pekka Ala-Pietilä working with the team drafting the Finnish approach to AI, and the former startup entrepreneur and investor Chang Byung-gyu leading the South Korean Committee of the Fourth Industrial Revolution, made up of 25 private sector representatives and 5 government officials (Yonhap, 2017).
One important consequence of assigning a dominant place for industry in the drafting process is that hybridity is embedded from the start, without an explicit assumption of power balance between the public and the private sector. Subsequently, in overseeing developments as part of public governance initiatives, industry representatives continue to play an important role, generally constituting at least a third of the members of these bodies. The growing boundary permeability noted by hybrid governance scholars concerns not only sectors, but also practices and, crucially, knowledge. The enduring concentration of emerging technology expertise in private hands (Radu, 2019; Radu et al., 2014) is further accentuated. This complements more advanced commercial strategies of companies driving AI innovation, generally consolidating their position around two poles of (digital) power. In international patent applications, China came second after the US in 2018 (WIPO, 2018). A handful of technology companies from these two countries also have the largest AI research investments and presence within the industry bodies developing standards.
National strategies under the magnifying glass: broad trends
The majority of countries included in the analysis embraced a coordinated approach with a centralized vision of AI development. Plural institutional logics are always at play in the strategies analysed here, many times by design. Nowhere is this more visible than in AI Singapore, a programme led by the National Research Foundation with participants such as Smart Nation and Digital Government Office, the Economic Development Board, the Infocomm Media Development Authority, the state-owned company SGInnovate, and the Integrated Health Information Systems. Japan provides another example of multiple rationalities coming together, explicitly combining the ‘wisdom of industry, academia and government’ in order to ‘build a framework for sustainable social implementation’. The French strategy talks about strategic engagement in four sectors of particular interest to the state (health, transport/mobility, environment and defence/security), deliberately leaving aside other issue areas such as banking and insurance as ‘their development is less a matter of public initiative as it is of private impetus, largely initiated as is, and (…) any State involvement in it would be undesirable’.
Looking at what is missing in these strategies is equally important: from specific details on how the drafting process is conducted to military and surveillance uses of AI. Despite the relevance of emerging technologies for national security, only few countries (China, France and the US) make reference to military interests in the field. Whether explicitly mentioned or not, the pursuit of offensive capabilities underlies a significant part of investments in AI technology.5 According to the 2019AI Global Surveillance Index, at least 75 countries around the world actively use AI for surveillance, primarily for predictive analysis in smart city platforms, smart policing and facial recognition (Feldstein, 2019).
While many strategies engage in an elaborate, but selective discussion of the changing role of the state, only few problematize the distinction between the public and the private interests (e.g. Finland, Japan, South Korea). The strategic leadership of states in the field remains generally disconnected from the impetus to regulate AI more strictly, although there is consensus around data sharing and standardization across the board, from China to Sweden. The Finnish national programme sees the unlocking of AI potential as dependent on both public and private sectors and declares that ‘legislation should naturally also support the change’.
The strong involvement of industry representatives in the expert discussions and AI working groups brings to the surface the perpetuation of functional indetermination. In the absence of an authoritative differentiation, hybrid governance implicitly requires a high reliance on experts via informal mechanisms (Graz, 2014). The mutual influence of different individuals over each other’s decisions through formal and informal rules also characterises notions of pluralistic and non-hierarchical governance present in hybrid configurations.
In the cases analysed here, governments envision broad roles for themselves, such as: leading AI developments worldwide (China, the UK, the US), ensuring technological sovereignty (Germany), overseeing the process of AI adoption (Finland, France), correcting market failures for the most vulnerable (South Korea), or being the first buyer of advanced technologies (UAE). As becomes clear in the Singaporean strategy, the role of the government consists not only in oversight and immediate control, but also in coordinating networks and selecting instruments for policy experimentation. Yet all strategies remain vague on the concrete measures enabling them to act in these roles. Beyond the call for rapidly introducing AI in public administration and modernizing the governmental services in response to the new technological revolution underway, gaps in public investment are often noted with regard to research, human resources, and infrastructure.
This discussion unpacked the logics at play in the development of institutional dynamics and the relatively weak regulatory approach to AI. However, this perspective is incomplete without an exploration of the specific hybrid arrangements set up for governing the field. The interplay between public and private sector is further consolidated in the creation of new institutions, as discussed below.
Variation in the national AI approaches: new roles and institutions
Crucial to the hybridization thesis presented here, the positions and interests of the public and for-profit sectors did not appear to be clearly defined in the national documents analysed, revealing a high degree of functional indetermination. Rather than adopting regulation or presenting a consistent state intervention direction, most governments appear to take a reflexive turn and ponder upon the changes needed. The South Korean strategy deems it ‘critical for policymakers to embrace a new technology regulation paradigm and remove regulatory obstacles to innovation’ and the German strategy refers to the need ‘to factor in the regulatory framework for later use’, singling out healthcare as a priority sector.
Instead of a rule-based system, the 12 national strategies introduced and prioritized an ethics orientation. All the documents analysed here (except for the Chinese and American strategies) place an emphasis on designing ethical principles and guiding developments in a normative direction. The German government promoted the use of ‘ethics by, in and for design’, whereas the French strategy noted that ethical considerations were ‘lagging behind practice, but would be necessary for the acceptance of AI’. Whereas the rules and ethics directions are not contradictory and could potentially co-exist, they represent regulatory regimes built on different values and trust systems. The general reluctance to regulate AI at an early stage is reminiscent of the approach chosen for regulating the Internet only when security and legal problems became widespread (Radu, 2019). It is also tightly linked to the fear of stifling innovation and the complex management of uncertainties inherent to new technologies, explicitly mentioned in some of the strategies.
Importantly, the majority of the nations included in this analysis envisioned the creation of special AI Councils or Data Committees to monitor AI adoption and implementation processes. Oversight bodies or AI councils driving the AI policy mandates were generally dominated by representatives of academia and private sector. Overall, NGOs and rights groups were not equally represented (e.g. UK, Canada). AI strategies rarely included end-users of these technologies as a specific group for policy dialogue. The Finnish strategy is an exception here, specifically mentioning that ‘cooperation would be needed between the private and public sectors as well as with individual people’. The real-time experimentation specific to AI techniques also applies to the governance dynamics envisioned in the broader politico-economic environment restructured by this technology. This is particularly noticeable in the efforts to create new institutions and new programmes to work on AI.
In designing them, hybridity results in a common horizon approach in which market and state actions can no longer be disentangled, as there is a sharing of goals and a growing mutuality and reliance on one another. This may go as far as moving closer to a private sector logic. The French strategy provides an eloquent example: in its proposal to test out sectoral platforms, advantages are weighted and industry moves are mimicked:
‘the digital ecosystem is characterized by an omnipresent “winner takes all” logic and dominant positions seem increasingly difficult to challenge. And the fields covered by AI are no exception, which is why it is up to the public authorities to introduce “platformisation” into these various sectors, if only to avoid value being vacuumed off by a private actor in a paramount position’.
This wording shows that different stages can be distinguished in the discourse presented, oscillating between the acknowledgement of the risk of co-optation and the normalization of hybrid elements in the design of future directions for the country.
Diverse approaches regarding the institutionalization of AI governance can be identified in the national strategies. In a few cases (China, Japan, France, the US), existing ministries were asked to drive and coordinate cross-sector work, with responsibilities in the areas they generally cover. France also proposed the creation of a shared specialist centre of 30 members to help provide specific inputs and implement projects in other departments. The UK diverged from this model by proposing a permanent institution, the Office for AI, as a joint unit situated between the Department of Digital, Culture, Media and Sport and the Department for Business, Energy and Industrial Strategy. The UAE went a step further and designated a Minister of State for AI, to oversee technological reforms in the country.
The governments that focused more on research excellence directed their attention to the new institutes and research programmes to be established. Both Canada and Germany envisioned channelling research efforts through existing bodies such as the nonprofit Canadian Institute for Advancement and Research (CIFAR) and the academia and industry-led German Research Centre for Artificial Intelligence. Finland, Sweden and Canada took a broader approach to the development of skills across their societies, enhancing academic leadership and public awareness. France proposed a European DARPA-style organization for AI, alluding to its early success in developing the Internet, but also embracing a Franco-German collaboration via new cross-border cooperation and research centres.
Alongside these structures, many national strategies introduced formal policy input in the form of loosely defined expert groupings (referred to as ‘independent’ or ‘multistakeholder’), whose final composition was generally not defined at the time of document publication. They were involved in designing ethical principles for working with AI data and providing broader guidance to the government on AI-related priorities. Examples of these abound: a New Centre for Data Ethics and Innovation and an AI Council in the UK, an AI Consultative Council in the UAE, a Data Ethics Commission in Germany, an Advisory Council on the Ethical Use of AI and Data in Singapore, a Committee for Technological Innovation and Ethics in Sweden. The need for international cooperation in the development of ethical frameworks was explicitly brought forward in the French national strategy, which proposes the establishment of a group of experts beyond national borders.
These newly established bodies represent an institutional response to the uncertainty embedded in new technologies, as their competences are not clearly specified and remain dependent on the members selected. The redefinition of roles for the public and private sectors thus takes a new turn, oscillating between integrating AI governance among the competences of existing ministries, setting up new functions and proposing new bodies with vaguely defined mandates. In most cases, these solutions are combined in what becomes an increasingly complicated configuration for the AI field. Institutionally, there is little variation in the forms of hybridity emerging, confirming the deliberate choice for functional indetermination. Against this background, accountability frameworks remain difficult to establish. What is currently missing in the national strategies is a clear indication of who makes the rules and for how long.
Conclusion
As an all-purpose technology deployed in everyday services, AI requires both national and international governance systems. Current policy debates focus on designing a set of ethical principles in ways that elude the core issues at stake in the new distribution of digital power, beyond a ‘race to the bottom’ discourse. This study sought to explain how states chose to design AI governance arrangements based on their vision for a national strategy, providing and furthering theoretical insights from hybridization theory. The co-existence of various institutional logics in AI strategies is a property of nascent AI systems which impacts the adoption of a regulatory approach and gives limited attention to developing an accountability framework.
In the 12 national strategies analysed here, uniting the political will and public resources with the industry interests appears to be the preferred recipe for AI policy development. Publicly-funded research with deployments by start-ups and small and medium size companies remains the main strategy of public engagement Although some documents include scarce references to regulation and to the ‘red lines’ not to be crossed from an ethical perspective, AI industry growth is desired, enabled and facilitated by states all around the world. From China to Germany, very few limitations are imposed on AI development and implementation. Consequently, it becomes increasingly hard to disentangle public interest policies from market dominance interests, a characteristic of plural logic at play in hybrid governance systems.
The emergence of new consultative bodies – with loosely defined mandates – is likely to lead to a greater acceptance of functional indetermination as a mainstream practice for governing AI, which allows for similarly flexible arrangements in the future. It is noteworthy that the interests of governments and industry are closely aligned at the national level. The strong market creation orientation, the vague definition of roles for the public and private sectors, as well as the prioritization of ethical guidelines suggest that hybridity is both an intention of the government and an outcome of the fast AI developments.
Future studies are needed to expand on the effects of the nascent AI ordering, capturing how its early design plays out in the distribution of (digital) power, in particular at the level of private standards and international agendas. Building on this in-depth study of national initiatives, future work should identify the key parameters and dimensions of hybridity permeating the practices of various stakeholders. When it comes to AI innovation, we are reminded that technologically-advanced nations are setting the bar. It is against this background that the AI strategies of developing countries would need to be analysed in the near future.
",https://academic.oup.com/policyandsociety/article/40/2/178/6509308
Navigating the Power of Artificial Intelligence in Risk Management: A Comparative Analysis,MDPI,"Mohammad Yazdi, Esmaeil Zarei, Sidum Adumene, Amin Beheshti","STUDY presents a responsive analysis of the role of artificial intelligence (AI) in risk management, contrasting traditional approaches with those augmented by AI and highlighting the challenges and opportunities that emerge. AI, intense learning methodologies such as convolutional neural networks (CNNs), have been identified as pivotal in extracting meaningful insights from image data, a form of analysis that holds significant potential in identifying and managing risks across various industries",2024,"This study presents a responsive analysis of the role of artificial intelligence (AI) in risk management, contrasting traditional approaches with those augmented by AI and highlighting the challenges and opportunities that emerge. AI, intense learning methodologies such as convolutional neural networks (CNNs), have been identified as pivotal in extracting meaningful insights from image data, a form of analysis that holds significant potential in identifying and managing risks across various industries. The research methodology involves a strategic selection and processing of images for analysis and introduces three case studies that serve as benchmarks for evaluation. These case studies showcase the application of AI, in place of image processing capabilities, to identify hazards, evaluate risks, and suggest control measures. The comparative evaluation focuses on the accuracy, relevance, and practicality of the AI-generated findings alongside the system’s response time and comprehensive understanding of the context. Results reveal that AI can significantly enhance risk assessment processes, offering rapid and detailed insights. However, the study also recognises the intrinsic limitations of AI in contextual interpretation, advocating for a synergy between technological and domain-specific expertise. The conclusion underscores the transformative potential of AI in risk management, supporting continued research to further integrate AI effectively into risk assessment frameworks.
Keywords: artificial intelligence; system safety management; image data analysis; hazard identification; storytelling


1. Introduction
Risk management has always been a cornerstone of organisational strategy, forming the bulwark against the system’s potential financial, strategic, operational, and reputation losses [1,2]. Its importance lies in its ability to identify, assess, and prioritise the risks, followed by resource allocation to minimise, control, and monitor the probability and/or impact of unfortunate events into an acceptable level of the risks or ALARP “as low as reasonably practicable” in some contexts [3,4,5]. In essence, risk management is not just about averting crises; it is about navigating through them with minimal damage and emerging resilient on the other side.
As we are going into the 21st century, the advent of artificial intelligence (AI) in risk management can mark a significant paradigm shift. AI, with its ability to process and analyse large volumes of data at speeds and accuracies unattainable by humans, can become a game-changer. It can enhance capability for the identification of potential risks and opportunities with greater precision, thereby enhancing decision-making processes. Machine learning models, especially those harnessing the power of deep learning, are now fundamental tools in predicting market trends, detecting fraudulent activities, and automating risk assessment tasks [6,7,8,9,10].
The progression of AI has led to immersive developments. In today’s information-driven world, the multifaceted potential of image data as an invaluable decision-making asset has increasingly caught the attention of industries and researchers alike. Image data provides a granular and rich form of information, capturing subtle patterns often elusive to traditional data forms. Its significance in the domain of risk management cannot be understated. As highlighted by Goodfellow et al. [11], deep learning techniques, especially convolutional neural networks (CNNs), have shown substantial promise in gleaning insights from image data for diverse applications.
Moreover, the challenges in harnessing image data effectively are manifold. High dimensionality, potential biases in data, and the need for substantial computational resources often make image data analysis a daunting task. Moreover, as underscored by Bengio [12], the interpretation and contextual understanding of image data are as critical as the computational aspects, particularly in domains like risk management where stakes are high. Recent advancements in machine learning, particularly deep learning, offer promising avenues for comprehensive image data analysis. For instance, LeCun et al. [13] extensively discussed the role of CNNs in image recognition, which could be repurposed for risk analysis by identifying anomalies or patterns indicative of potential risks. The integration of image data into risk management presents a transformative opportunity. For sectors like high-tech industries, where risks often manifest in numerical indicators, the use of image data (e.g., satellite imagery) can forecast potential economic downturns by analysing changes in physical infrastructure. In health, medical imaging can pre-empt potential outbreaks or diagnose illnesses at a scale and precision previously unattainable. Girshick et al. [14] work on object detection can be a boon for security and surveillance, flagging potential physical threats in real time. Furthermore, Zhou et al. [15] emphasised the need to pair technological advancements with domain-specific knowledge. This integration of tech and expertise amplifies the potential applications of image data in risk management, ensuring insights are both precise and actionable. The dynamic interplay of challenges and advancements paints a promising picture for image data in risk management. By embracing this frontier, industries stand to gain unprecedented depth in risk assessment practices, opening avenues for intelligent proactive and informed strategies.
Therefore, the integration of these advanced AI technologies, among which is Chatbot, like ChatGPT-4 (https://openai.com/ accessed on 2 March 2024), an advanced iteration of generative pre-trained transformers developed by OpenAI, into the domain of risk management not only presents transformative opportunities but also introduces a new era of efficiency and effectiveness. For industries ranging from manufacturing, oil and gas, marine, and finance to healthcare, the capabilities of such AI systems can lead to the development of predictive models that identify potential risks with unprecedented accuracy. These AI-driven insights, when combined with domain expertise, can fortify strategies, enabling organisations to respond to threats with agility and informed confidence [16,17]. In recent studies, the application and accuracy of ChatGPT in risk management for construction projects have been extensively examined. Aladağ (2023) [18,19] highlights ChatGPT’s capability to deliver accurate risk-based decisions across various project types, emphasising the importance of key performance indicators (KPIs) in risk management. Hofert (2023) [20] provides an in-depth analysis of ChatGPT’s proficiency in quantitative risk management, offering a numerical assessment of its performance and effectiveness in managing project risks. Furthermore, Al-Mhdawi et al. (2023) [19] focus on evaluating ChatGPT’s performance in the risk management process as per the ISO 31000 standard, showcasing its potential in managing construction risks effectively. Lastly, Nyqvist, Peltokorpi, and Seppänen (2024) [21] explore the capabilities of the ChatGPT GPT-4 model against human experts in construction project risk management, aiming to ascertain if AI can surpass human proficiency in this domain. These studies collectively underscore the evolving role of AI tools like ChatGPT in enhancing the precision and effectiveness of risk management strategies in the construction industry. Building upon the foundational research on ChatGPT’s application in construction risk management, recent scholarly works have broadened the exploration of its capabilities and regulatory needs in various fields. Hacker, Engel, and Mauer (2023) [22] discuss the necessity for a comprehensive risk management framework for large generative AI models like ChatGPT, emphasising the need to address potential high-risk applications and innate vulnerabilities of these technologies. In the realm of finance and accounting, Rane (2023) [23] investigates the role and challenges of ChatGPT and similar AI systems, pointing out the critical need for secure and dependable financial management, while also highlighting AI’s potential in risk management within these sectors [18,24,25]. He et al. [26] explore the critical role of AI in transforming project risk management, a key determinant of success in modern organisations. They examine how AI revolutionises risk identification, evaluation, and mitigation, thereby improving project outcomes. The chapter details how predictive analytics and machine learning mine historical data for patterns, enabling proactive risk management and simulation-driven approaches.
Accordingly, we would like to draw the attention of our esteemed readers to a poignant instance from the past: the 2016 Brussels airport bombings. This tragic event underscores the significant impact that enhanced risk management in image data analysis could have in averting disasters. Specifically, it demonstrates the potential of using AI to identify suspicious activities and unattended items. By analysing image data and integrating it with additional intelligence, AI systems can create comprehensive narratives that provide vital context and insights for security personnel. Such advanced warning systems empower rapid and effective responses to emerging threats, which could prevent harm and save lives. Figure 1 illustrates an end-to-end risk management process using image data analysis. It emphasises the potential of harnessing camera footage from airports to enhance our comprehension of events and activities.


Figure 1. The end-to-end risk management process using image data analysis (modified after AI-generated).
By analysing details from this image, an AI system can specify potential hazards and construct comprehensive risk management that offers valuable context for security personnel. Integrating these data with external knowledge sources, such as social databases and historical police records, the AI system further enhances the risk management depth. This comprehensive understanding allows the system to spotlight potential red flags, identify imminent threats, and offer actionable recommendations for response. As a language model with image input capabilities, such AI tools are not just a step up from their predecessors in understanding and generating text; they are also capable of interpreting visual data, thereby broadening their applicability across various domains. Its understanding of context, ability to process complex instructions, and generate human-like text while also being able to analyse image data make it particularly useful in risk management. We believe that by utilising its advanced capabilities, AI tools can play a pivotal role in hazard identification in diverse workplaces even by sifting through social media and other digital platforms to pinpoint potential safety concerns. It can also analyse satellite imagery to identify environmental hazards, thereby aiding in the early detection and prevention of ecological threats. In the area of workplace health and safety (WHS), they can be instrumental in scrutinising workplace imagery to uncover safety issues (e.g., simple photo shut or panorama), contributing to the enhancement of WHS protocols. Moreover, the interpretation of medical imagery can assist in the early detection of health conditions, facilitating timely interventions [27]. Beyond mere identification, they can also support the development and implementation of targeted control measures, ensuring comprehensive risk mitigation strategies are in place. This not only enhances proactive safety management but also ensures that risks are managed with an informed and responsive approach.
The field of risk management is evolving, driven by technological advancements in AI and the ever-growing capabilities of models like Chatbots. As we navigate through the intricacies of managing risks in an information-driven age, the synergy between AI’s analytical might and human expertise becomes ever more crucial. Accordingly, in this study, our discourse aims to dissect and understand this synergy and explore both the transformative potential and the attendant challenges of leveraging image data for robust risk management strategies [28,29,30,31]. Based on the given context and the objectives of the study, three main research questions that could guide the discourse are as follows:
How do AI technologies enhance the identification and assessment of risks in various industries? This question explores the capabilities relevant to risk management, such as its ability to process and analyse large datasets, including image data, and how these capabilities can be leveraged to improve hazard identification, risk assessment, and decision-making processes within different sectors.
What synergies and potential challenges emerge when integrating human expertise with AI-driven risk management strategies? This question seeks to understand the interplay between human intuition, experience, domain knowledge, and the data-driven insights provided by AI. It also considers the potential barriers to integration, such as resistance to change, interpretability of AI decisions, data privacy concerns, and the need for appropriate governance frameworks.
In what ways does the utilisation of image data, analysed by AI systems, lead to the development of proactive and preventive risk management measures across industries? The focus here is on the predictive power of AI when applied to image data and storytelling, examining the types of control measures that AI analyses can inform. Additionally, it explores the implications for early detection and prevention of risk occurrences, ultimately guiding industries toward more proactive risk management practices.
The structure of the rest of the paper is outlined as follows: Section 2 discusses the role of artificial intelligence in risk management. Section 3 investigates three different case studies through a comparative analysis. In Section 4, a comparative evaluation is conducted. Finally, the paper concludes with a summary of key findings and concluding remarks in the last section.
2. The Role of AI in Risk Management
As mentioned in the introduction, considering the industrial and academic experience and supported by the state of the arts, the field of risk management has traditionally revolved around the identification, analysis, and prioritisation of potential risks, followed by the coordinated application of resources to minimise, monitor, and control the risk of the unfortunate events. Traditional methods rely heavily on historical data and human expertise, with tools like risk registers, risk matrices, failure model and effect analysis (FMEA) [32,33,34], checklists, and qualitative assessments being predominant [35,36,37]. However, the advent of AI-enhanced risk management is a paradigm shift. AI systems, with their superior computational capabilities, can analyse vast quantities of data, uncover patterns invisible to the human eye, and predict future risks with higher accuracy. By juxtaposing traditional risk management with AI-enhanced risk management, we can observe a significant transformation characterised by real-time data processing, predictive analytics, and the ability to adapt to new information.
AI introduces specific capabilities that dramatically enhance risk identification and evaluation processes. Machine learning and data mining provide sophisticated means to analyse large datasets for predictive insights, detect anomalies, and forecast trends. Machine learning algorithms can be trained to recognise complex patterns and make decisions with minimal human intervention [38]. For risk evaluation, AI systems utilise these patterns to predict potential risks and their impacts, enabling organisations to allocate their resources more efficiently. Using these technologies in risk management not only can accelerate the decision-making process but also improve its accuracy, which is paramount in critical risk-laden scenarios.
Chatbot, i.e., GPT4 (from here called AIc), an advanced iteration of the Generative Pre-trained Transformer models, has sophisticated image processing capabilities. It incorporates a deep learning architecture capable of interpreting visual data, drawing inferences, and generating contextual information. This ability is particularly beneficial for risk management applications where visual data, such as images from surveillance cameras or satellite imagery, play a vital role. AIc processes this data through CNNs, adept at handling pixel-based information and extracting features essential for image classification, object detection, and semantic segmentation.
AIc’s interpretation of images is a multi-faceted process involving several stages. Initially, the model pre-processes the image data to normalise and structure it for analysis. Then, using its neural network, AIc identifies various elements within the images, recognising objects and discerning patterns [39]. It can analyse the context within the image, connecting it to its vast knowledge base for comprehensive understanding. Despite its advanced capabilities, AI image recognition has its nuances and limitations. One of the primary concerns is the quality and bias of the training data [40,41]. Suppose the data used to train the model must be more diverse and comprehensive. In that case, the AI system may develop biases or be unable to accurately recognise specific scenarios, leading to potentially significant consequences in risk management situations. Additionally, AI systems may need help interpreting images that contain novel elements or are of low quality, which can result in inaccuracies. The existing literature, as referenced by [25], critiques that AIcs often appear to be “jack of all trades, master of none”. This suggests that while AIcs can perform a wide range of tasks, they may lack depth in any single area of expertise. Consequently, it is crucial for risk assessors to thoroughly evaluate the comprehensiveness and accuracy of AIcs. This rigorous assessment is necessary to ensure that AIcs are not only versatile but also reliable and proficient in their responses and functionalities.
In this study, a stringent set of criteria guided the selection and processing of images for this study to ensure relevance and accuracy. Images were chosen based on their applicability to the risk management scenarios under investigation and were pre-processed to enhance quality and uniformity. This pre-processing included steps such as resolution normalisation, contrast adjustments, and removing irrelevant metadata, thereby facilitating more accurate analysis by the AI system. The methodology incorporates three case studies, each selected for its unique perspective on the application of AI in risk management. These case studies span different industries and risk types, from operational to strategic risks, providing a holistic view of AI’s potential in this domain. Each case study will detail the risks involved, the AI’s role in managing those risks, and the outcomes relative to traditional risk management approaches.
Methodology
The methodology section of the study provides a comprehensive examination of how AI, notably through machine learning and data mining, can augment traditional risk management practices. Traditional methods, which depend on historical data and human expertise, utilise tools such as risk registers and matrices along with qualitative assessments. The AI-driven approach, however, marks a significant shift by utilising sophisticated algorithms to parse through extensive datasets, discern intricate patterns, and forecast potential risks with greater precision.
The focus is primarily on the application of AI technologies, including machine learning and data mining, to bolster risk identification and evaluation efforts. These cutting-edge technologies are adept at scouring through sizable datasets to extract predictive insights, detect anomalies, and project trends, thus considerably enhancing the speed and accuracy of decision-making in risk management.
Illustrating AI’s application in risk management, the study discusses the use of advanced AI models like GPT-4 (AIc), renowned for their refined image processing abilities. AIc employs convolutional neural networks (CNNs) to interpret visual data from sources such as surveillance footage and satellite images, an indispensable capability in risk scenarios where quick and precise image analysis is paramount.
The methodology unfolds as a multi-stage process. It begins with the preprocessing of image data by AIc, which includes standardisation and normalisation techniques to prepare images for analysis. Following this, AIc deploys algorithms for pattern recognition and contextual interpretation within the images. Throughout this process, the study recognises and addresses potential challenges such as data quality and bias, which necessitates a careful evaluation of AIc’s comprehensiveness and precision.
The research methodology adopts a systematic approach to selecting and preprocessing images pertinent to risk management scenarios. The study presents three case studies across various industries to illustrate the versatile potential of AI in risk management contexts. Each case study contrasts AI’s performance in identifying and managing risks against traditional methodologies, providing empirical evidence of AI’s efficacy.
A detailed example within the methodology is the “Annual Airline Gala” case study, where AIc’s algorithms are tasked with identifying potential workplace health and safety (WHS) hazards. In this imaginative scenario, the study meticulously documents AIc’s operational sequence, including the explicit commands used for image processing, the algorithm parameters set for hazard detection, and the computational processes employed to interpret the visual data. This scenario is not only designed to evaluate AIc’s risk assessment capabilities but also its alignment with regulatory standards.
The methodology section outlines every step of the AI operations. For the preprocessing stage, the commands and parameters for image manipulation are enumerated, such as cropping, resizing, and filtering techniques that facilitate enhanced AI analysis. The model training phase involves detailing the architecture of the neural networks used, the training datasets, the validation steps to prevent overfitting, and the testing procedures to measure the model’s performance. Each of these steps is described with sufficient granularity to allow for precise replication in future research endeavours.
Furthermore, the details of the software and tools, including version numbers and configurations, were used throughout the study. This ensures that other researchers can replicate the conditions under which the AIc system was evaluated. By doing so, the work provides clarity on the AI’s operational framework and also sets a standard for methodological transparency and reproducibility in AI-driven risk management research.
3. Case Studies: Comparative Analysis
In this section, we outline a unique distinct case study (Annual Airline Gala), each with unique WHS considerations. We have tasked an AIc to identify all potential WHS hazards within these scenarios, assess the associated risks, and suggest practical control measures that comply with relevant jurisdictional regulations. The objective is to determine the AIc’s utility in recognising and responding to complex WHS issues by adhering to regulatory standards.
Figure 2 is assumed to be an imaginative and highly stylised concept for an event venue suggested and provided by a stakeholder, as three options (i.e., event producer and relevant contractors), themed around aviation and specifically tailored for an airline’s annual celebration. It shows a commercial airplane dissected into various sections, with parts of the aircraft repurposed as functional areas for the event. The front section of the plane is transformed into a staging area, complete with lighting and a podium, likely intended for speakers and presentations of national awards. Rows of seats, which resemble airline seating, are neatly arranged in front of the stage for the guests, mimicking the seating arrangement of an airplane but with more space and luxury. In the middle and rear sections of the plane, dining areas are created with round tables set for a fine dining experience, suggesting that a meal may be part of the event’s proceedings. These tables are elegantly arranged, complete with table settings and centrepieces, which indicate a formal event. The aircraft’s engines serve as unique standing structures on the sides, and the entire setup is placed on what appears to be a blueprint or schematic of an airport or an aircraft layout. This schematic includes markings for various parts of a typical airport, such as gates and jetways, integrating the event space within the theme.


Figure 2. The three floor plans of Annual Airline Gala, assuming suggested by stakeholder designer (AI-generated).
The following assumptions have been taken and defined for AIc:
The event’s execution requires the structuring of three distinct phases: setup (bump in), the event duration (live), and dismantling (bump out).
The venue for the event is situated in New South Wales (NSW), Australia.
The event is designed to be held indoors.
There is no existing historical precedent or detailed descriptions available for an event of this nature.
Next, we asked AIc to carry out compressive risk management within three trials, which lastly updated its information in January 2022. The results of risk management using a simple risk matrix are created in less than a minute and presented in Table 1 (the risk matrix is provided in Appendix A). The risk assessment for the Annual Airline Gala in NSW, Australia, has identified potential hazards ranging from physical (like trips and falls, fire hazards), health-related (food poisoning, disease spread), safety concerns (unauthorised access, emergency egress), and environmental issues (spills, waste, air quality). Control measures are comprehensive, aligning with NSW regulations and involve detailed planning, staff training, infrastructure adjustments, and monitoring to mitigate risks to an acceptable level, with a clear delineation of responsibilities among various designated risk owners. Following that, we performed our risk management for Annual Airline Gala, which was fully performed by subject matter experts (SMEs) and is presented in Table 2. It should be added that risk assessors possess relevant educational knowledge in safety as well as substantial industrial expertise. Additionally, the procedure for developing the risk matrix is detailed in the second part of the Appendix A.
Table 1. The risk management of Annual Airline Gala based on the AIc.


Table 2. The risk management of Annual Airline Gala based on the SMEs (i.e., risk analysts).


Briefly, when we looked at the differences between the results of the risk management—one created by AI (Table 1) and the other crafted by a human expert (Table 2)—we saw distinct approaches in the level of detail and mitigation strategies. The table born out of AI’s capabilities is meticulously organised, listing hazards, potential outcomes, measurable risks, and targeted mitigation steps. It is impressively detailed, featuring actionable measures such as collaborating with emergency responders and maintaining a risk register, which is in line with the best practices of the industry. In contrast, the Table generated by a human expert opts for a wider lens, favouring a narrative style that speaks to broad safety principles such as collective responsibility and constant alertness. While it may not dive into the minutiae as the AI’s output does, it champions conceptual strategies to nurture a culture of safety and collective awareness, which are just as crucial as specific measures.
Structurally, both Tables use a columnar format to classify hazards and mitigation measures. The AI’s rendition is steeped in formal and technical jargon, tailoring it for industry insiders who operate under rigorous regulatory mandates. Meanwhile, the human expert’s rendition employs a more straightforward language, which, while sacrificing some detail, broadens its appeal to a more general audience. The difference in the clarity of information is also telling; the precision of the AI-crafted table cuts through ambiguity, providing a clear-cut guide for implementing safety measures. On the flip side, the human expert’s table offers a sketch that invites further discussion and elaboration, making it a prime tool for teaching rather than immediate procedural application. When it comes down to it, the AI’s Table shines in providing clear directives and structured risk management ready for action. The human expert’s Table, however, excels at laying the groundwork for safety principles and fostering a culture of mindfulness. Within the broader context of a risk management plan, they could play supportive and enriching roles to one another: the AI’s output directing specific safety actions and the human expert’s insights being pivotal for team discussions and promoting an engaged, safety-first attitude among the workforces. In unison, they offer a dual approach that encapsulates the precision of risk control and the essential cultivation of a conscious approach to workplace safety.
In the following section, we conduct a comprehensive comparison of risk management that includes the analysis of image data, considering multiple criteria.
4. Comparative Evaluation
In an era where AI systems like ChatGPT-4 are increasingly employed to supplement human expertise in risk management, it becomes essential to scrutinise their performance critically. This section comprehensively analyses how ChatGPT-4 interprets and responds to risk management tasks across various criteria. We explore the system’s effectiveness in identifying hazards, evaluating risks, and proposing control measures while addressing its limitations and the factors influencing its interpretative capabilities. We consider the role of context as a vital component in the application of AI to risk management, acknowledging that the effectiveness of AI is not only in the processing of image data but also in understanding the context within which the risks occur. To evaluate the performance of ChatGPT-4 in these dimensions, we have established a set of criteria (Figure 3) that cover the accuracy, relevance, practicality, response time, comprehensiveness, and contextual understanding of its output.


Figure 3. Simple bar chart comparing the performance scores of AI and human experts in various risk management criteria.
Completeness of Hazard Identification: This aspect assesses ChatGPT-4’s proficiency in identifying all potential hazards in a given image. The evaluation focuses on the model’s accuracy in detecting risks without missing any (false negatives) and ensuring that it does not erroneously flag harmless elements as hazardous (false positives). This metric is crucial for establishing the reliability of the AI in hazard detection scenarios.
Relevance of Risk Evaluations: This criterion examines how effectively the AI assesses the severity and likelihood of identified risks. It involves evaluating ChatGPT-4’s consistency in risk evaluation across various scenarios, ensuring that its judgments align with the threat levels of different hazards.
Practicality of Proposed Control Measures: Here, the focus is on the feasibility and appropriateness of the control measures suggested by ChatGPT-4. The evaluation checks whether these suggestions align with established industry standards and best practices for hazard mitigation and safety management.
Response Time: In high-risk environments, the speed at which ChatGPT-4 provides hazard assessments is critical. This metric measures the AI’s ability to deliver prompt and timely information, a crucial factor in dynamic and potentially dangerous settings.
Comprehensiveness: This factor evaluates the depth and thoroughness of ChatGPT-4’s feedback on identified hazards. It involves assessing whether the AI provides a detailed analysis of each risk, offering a holistic view of the potential dangers present in the environment.
Contextual Understanding: This element measures ChatGPT-4’s capability to integrate the broader operational context into its risk analysis. It includes understanding different locations, cultural norms, and operational conditions that influence risk perception and the effectiveness of various management strategies.
Key Concerns of Industry and Safety: This new aspect examines ChatGPT-4’s alignment with specific industries’ primary safety concerns and priorities. It assesses the AI’s understanding of industry-specific hazards, compliance with regulatory requirements, and the adaptability of its risk assessments and recommendations to cater to industry-specific safety needs and practices. This ensures that the AI’s hazard assessments are technically accurate and practically relevant to industry requirements.
Selecting these criteria for evaluation allows for a structured and objective review of ChatGPT-4’s utility in practical risk management. It identifies areas where AI can be most beneficial and where human oversight remains indispensable. This introduction sets the stage for a detailed investigation into the capabilities and potential of ChatGPT-4 within the domain of risk assessment and management. In Table 3, the comparative analysis is carried out subjectively.
Table 3. The comparative analysis of AI risk management.


The table provides a structured comparison of an AI system’s capabilities in risk management across six criteria. It begins with “Accuracy of Hazard Identification,” where the AI’s performance is compared to a dataset of annotated images with known hazards. The evaluation indicates that while the AI can identify hazards, there are notable instances where it misses some, suggesting room for improvement in its accuracy, particularly in a bump-in and bump-out. In assessing the “Relevance of Risk Evaluations,” the AI’s assessments are measured against standard industry practices. The AI is highly relevant here, suggesting that its risk evaluations are well-aligned with existing risk matrices and can be considered consistent and reliable.
The “Practicality of Proposed Control Measures” criterion contrasts the AI’s suggested measures with industry standards and expert recommendations. The AI’s suggestions are criticised for being too general and not considering the specific context of New South Wales, Australia. This implies that while the AI may propose control measures, they may only sometimes be practical or applicable in specific regional contexts. The “Response Time” is a highlight for the AI, emphasising its ability to deliver assessments within minutes. This is contrasted with the time it takes for human experts to complete similar tasks, which can take a full day, not including the time required for additional processes such as approvals and meetings.
The “Comprehensiveness” of the AI’s feedback is partially incomplete. This suggests that while the AI provides some information, it only covers some aspects thoroughly and could benefit from offering more detailed feedback. Lastly, the “Contextual understanding measures” provide a fair evaluation of the AI. The AI’s ability to adjust its responses is tested when presented with images in varying contexts. The AI’s performance in this area is deemed fair, indicating that it has a moderate capability to understand and incorporate contextual nuances into its risk assessment process.
Overall, the table reflects a slight view of the AI’s performance in risk management, acknowledging its strengths in relevance and response times while highlighting areas such as accuracy, practicality, and comprehensiveness where it could improve. The AI demonstrates a reasonable level of contextual understanding, although it needs to excel in this area.
In addition to the performance criteria outlined in the comparative analysis of AI in risk management, several challenges need to be highlighted. One of the primary challenges is accountability. When AI systems make decisions or recommendations, it can be not easy to trace the rationale behind those decisions due to the often-opaque nature of AI algorithms. This leads to questions about who is responsible when an AI system fails to identify a risk or suggests an ineffective control measure.
Another significant challenge is data dependency. AI systems require large amounts of high-quality, relevant data to function accurately. In risk management, the availability of such data can be limited, and the AI may perform poorly if trained on incomplete or biased datasets. Bias is an inherent challenge as well. AI systems can inadvertently perpetuate and amplify existing biases present in the training data. Risk management could lead to unequal or unfair risk assessments for different populations or scenarios. The dynamic nature of risk is also a concern. Risks evolve, and an AI system that needs to be continuously updated may become outdated quickly. Ensuring an AI system remains current with the latest risk management practices and data is an ongoing challenge.
Moreover, there’s the issue of interpretability. Stakeholders may need help to interpret the complex outputs of AI systems. For risk management decisions, where clarity and justification are essential, AI’s “black box” nature can be a significant barrier.
Finally, the integration with existing systems and practices poses a challenge. AI tools must be able to integrate seamlessly with the current risk management infrastructure and must be designed to complement and enhance human decision-making processes rather than replace them. This requires careful planning and consideration of human-AI interaction.
These challenges underscore the need for careful consideration and management when integrating AI into risk management. Organisations need to address these issues by establishing clear protocols for accountability, ensuring data quality, actively working to mitigate bias, regularly updating AI systems, enhancing interpretability, and thoughtfully integrating AI into existing workflows.
Incorporating AI into risk management extends beyond technological integration, encompassing significant ethical implications, regulatory challenges, and long-term impacts on industry practices and job roles. Privacy concerns are paramount, as AI systems often handle sensitive data, necessitating responsible use, precise consent mechanisms, and transparency. Additionally, AI must comply with existing and evolving safety and privacy regulations, which vary by region and industry, posing a challenge in maintaining consistent regulatory compliance, even if it is an ongoing process.
Ethical guidelines for AI development and use are critical, focusing on reducing biases, ensuring fairness, and establishing accountability for AI decisions. As AI transforms risk management practices, it enables more predictive analytics and customised solutions, though it also requires new skill sets focused on AI management and ethical considerations. Job roles are likely to shift towards AI-human collaborative positions.
Decision-making processes will become more data-driven due to AI’s analytical capabilities, yet human oversight remains essential, especially in complex scenarios. This shift raises ethical concerns about the impact of automation on employment and the necessity to build public trust in AI’s role in risk management, especially in safety-critical industries.
Finally, the regulatory landscape must rapidly evolve to keep pace with AI advancements, potentially requiring international collaboration to effectively manage AI’s global impact. Overall, integrating AI in risk management presents a multifaceted challenge beyond technology, encompassing ethical, regulatory, and societal dimensions that need careful consideration and proactive management. Ethical considerations in the use of AI applications in risk management are paramount, especially given their potential impact on privacy, fairness, transparency, and accountability. As AI systems are increasingly employed to predict and mitigate risks in safety-critical industries, they raise ethical dilemmas, such as the potential for intrusive surveillance that compromises individual privacy, or the propagation of biases leading to unfair outcomes. The opacity of some AI models can also challenge transparency, making it difficult for stakeholders to understand decision-making processes and assess their fairness. Furthermore, accountability becomes complex when decisions are delegated to AI, raising questions about responsibility in cases of failure or harm. Establishing robust ethical frameworks is essential to guide the implementation of AI in these contexts, ensuring that the technology is used responsibly, respects individual rights, and aligns with societal values, thereby fostering trust and acceptance. These frameworks should address potential conflicts between organisational goals, like efficiency or profit, and ethical imperatives, ensuring that AI contributes positively to societal well-being while mitigating risks.
In addition to that, Table 4 presents the quality assurance (QA) description for evaluating AIc’s performance in construction project risk management. This QA process involves rigorous testing against key performance indicators (KPIs) to ensure accuracy, reliability, and usability. Testers simulate various scenarios to assess AIc’s ability to identify risks, generate responses, adapt to new information, and communicate effectively. Documentation is maintained, and feedback is collected to identify areas for improvement and ensure AIc meets the highest standards of performance.
Table 4. Quality assurance of AIc.


In response to the pivotal need for real-world application and understanding, actionable guidelines that practitioners in the field of risk management can readily apply. We have outlined a series of best practices, assimilated strategies for the effective incorporation of AI into risk management and established clear criteria for the evaluation of AI performance. This guidance is intended to serve as a pragmatic compass for industry professionals, steering the responsible and efficacious utilisation of AI technologies. The recommendations are crafted to not only facilitate the seamless adoption of AI but also to ensure that such integration is in alignment with ethical standards, operational efficiency, and the enhancement of decision-making processes. Through this initiative, our work strives to bridge the gap between theoretical research and practical implementation, empowering practitioners to navigate the AI landscape with confidence and foresight.
Reflecting on the comparison between AI and human expert-driven risk management, it becomes evident that each approach offers unique strengths and insights. As seen in Table 1, AI-generated results provide a detailed, structured view of risk management, highlighting specific hazards, potential outcomes, and targeted mitigation strategies. The AI’s meticulous organisation and detail orientation align with industry best practices, offering actionable measures and a clear path for implementation.
In contrast, the human expert approach, represented in Table 2, adopts a broader perspective, prioritising overarching safety principles and promoting a culture of collective responsibility and alertness. While less granular, this method is invaluable for fostering a safety-conscious environment and encouraging proactive risk management practices.
The two approaches also differ in their communication style and audience engagement. The AI-generated table is characterised by technical jargon, appealing to professionals well-versed in regulatory standards and industry-specific terminologies. On the other hand, the human expert’s table uses more straightforward language, making risk management principles more accessible to a general audience and facilitating educational and discussion-driven applications.
The comparative evaluation in the subsequent section explores into how AI, particularly ChatGPT-4, performs in various risk management criteria. This analysis underscores the importance of evaluating AI systems like ChatGPT-4 beyond their ability to process image data, considering their contextual understanding and alignment with the nuanced realities of risk management.
This study introduces a new evaluation criterion, “Key Concerns of Industry and Safety,” which assesses AI’s alignment with industry-specific safety priorities. This addition acknowledges the necessity of ensuring AI’s recommendations are technically sound, practically relevant, and applicable to specific industry contexts.
AI systems like ChatGPT-4 demonstrate significant potential in enhancing risk management through rapid and detailed analyses, they must be continuously refined and evaluated against human expertise to ensure a balanced and effective risk management strategy. The future of AI in risk management lies in creating a synergistic relationship between AI-generated insights and human expertise, ensuring a comprehensive, accountable, and contextually aware approach to managing risks.
The bar chart (Figure 3) visualises a comparative analysis of AI and human expert performances across six different criteria in risk management. Completeness, relevance, practicality, response time, comprehensiveness, and contextual understanding are evaluated criteria. Each criterion is represented on the x-axis, with the corresponding scores on the y-axis.
In the chart:
AI’s performance is depicted in blue bars, while human experts are represented in red bars.
AI scores higher in relevance, response time, and somewhat in comprehension, indicating its efficiency in quickly processing and analysing risk-related data and providing relevant information.
Human experts excel in practicality, completeness, and contextual understanding, showcasing their ability to provide more nuanced and context-aware risk assessments, which likely stems from their broader understanding and experience in the field.
The gap between AI and human performance in areas like completeness and practicality highlights the importance of human insight in capturing the full spectrum of risks and devising practical, context-specific mitigation strategies.
This chart underscores the complementary strengths of AI and human expertise in risk management, suggesting that integrating both approaches could lead to more robust and effective risk analysis and mitigation.
Our comparative analysis evaluated the performance of AI and human experts in risk management using a quantified scoring methodology. Six criteria were assessed: completeness, relevance, practicality, response time, comprehensiveness, and contextual understanding.
Completeness was measured by the proportion of identified risks from a standardised set, with a score ranging from 0–100. Relevance scores were determined by expert peer review, rating the pertinence of the identified risks to the specific context on a scale of 0–100. Practicality was evaluated based on the applicability and actionability of the proposed risk mitigation strategies, and again, the score was 0–100.
Response time was quantified by measuring the duration from risk presentation to the delivery of a risk assessment, with faster times receiving higher scores. Comprehensiveness was assessed by the number of risk dimensions addressed in each evaluation, with each dimension carrying equal weight in the final score. Lastly, contextual understanding was scored by a panel of risk management professionals, who judged the degree to which each risk assessment considered the specific and subtleties of the case at hand.
Scores were normalised across all criteria to account for inherent differences in scoring mechanisms. The methodology for scoring was designed to minimise subjective bias and ensure a level playing field between AI and human experts. Statistical tests, including t-tests and ANOVA, were employed to determine significant differences between the AI’s and human experts’ scores. The result of this rigorous evaluation is presented in Figure 3, depicting the strengths of AI in processing speed and relevance and the superiority of human experts in providing practical, well-rounded, and contextually nuanced risk assessments.
The pie chart (Figure 4) illustrates the proportional significance of various criteria in risk management. It categorises the importance into six areas: completeness, relevance, practicality, response time, comprehensiveness, and contextual understanding, each represented by a slice of the chart. The percentages indicate the relative importance of each criterion in the overall risk management process, highlighting how different aspects contribute to effective risk assessment and mitigation.


Figure 4. The distribution of importance across different criteria in risk management, showing how each aspect contributes to the overall process.
The line chart (Figure 5) illustrates the comparative trends in the effectiveness of AI and human-driven risk management from 2015 to 2024. The vertical axis represents the effectiveness score, quantifying how well each approach performs in managing risks, while the horizontal axis shows the years.


Figure 5. The trend of AI and human effectiveness in risk management over the years, showing how each has evolved from 2015 to 2024.
AI effectiveness, shown in one line, displays a generally upward trend, indicating improvements in AI’s ability to manage risks over time. This increase suggests advancements in AI technology and its growing capability to handle complex risk management tasks more effectively.
The human effectiveness line, on the other hand, also shows an upward trend but with slight fluctuations. This pattern reflects the human capacity to adapt and improve risk management strategies, although it is influenced by various factors like experience, training, and technological support.
Overall, the chart presents a dynamic view of the evolving landscape of risk management, highlighting how both AI and human approaches have developed over the years, with AI showing a more consistent upward trend in effectiveness [42,43,44]. This trend suggests a promising future for AI in complementing and enhancing human efforts in risk management. From the graph, it can be observed that both AI and human effectiveness exhibit fluctuations over time. AI effectiveness (illustrated with the green line) demonstrates a general upward trend with some variance, indicating an improvement in AI’s capabilities in risk management as technology advances. In contrast, human effectiveness (depicted with the blue line) also shows variability with a generally upward trajectory, reflecting the adaptability and progressive enhancement of human-driven strategies, which may also benefit from evolving technological tools and increasing familiarity with AI systems.
The measurement of effectiveness in Figure 5 is clarified through the elucidation of benchmarks and performance indicators, which may include accuracy, timeliness, cost-effectiveness, and user satisfaction, among other relevant metrics. For replicability, the exact methodologies applied, such as data collection procedures, analysis techniques, and the rationale behind the weighting of different indicators, are also provided. Effectiveness in Figure 5 could be measured based on several possible quantitative and qualitative factors that assess how well AI and human approaches manage risks [45,46]. These may include:
Accuracy: The correctness of the risk assessments and predictions made by AI compared to those made by human experts.
Timeliness: How quickly each approach can identify and respond to risks. For AI, this might involve computational speed and for humans, the speed of decision-making based on analysis.
Cost-Effectiveness: The resources required to implement and maintain AI solutions compared to the costs of traditional human-driven methods.
Adaptability: The ability of the approach to adjust to new and emerging risks. AI might demonstrate this through learning algorithms, while humans may showcase adaptability through experiential learning.
Compliance and Alignment with Standards: The degree to which each approach adheres to existing risk management standards and best practices.
User Satisfaction: Feedback from end-users or stakeholders regarding their confidence in the approach and its outputs.
Coverage and Scope: The breadth of risks that the approach can evaluate and mitigate.
Contextual Understanding: For AI, this would be the extent to which the system can understand and interpret complex contexts and for humans, the application of their experience and judgment.
In a typical analysis for a graph such as Figure 5, these factors would be defined and measured through a combination of data collection methods, including historical performance data, simulations, user surveys, and expert evaluations [47,48,49]. Each factor would be assigned a score, which could be plotted annually to show the trend over time. For transparency and replicability, the study would specify the exact metrics used, the scoring system, and any weighting applied to different factors based on their perceived importance in effective risk management. For instance, if accuracy is considered more critical than cost, it might be given more weight in the final effectiveness score. AI’s scores may rise over time due to improvements in algorithms and processing power, while human scores could fluctuate based on organisational learning and the evolving nature of human-centric risk management practices. By integrating these diverse factors into an overall effectiveness score, the graph provides insights into the comparative performance of AI and humans in risk management across a specific timeframe [50,51].
The methodology for analysing the trend of AI and human effectiveness in risk management, as depicted in Figure 5, began with a clear definition of “effectiveness”. For this study, effectiveness encompassed accuracy in identifying and responding to risks, speed of response, adaptability to different types of risks, and cost-efficiency of the risk management process. Data collection employed a multimodal approach, integrating historical performance data from organisational records, outcomes from risk management simulations, responses from user surveys, and input from expert evaluations. Each method contributed a unique perspective on the effectiveness of AI and human-based risk management. Next, we assigned scores to the collected data. Each element of effectiveness was quantified, typically on a 0–100 scale, with 100 indicating peak effectiveness. These scores were tabulated annually to illustrate the trend over the studied period from 2015 to 2024. A weighting system was introduced to reflect the differential impact of various factors on overall effectiveness. For instance, if the study deemed accuracy more crucial than cost savings, accuracy was given a higher weight in the composite effectiveness score. This scoring and weighting were meticulously documented to maintain the study’s transparency and allow for replication.
The trend analysis considered the evolution of AI scores regarding technological advancements such as improved algorithms and processing power. In contrast, fluctuations in human effectiveness scores were interpreted against organisational learning and the dynamic nature of human-centric risk management practices. The annual scores were plotted to visually represent these trends, showing the progression of both AI and human effectiveness in risk management.
An overall effectiveness score for each year was computed by integrating the weighted factors for AI and human risk management approaches. The final visualisation presented in Figure 5 offered a comparative view of the effectiveness trends. Insights gleaned from the graph indicated the comparative performance of AI and humans. They highlighted specific periods where significant changes occurred, which might correspond with external events or internal advancements in risk management strategies.
To ensure the robustness of our methodology, we grounded our approach in the existing literature and standards within the field of risk management. References [46,47,48,49,50] provided a foundational basis for our scoring system and trend analysis, ensuring that our study aligns with recognised practices and contributes meaningfully to the knowledge of AI and human effectiveness in risk management.
To further enrich the discourse on the integration of AI in risk management the results indicated that, we explored deeper into the practical applications of these technologies, framing them within the context of current industry challenges and opportunities. We provide detailed examples and case studies to illustrate how AI can be utilised in various risk scenarios, offering nuanced insights into its potential to transform traditional practices. These examples serve as a practical guide for professionals, illustrating not just theoretical concepts but also real-world applications and the tangible benefits of AI adoption.
The results also address potential pitfalls and ethical concerns associated with AI in risk management, such as data privacy issues, the risk of bias in AI algorithms, and the dependence on technology which might lead to a devaluation of human expertise. We discuss strategies to mitigate these risks, emphasising the importance of a balanced approach that leverages the strengths of both AI and human judgment. This includes the development of ethical guidelines for AI deployment in risk management, which ensure that AI applications are not only effective but also fair and transparent.
The algorithms used to assess AI capabilities in terms of completeness, relevance, practicality, response time, comprehensiveness, and contextual understanding are also explained. For example, completeness is measured through the AI’s ability to cover all necessary aspects of risk scenarios as compared to human experts, and relevance is assessed based on the AI’s ability to generate outputs that are directly applicable to specific risk contexts. Practicality is evaluated by analysing how feasible it is to implement AI-driven solutions in real-world settings, considering factors like cost, resource requirements, and ease of integration into existing systems. Response time is measured by the speed with which AI systems can deliver insights compared to traditional methods, a critical factor in environments where timeliness can drastically affect outcomes. Comprehensiveness is evaluated by the depth and breadth of the analysis that AI systems can perform, assessing whether these systems can provide a multi-dimensional view of risk that encompasses various factors and interdependencies. Contextual understanding is gauged by examining the AI’s ability to interpret the context of the data and make informed decisions that reflect an understanding of the larger operational and strategic framework.
Each of these criteria is critically important, and it is clarified in the work how these measurements provide a reliable basis for comparing AI with human-driven approaches. By establishing a clear and detailed methodology, the reader’s understanding of the strengths and limitations of AI in risk management is enhanced, offering a balanced view that highlights areas where AI can complement human expertise and where it still requires further development. This comprehensive approach not only supports the conclusions but also contributes to the broader discourse on the integration of AI technologies in risk management strategies.
Furthermore, we expand on the methodological framework used to evaluate AI performance in risk management, as highlighted in Figure 3, Figure 4 and Figure 5. This includes a detailed breakdown of the statistical methods, the selection criteria for data sets, and the algorithms used to assess AI capabilities in terms of completeness, relevance, practicality, response time, comprehensiveness, and contextual understanding. Each of these criteria is critically important and our work aims to clarify how these measurements provide a reliable basis for comparing AI with human-driven approaches.
The results further elaborated on how the effectiveness of risk management strategies, whether AI-driven or human-led, has evolved from 2015 to 2024. This analysis charts the progress made over the years and also anticipates future trends in the field. The line chart in Figure 5 is augmented with additional data points and a discussion on the implications of these trends for the future of risk management. This includes an exploration of how advancements in AI technologies might continue to shape risk management strategies, potentially leading to more predictive and adaptive systems.
In our expanded discussion, we also critically examine the interplay between AI and human expertise in risk management. While AI can provide rapid analyses and insights based on large data sets, human experts contribute contextual intelligence, ethical judgments, and creative problem-solving capabilities that are currently beyond the reach of machine algorithms. By illustrating this synergy through detailed case studies and theoretical reflections, we advocate for a model of co-evolution where AI tools and human skills are continuously integrated and enhanced in response to changing risk landscapes.
5. Conclusions
The study’s comparative analysis sheds light on AI’s impactful role in risk management, focusing on the capabilities of AIc-4 in image data analysis. Advanced learning algorithms, particularly convolutional neural networks, have revolutionised risk assessment by providing quick, comprehensive, and pertinent evaluations. Nonetheless, the paper recognises AI’s shortfalls compared to human precision and contextual understanding. Despite AI’s quick response and relevance, its hazard identification accuracy and the practicality of its suggested measures need refinement, especially for locale-specific contexts like NSW.
Furthermore, the research identifies gaps in AI’s overall comprehensiveness and its partial contextual grasp, which, despite being hopeful, need further development for effective management in ever-changing risk environments. The study underlines several challenges, including accountability, data dependency, inherent biases, the dynamic nature of risks, interpretability, and the difficulty of integrating AI systems. Addressing these issues is vital for AI’s robust contribution to human decision-making in system safety management.
Looking ahead, the study suggests that AI’s role in risk management should evolve by creating synergies with human expertise. To truly complement human judgment, AI systems must be accountable, interpretable, regularly updated, and purged of biases to adhere to industry norms and to cater to the complexities of risk scenarios. It underscores the necessity of continuous research investments to foster a synergistic relationship where AI and human intelligence work in concert, promoting more resilient, just, and efficient risk management approaches.
",https://www.mdpi.com/2313-576X/10/2/42
Powering the Digital Economy: Opportunities and Risks of Artificial Intelligence in Finance,IMF,"El Bachir Boukherouaa, Khaled AlAjmi, Jose Deodoro, Aquiles farias, Rangachary Ravikumar","ARTICLE This paper discusses the impact of the rapid adoption of artificial intelligence (AI) and machine learning (ML) in the financial sector. It highlights the benefits these technologies bring in terms of financial deepening and efficiency, while raising concerns about its potential in widening the digital divide between advanced and developing economies.",2021,"This paper discusses the impact of the rapid adoption of artificial intelligence (AI) and machine learning (ML) in the financial sector. It highlights the benefits these technologies bring in terms of financial deepening and efficiency, while raising concerns about its potential in widening the digital divide between advanced and developing economies. The paper advances the discussion on the impact of this technology by distilling and categorizing the unique risks that it could pose to the integrity and stability of the financial system, policy challenges, and potential regulatory approaches. The evolving nature of this technology and its application in finance means that the full extent of its strengths and weaknesses is yet to be fully understood. Given the risk of unexpected pitfalls, countries will need to strengthen prudential oversight.
1. Introduction
This paper explores the use of artificial intelligence (AI) and machine learning (ML) in the financial sector and the resultant policy implications.1 It provides a nontechnical background on the evolution and capabilities of AI/ML systems, their deployment and use cases in the financial sector, and the new challenges they present to financial sector policymakers.
AI/ML systems have made major advances over the past decade. Although the development of a machine with the capacity to understand or learn any intellectual task that a human being performs is not within immediate grasp, today’s AI systems can perform quite well tasks that are well defined and normally require human intelligence. The learning process, a critical component of most AI systems, takes the form of ML, which relies on mathematics, statistics, and decision theory. Advances in ML and especially in deep learning algorithms are responsible for most of the recent achievements, such as self-driving cars, digital assistants, and facial recognition.2
The financial sector, led by financial technology (fintech) companies, has been rapidly increasing the use of AI/ML systems (Box 1). Recent adoption by the financial sector of technological advances, such as big data and cloud computing, coupled with the expansion of the digital economy, made the effective deployment of AI/ML systems possible. A recent survey of financial institutions (WEF 2020) shows that 77 percent of all respondents anticipate that AI will be of high or very high overall importance to their businesses within two years. McKinsey (2020a) estimates the potential value of AI in the banking sector to reach $1 trillion.
AI/ML capabilities are transforming the financial sector.3 AI/ML systems are reshaping client experiences, including communication with financial service providers (for example, chat bots), investing (for example, robo-advisor), borrowing (for example, automated mortgage underwriting), and identity verification (for example, image recognition). They are also transforming the operations of financial institutions, providing significant cost savings by automating processes, using predictive analytics for better product offerings, and providing more effective risk and fraud management processes and regulatory compliance. Finally, AI/ML systems provide central banks and prudential oversight authorities with new tools to improve systemic risk surveillance and strengthen prudential oversight.
The COVID-19 pandemic has further increased the appetite for AI/ML adoption in the financial sector. BoE (2020) and McKinsey (2020b) find that a considerable number of financial institutions expect AI/ML to play a bigger role after the pandemic. Key growth areas include customer relationship and risk management. Banks are exploring ways to leverage their experience of using AI/ML to handle the high volume of loan applications during the pandemic to improve their underwriting process and fraud detection. Similarly, supervisors relying on off-site intensive supervision activities during the pandemic could further explore AI/ML-supported tools and processes in the post-pandemic era.
The rapid progress in AI/ML development could deepen the digital divide between advanced and developing economies. AI/ML deployment, and the resulting benefits, have been concentrated largely in advanced economies and a few emerging markets. These technologies could also bring significant benefits to developing economies, including enhanced access to credit by reducing the cost of credit risk assessments, particularly in countries that do not have an established credit registry (Sy and others 2019). However, these economies are falling behind, lacking the necessary investment, access to research, and human capital.4 Bridging this gap will require developing a digital-friendly policy framework anchored around four broad policy pillars: investing in infrastructure; investing in policies for a supportive business environment; investing in skills; and investing in risk management frameworks (IMF 2020).
Cooperation among countries and between the private and public sectors could help mitigate the risk of a widening digital divide. So far, global initiatives—including the development of principles to mitigate ethical risks associated with AI (UNESCO 2021; OECD 2019), calls for cooperation on investing in digital infrastructure (see, for example, Google and International Finance Corporation (2020)), and the provision of access to research in low-income countries (see, for example, AI4Good.org)—have been limited. Multilateral organizations could play an important role in transferring knowledge, raising investments, building capacity, and facilitating a peer-learning approach to guide digital policy efforts in developing economies. Similarly, the membership in several intergovernmental working groups on AI (such as the Global Partnership on Artificial Intelligence and the OECD Network of Experts on AI, among others) could be expanded to include less-developed economies.
AI/ML adoption in the financial sector is bringing new unique risks and challenges that need to be addressed to ensure financial stability. AI/ML-based decisions made by financial institutions may not be easily explainable and could potentially be biased. AI/ML adoption brings in new unique cyber risks and privacy concerns. Financial stability issues could also arise with respect to the robustness of the AI/ML algorithms in the face of structural shifts and increased interconnectedness through widespread reliance on few AI/ML service providers. Chapter 2 explores the adoption of AI/ML in the financial sector and possible associated risks, Chapter 3 discusses related policy concerns, and Chapter 4 provides some conclusions.
Box 1.
Artificial Intelligence and Machine Learning Capabilities
Forecasting. Machine learning algorithms are used for forecasting and benefit from using large data sets. They usually perform better than traditional statistical or econometric models.1 In the financial sector, this is used in such areas as credit risk scoring, economic and financial variables forecasting, risk management, and so on.
Natural language processing. Artificial intelligence systems can communicate by understanding and generating human language. Boosted by deep learning and statistical models, natural language processing has been used in the financial sector in such applications as chat bots, contract reviewing, and report generation.
Image recognition. Facial and signature recognition is being used by some financial institutions and financial technology companies to assist with carrying out certain anti-money laundering/combating the financing of terrorism (AML/CFT) requirements (for example, the identification and verification of customers for customer due diligence process), and for strengthening systems security.
Anomaly detection. Classification algorithms can be applied to detect rare items, outliers, or anomalous data. In the financial sector, insider trading, credit card and insurance fraud detection, and AML/CFT are some of the applications that leverage this capability (Chandola, Banerjee, and Kumar 2009).
2. Artificial Intelligence in the Financial Sector
The capability of acquiring large sets of data from the environment and processing it with artificial intelligence (AI) and machine learning (ML) is changing the financial sector landscape. AI/ML facilitates enhanced capacity to predict economic, financial, and risk events; reshape financial markets; improve risk management and compliance; strengthen prudential oversight; and equip central banks with new tools to pursue their monetary and macroprudential mandates.
A. Forecasting
AI/ML systems are used in the financial sector to forecast macro-economic and financial variables, meet customer demands, provide payment capacity, and monitor business conditions. AI/ML models offer flexibility compared to traditional statistical and econometric models, can help explore otherwise hard-to-detect relationships between variables, and amplify the toolkits used by institutions. Evidence suggests that ML methods often outperform linear regression-based methods in forecast accuracy and robustness (Bolhuis and Rayner 2020).
While the use of AI/ML in forecasting offers benefits, it also poses challenges. Use of nontraditional data (for example, social media data, browsing history, and location data) in AI/ML could be beneficial in finding new relationships between variables. Similarly, by using AI natural language processing (NLP), unstructured data (for example, the information in email texts) can be brought into the forecasting process. However, the use of nontraditional data in financial forecasting raises several concerns, including the governing legal and regulatory framework; ethical and privacy implications; and data quality in terms of cleanliness, accuracy, relevancy, and potential biases.
B. Investment and Banking Services
In the financial sector, advances in AI/ML in recent years have had their greatest impact on the investment management industry. The industry has used technology for decades in trading, client services, and back-office operations, mostly to manage large streams of trading data and information and to execute high-frequency trading. However, AI/ML and related technologies are reshaping the industry by introducing new market participants (for example, product customization), improved client interfaces (for example, chatbots), better analytics and decision-making methods, and cost-reduction through automated processes (Box 2).
Compared to the investment management industry, the penetration of AI/ML in banking has been slower. The banking industry has traditionally been at the forefront of technological advancements (for example, through the introduction of ATMs, electronic card payments, and online banking). However, confidentiality and the proprietary nature of banking data have slowed AI/ML adoption. Nonetheless, AI/ML penetration in the banking industry has accelerated in recent years, in part on account of rising competition from financial technology (fintech) companies (including fintech lenders), but also fueled by AI/ML’s capacity to improve client relations (for example, through chatbots and AI/ML-powered mobile banking), product placement (for example, through behavioral and personalized insights analytics), back-office support, risk management, credit underwriting (Box 3), and, importantly, cost savings.5
Box 2.
Artificial Intelligence in Investment Management—Sample Use Cases1
Increased market liquidity provision through a wider use of high-frequency algorithmic trading and more efficient market price formation.
Expanded wealth advisory services by providing personal and targeted investment advice to mass-market customers in a cost-effective manner, including for low-income populations.
Enhanced efficiency with artificial intelligence and machine learning (AI/ML) taking on a growing portion of investment management responsibilities.
More customized investment portfolios based on AI/ML targeted customer experiences.
Development of new return profiles through the use of AI/ML instead of established strategies.
1 See WEF (2018) for a more detailed discussion.
Box 3.
Artificial Intelligence in Credit Underwriting
Artificial intelligence/machine learning (AI/ML) predictive models can help process credit scoring, enhancing lenders’ ability to calculate default and prepayment risks. Research finds that ML reduces banks’ losses on delinquent customers by up to 25 percent (Khandani, Adlar, and Lo 2010). There is also evidence that, give their greater accuracy in predicting defaults, automated financial underwriting systems benefit underserved applicants, which results in higher borrower approval rates (Gates, Perry, and Zorn 2002), as does the facilitation of low-cost automated evaluation of small borrowers (Bazarbash 2019).
AI/ML-assisted underwriting processes enable the harnessing of social, business, location, and internet data, in addition to traditional data used in credit decisions. AI/ML reduces turnaround time and increases the efficiency of lending decisions. Even if a client does not have a credit history, AI/ML can generate a credit score by analyzing the client’s digital footprint (social media activity, bills payment history, and search engine activity). AI/ML also has the potential to be used in commercial lending decisions for risk quantification of commercial borrowers.1 However, financial institutions and supervisors should be cautious in using and assessing AI/ML in credit underwriting and build robust validation and monitoring processes.
1 See Bazarbash (2019) for a discussion of the potential strengths and weaknesses of AI/ML-based credit assessment.
AI/ML introduces new challenges and potential risks. The use of AI/ML in investment and banking depends on the availability of large volumes of good-quality, timely data. With the storage and use of large quantities of sensitive data, data privacy and cybersecurity are of paramount importance. Difficulties in explaining the rationale of AI/ML-based financial decisions is increasingly an important issue as AI/ML algorithms may uncover unknown correlations in data sets that stakeholders may struggle to understand because the underlying causality is unknown. In addition, these models may perform poorly in the event of major and sudden movements in input data resulting in the breakdown of established correlations (for example, in response to a crisis), potentially providing inaccurate decisions, with adverse outcomes for financial institutions or their clients.
C. Risk and Compliance Management
AI/ML advances in recent years are changing the scope and role of technology in regulatory compliance. Regulatory technology (regtech)6 has assumed greater importance in response to the regulatory tightening and rising compliance costs following the 2008 global financial crisis. For the most part, technology has been used to digitize compliance and reporting processes (Arner, Barberis, and Buckley 2017). However, advances in AI/ML over the past few years are reshaping risk and compliance management by leveraging broad sets of data, often in real time, and automating compliance decisions. This has improved compliance quality and reduced costs.
Maturing AI/ML technology has the potential to propel further adoption of regtech in the financial sector. According to a recent global survey, AI/ML is the top technology under consideration among regtech firms (Schizas and others 2019; Figure 1). Increased adoption of AI/ML in regtech has significantly expanded its use cases, cutting across banking, securities, insurance, and other financial services, and covering a wide variety of activities. These include identity verification, anti-money laundering/combating the financing of terrorism, fraud detection, risk management, stress testing, microprudential and macroprudential reporting, as well as compliance with COVID-19 relief requirements (Box 4).

View Full Size
Figure 1.
Top Five Technologies Employed in Regulatory Technology Offerings
Citation: Departmental Papers 2021, 024; 10.5089/9781589063952.087.A001
Source: Schizas and others (2019).
Download Figure
Download figure as PowerPoint slide
Regulators have generally been supportive of the adoption of regtech by regulated financial entities. Various regulators (for example, in Hong Kong Special Administrative Region) have developed strategies to promote the adoption of regtech that include boosting awareness, promoting innovation, and enhancing regulatory engagement within the regtech ecosystem. Even where there are no explicit strategies, many authorities have supported regtech adoption. For example, AI/ML systems, in conjunction with facial and voice recognition and NLP capabilities, could play an important role in helping digital banks secure licenses (for example, in Hong Kong Special Administrative Region and Malaysia) and for digital onboarding of customers.
D. Prudential Supervision
Although decisions will ultimately depend on the judgment of supervisors, there is a place for AI/ML, primarily in data collection and data analytics. Many Financial Stability Board member country authorities are currently using ML and NLP tools in data analysis, processing, validation, and plausibility (FSB 2020). With AI, supervisors can draw deeper insights from any type of data and make more informed, data-driven decisions. AI can identify patterns that humans fail to spot and thereby enhance the quality of supervision. It can also make supervision more agile by flagging anomalies to supervisors in real time (ECB 2019). Additionally, supervisory technology (suptech)7 applications that leverage AI can provide predictive analyses, with the potential to improve the quality of supervision. Despite all its potential, AI/ML is not a silver bullet, and the effectiveness of supervision will always very much depend on human judgement and an organization’s risk culture.
Box 4.
Artificial Intelligence in Regulatory Compliance—Sample Use Cases
Anti-money laundering/combating the financing of terrorism (AML/CFT) compliance. Artificial intelligence/machine learning (AI/ML)-powered technologies, including analysis of unstructured data and consumer behavior, are used to reduce false positives, which constitute the vast majority of AML/CFT alerts, allowing financial institutions to devote more resources to cases that are likely to be suspect.
Mapping and updating regulatory obligations. AI/ML-based applications are helping financial institutions identify and update relevant regulations, reducing costs and improving regulatory compliance in the process.
Conduct risk management. AI/ML and natural language processing are used to monitor sales calls by financial institutions’ employees to ensure compliance with regulatory requirements to accurately disclose the offered financial products’ features and risks.
Stress-testing. Many global banks are using AI/ML-enabled data analytics to improve the analysis of complex balance sheets and stress testing models to meet stress testing regulatory requirements.
COVID-19 relief. AI/ML and robotic process automation are used to process COVID-19-mandated credit relief by processing large volumes of applications and loan documents to determine if the loans are eligible for payment modification terms.
Currently, suptech use cases are mainly concentrated in misconduct analysis, reporting, and data management (Box 5). Virtual assistance, microprudential, macroprudential, and market surveillance make up a smaller share of cases (di Castri and others 2019). AI in market conduct supervision is used in the collection and analytics of structured and unstructured data,8 work-flow automation, and conduct risk profiling and early warnings. Off-site surveillance, on-site inspection, and complaints handling, are areas of interest for market conduct authorities (FinCoNet 2020).
Many supervisory authorities are actively exploring the use of AI/ML systems in their risk-based supervision process (Figure 2). AI/ML in microprudential supervision could improve the assessment of risks, including credit and liquidity risks, and governance and risk culture in financial institutions, allowing supervisors to focus on risk analysis and on forward-looking assessments. Risks and trends could be assessed at the sectoral level, improving the quality of macroprudential supervision. AI/ML systems are also applied for market surveillance purposes to detect collusive behavior and price manipulation in the securities market—potential misconducts that can be especially hard to detect using traditional methods (IIF 2017). The greater use of remote working arrangements as a result of the COVID-19 pandemic is also driving authorities to use technology to improve the effectiveness of remote supervision. Drawing on these experiences, supervisors, post pandemic, might lean more on AI/ML-supported off-site supervision engagements.

View Full Size
Figure 2.
Technologies Used in Supervisory Technology Tools
Citation: Departmental Papers 2021, 024; 10.5089/9781589063952.087.A001
Source: FSB (2020).
Note: DLT = districted ledger technology; “Self-sovereign ID° refers to technologies that give individuals and businesses control of their digital identities.
Download Figure
Download figure as PowerPoint slide
Box 5.
Artificial Intelligence in Supervision—Sample Applications

View Table
The use of AI/ML by supervisory authorities comes with challenges and risks that need to be carefully considered. The effectiveness of AI/ML-driven supervision will depend on data standardization, quality, and completeness, which could be challenging for authorities and regulated institutions alike, particularly when leveraging nontraditional sources of data, such as social media (FSB 2020). Resource and skills gaps could pose a challenge to the effective and safe adoption of AI/ML by supervisory authorities. The personnel pool could be expanded to include AI/ML specialists and data scientists. Finally, deploying AI/ML systems presents supervisors with risks, including those associated with, privacy, cybersecurity, outcome explainability, and embedded bias.
E. Central Banking
AI/ML applications could help central banks implement their mandates, but they have largely been slow to embrace the technology.9 AI/ML systems could help central banks improve their understanding of economic and financial developments, support better-tuned monetary and macroprudential policies, and improve central banks’ operations. They could also strengthen systemic risk monitoring and potentially help predict the buildup of systemic risks and speed up crisis response.10 Notwithstanding these potential benefits, the use of AI/ML processes for policymaking should remain subject to good judgment. Furthermore, AI/ML could provide increased efficiency and better internal control opportunities for central banks, including monitoring of internal operations and resource allocation across functions.11 The technology to develop these applications largely exists; central banks’ slow adoption thereof can be attributed to cultural, political, and legal factors, and lack of adequate capacity (Danielsson, Macrae, and Uthemann 2020). Nevertheless, some central banks are exploring the possibilities of AI/ML (Box 6). Experiments and research by central banks have focused on improving the near-term forecasting capacity and the monitoring of market sentiment to inform policy decisions. Some central banks are also developing applications to improve internal processes and back-office functions (for example, cash management).
AI/ML use in central banking does not seem to raise significant concerns for the time being, but this could change with broader deployment. Expanded use of large nontraditional and unstructured data sets to produce analytics in support of monetary and macroprudential policymaking could expose a central bank to data biases that are usually avoided in sampling methods. ML algorithms are still susceptible to sudden structural shifts in the data from unforeseen events, and the resulting errors could undermine the effectiveness of a central bank’s crisis monitoring and response capacity. Central banks could also face challenges in acquiring quality and representative data as well as data privacy and security issues.12 These concerns would be amplified if the central bank does not have the resources and skills required to safely operate AI/ML or mitigate the risks associated with it.
Box 6.
Artificial Intelligence in Central Banking—Sample Applications
Strengthening nowcasting
Sveriges Riksbank has developed real-time indicators to support its policy analysis. These include investigating whether fruit and vegetable prices, scraped daily from the Internet and machine learning (ML)-processed into an index, could improve the accuracy of short-term inflation forecasts.
Reserve Bank of New Zealand is experimenting with using ML to process large, real-time data sets of about 550 macroeconomic indicators to improve its nowcasts of GDP growth, with results so far outperforming comparable statistical benchmarks.
Assessing market sentiment
Banca d’Italia has developed a real-time tracking system of consumer inflation expectations, using ML and textual analysis of millions of daily Italian Twitter feeds. The Twitter-based indicators seem to be highly correlated with standard statistical measures, provide superior forecasting for survey-based monthly inflation expectations than all other available sources, and accurately anticipate consumers’ expectations.
Bank Indonesia is testing ML-based techniques for identifying the expectation of stakeholders, scraped from published news articles, of Bank Indonesia’s policy rate, in support of the monthly Board of Governor’s meeting.
Monitoring uncertainty
Banco de Mexico staff has built a sentiment-based risk index using an artificial intelligence (AI)/ML system to analyze Twitter messages in response to positive or negative shocks to the Mexican financial sector. The research finds that index shocks correlate positively with an increase in financial market risk, stock market, volatility, sovereign risk, and foreign exchange rate volatility.
Banco Central de Chile staff has developed a daily-frequency index of economic uncertainty for Chile using AI/ML to analyze Twitter feeds. It tracks the level of general disagreement—a proxy for economic uncertainty—on economic developments and policies. The index shows significant spikes that coincide with several episodes of substantial economic uncertainty of both local and international origin.
De Nederlandsche Bank is exploring whether AI is useful for detecting liquidity problems at banks in anticipation of potential deposit runs (Triepels, Daniels, and Heijmans 2018).
Improving internal processes
Banco de España has developed an AI tool for sorting banknotes between fit and unfit for circulation.
3. Risks and Policy Considerations
The rapid deployment of AI/ML systems in finance will have significant impact that will require robust policy responses to ensure the integrity and safety of the financial system. Concerns are rising regarding a number of issues, such as embedded bias in AI/ML systems, the ability to explain the rationale of their decisions, their robustness (particularly with respect to cyber threats and privacy), and their potential impact on financial stability. This chapter discusses those concerns. Annex 2 provides a consolidated profile of risks arising from Al/ML use in the financial sector.
A. Embedded Bias
The growing use of AI/ML in the financial sector, which is highly regulated and where public trust is an essential factor, has stirred discussions on the risk of embedded bias. Friedman and Nissenbaum (1996) defines embedded bias as computer systems that systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others.13 Customer categorization processes used in AI/ML can lead to bias in the financial sector through differentiation in pricing or service quality. Bias in AI/ML decisions is often the result of biased training data that comes from existing biased processes and data sets, which will teach AI/ML models to be biased too (Wang 2016). Data biases, or inaccurate and insufficient information, could result in greater financial exclusion and feed distrust for the technology, especially among the most vulnerable (Sahay and others 2020). Data collection could result in bias in two ways:
	■ The data used to train the system might be incomplete or unrepresentative. For example, predictive algorithms (such as for a loan approval) favor groups that are better represented in the training data, given that there will be less uncertainty associated with those predictions (Goodman and Flaxman 2016).
	■ The data may underpin prevailing prejudices (Hao 2019). For example, Amazon discovered that its internal recruiting tool was dismissing female candidates because it was trained on historical hiring decisions, which favored men over women.14
Human bias can give rise to bias in the algorithm during the design and training of AI/ML systems. For instance, when a researcher decides on the features to include or exclude in the ML model, the choice can be influenced by various psychological, social, emotional, and cultural factors, which in turn influence the researcher’s feedback on the outputs when training the AI/ML system.15
Notwithstanding the potential for bias in AI/ML systems, they may help reduce prevailing biases. Mayson (2019) and Silberg and Manyika (2019) argue that AI/ML systems can reduce human bias in decision making because ML algorithms can eliminate irrational biases that stem from the subjective interpretation of data.16 Miller (2018) notes that AI systems, while susceptible to embedded bias, can still improve the decision-making process by mitigating human bias. Finally, Silberg and Manyika (2019) suggests that, even though many AI/ML systems could be perceived as black boxes, their prediction and decision-making processes could be scrutinized more than those of individuals and, thus, existing biases could be identified and mitigated.
Given that bias may arise as an unintended consequence of AI/ML systems, regulators could view this as a potential source of operational and reputational risks. Financial institutions that deploy AI/ML systems in a significant manner, particularly with regard to the provision of credit and financial services and in risk management, should develop and implement bias mitigation and detection plans as part of their operational risk management framework. Such plans could include adequate assurances about the algorithm’s robustness against systemically generating biased decisions, disclosure of data sources, awareness of potential bias-generating factors in the data, monitoring and evaluation tools, and, more generally, how the institution intends to meet the standing anti-discrimination rules in the context of AI/ML deployment.
Policy responses to AI/ML embedded bias issues could be advanced by developing and deploying a broader framework for the governance and ethical use of AI/ML applications. In recent years, several initiatives have been launched to help develop such a framework. For example, in April 2019 the European Union released its “Ethics Guidelines for Trustworthy AI,” which outlines key requirements for a trustworthy AI system.17 Similarly, in May 2019 the Organisation for Economic Co-operation and Development adopted a set of principles to promote ethical and innovative AI development. These principles contributed to the June 2019 Group of Twenty declaration on “human-centered” AI.
B. Unboxing the “Black Box”: Explainability and Complexity
Explainability of AI/ML systems outcomes is an important issue, particularly when used in the financial sector. ML models are often referred to as black boxes because they are not directly explainable by the user (Guidotti and others 2019). This characteristic could make detection of the appropriateness of ML decisions difficult18 and could expose organizations to vulnerabilities—such as biased data, unsuitable modeling techniques, or incorrect decision making—and potentially undermine the trust in their robustness (Silberg and Manyika 2019).
Explainability is a complex and multifaceted issue. There are several reasons why ML models are frequently considered to be black boxes: (1) they are complicated and cannot be easily interpreted, (2) their input signals might not be known, and (3) they are an ensemble of models rather than a single independent model. Furthermore, stronger explainability might enable outsiders to manipulate the algorithm (Molnar 2021) and create risks in the financial system. Generally, there is a trade-off between model flexibility—which refers to its capacity to approximate different functions and is directly related to the number of parameters of the model—and its explainability. ML models are more flexible and accurate, but are less explainable than, for example, linear models that produce easily interpretable results but are less accurate. Box 7 provides a brief overview of ML explanation methods.
Ongoing research and regulatory initiatives may help address AI/ML explainability issues. The literature points to various levels of explainability related to different stages of the modeling process, or different objectives, such as explaining individual predictions or the overall model behavior (Guidotti and others 2019). To advance the policy discussion, regulators could explore the possibility that financial sector AI/ML models may require different levels of explainability, depending on the impact of the model’s outcome or the governing regulation. Broadly, regulatory guidance that creates a proper framework and strategy for managing explainability risks at different levels is needed for the financial sector. Currently, ML explainability is included in only a few regulatory frameworks as part of the accountability and transparency requirements (for example, in the European Union, Hong Kong Special Administrative Region, Netherlands, and Singapore).
Box 7.
Explaining the “Black Box”
Explainability by design. Some machine learning (ML) models are inherently interpretable by design, such as linear models and decision trees, but this but may constrain their accuracy.
Model inspection. During model development, various techniques could be used for testing the sensitivity of a model to changes in variables. This approach can help avoid unintended bias and other modeling pitfalls, however, the techniques have limited applicability to deployed models.
Individual prediction explanation. After the model is deployed, some methods can provide local approximations that help explain individual predictions.1 For instance, why credit was or was not granted to an individual or why a buy or sell order was posted, thereby making it easier to audit results.
Global model explanation. This method consists of using an interpretable model, such as linear regression or decision tree, to explain the behavior of the more complex ML models used for prediction. The interpretable model would not be used for prediction, but to provide a general explanation of the behavior of the main model. While useful to comprehend the overall behavior of the predictive model, there is no assurance that the surrogate model can fully mimic the more complex model while keeping a reasonable level of explainability.
Models to explain ML models. Research is increasingly focused on developing techniques that use ML models or their outcomes as training input to achieve explainability. This is accomplished by repeatedly disturbing the ML inputs or variables and comparing them to the ML outcome to ascertain the decision-making process.2
1 For example, Shapley values (Shapley 1953) and Lime values (Ribeiro, Singh, Guestrin 2016).
2 See examples of explainability in Cloudera Fast Forward (2020).
C. Cybersecurity
AI/ML adoption increases the scope for cyber threats and brings in new unique cyber risks. In addition to traditional cyber threats from human or software failures, AI/ML systems are vulnerable to novel threats. Such threats focus on manipulating data at some stage of the AI/ML lifecycle to exploit inherent limitations of AI/ML algorithms (Comiter 2019). Such manipulation allows attackers to evade detection and prompts AI/ML to make the wrong decisions or to extract information. Due to their complexity and the potential impact for financial sector institutions, ML models require constant oversight to certify that such attacks are detected consistently and managed promptly. In addition to typical cybersecurity concerns, specific cyber threats to AI/ML can broadly be grouped as:
	■ Data poisoning attacks intend to influence an ML algorithm during the training stage by adding special samples to its training data set. These attacks cause the AI to incorrectly learn to classify or recognize information. Data poisoning also may be used to create Trojan models, which hide malicious actions that wait for special inputs to be activated (Liu, Dolan-Gavitt, and Garg 2018). Data poisoning attacks require privileged access to model and training information, but once executed successfully, and as long as the malicious behavior does not interfere in regular diagnostics tests, infected models may be undetectable.
	■ Input attacks19 allow attackers to introduce perturbations to data inputs and mislead AI systems during operations. For example, attackers could alter images with elements unperceivable to human vision, but which provoke AI/ML image recognition systems to mislabel the images.
	■ Model extraction or model inversion attacks attempt to recover training input and data, or the model itself. Membership inference (a variant) simply aims to check whether a specific data instance was in the training set. Such attacks may be performed as black-box attacks, whereby attackers have merely read-only access to the model, possibly through business application programming interfaces. Current privacy protection laws (for example, the European Union’s General Data Protection Regulation) are not designed to deal with the methods of model inversion attacks, raising privacy and copyright concerns.
AI/ML cybersecurity is increasingly becoming a concern for financial sector regulators. AI/ML cyber threats could undermine the integrity of, and trust in, the financial sector. Corrupted systems could undermine the financial sector’s capacity to accurately assess, price, and manage risks, which could lead to the buildup of unobserved systemic risks. Attackers could also acquire training data sets that contain sensitive financial and personal information.
The regulatory perimeter of cybersecurity requirements in the financial sector could be expanded to cover AI/ML-specific cyber threats. Providers and users of AI/ML applications in the financial sector should be required to put in place, as part of their broader cybersecurity framework, mitigating strategies. These could include detection and reporting systems, robust protection of training data feeds, and strategies to secure model and data privacy.
D. Data Privacy
AI/ML introduces new unique privacy issues. Privacy concerns regarding big data are well known and predate the emergence of AI/ML into the mainstream.20 Tools have been developed to help maintain data anonymity and data subjects’ privacy. Legal data policy frameworks are being put in place across the globe to address these concerns. The robustness of AI/ML models, however, in preventing data leakage from the training data set raises new privacy concerns. For example, AI/ML has the capacity to unmask anonymized data through inferences (that is, deducing identities from behavioral patterns). Similarly, AI/ML may remember information about individuals in the training set after the data is used, or AI/ML’s outcome may leak sensitive data directly or by inference. Tools are being developed to address these issues and strengthen AI/ML models’ robustness in safeguarding sensitive data, but more work is needed, along with an appropriate update to the legal and regulatory framework that requires AI/ML systems and related data sources to adhere to enhanced privacy standards, along with relevant anti-money laundering/combating the financing of terrorism requirements.
E. Robustness
Robust AI/ML algorithms will help build public trust in an AI-driven financial system and safeguard financial stability and the integrity of the financial system. This is particularly important, given the relatively high concentration of AI/ML service providers and the lack of adequate oversight capacity in many jurisdictions to effectively engage financial institutions using AI/ML systems. Several challenges would need to be addressed by the industry and regulators to ensure the robustness of ML models with respect to cyber threats and safeguarding privacy, as discussed above, as well as with respect to their performance. The latter covers issues related to minimizing ML models’ false signals during periods of structural shifts and having in place an appropriate governance over the AI/ML development process.
AI/ML systems in the financial sector have performed well in a data environment that is relatively stable and produces reliable signals, but that could quickly change in periods of rapid structural shift. In a relatively stable environment, AI/ML models are reasonably able to incorporate evolving data trends without significant loss in prediction accuracy. However, they face a more challenging task with structural changes in their data environment, when a previously reliable signal becomes unreliable or when behavioral correlations shift significantly. The recently observed misalignment of AI/ML-generated risk assessments during the COVID-19 pandemic is a good illustration. The crisis negatively affected the performance of ML models because they were not originally trained for such an event. Harker (2020) and BoE (2020) highlight that ML algorithms trained using pre-COVID-19 data may experience performance deterioration, for example, by pointing to aggregate credit scores in the United States, which, despite the record job loss and rising defaults, have improved during the acute part of the crisis, reflecting temporary relief measures that were not captured by the algorithms.
New governance frameworks for the development of AI/ML systems are needed to strengthen prudential oversight and to avoid unintended consequences. Given that the process of creating AI/ML systems is very similar to software development, proper quality control and agility of the process is required. The process could encompass all phases of development, testing, and deployment, and focus on the relevant risks and controls. Checks for embedded bias, data poisoning, other security risks, and performance should comply with best practices of software development, including separation of duties and constant monitoring.
F. Impact on Financial Stability
The widespread deployment of AI/ML systems in the financial sector will be transformational, and their impact on financial stability is yet to be fully assessed. As highlighted above, on one hand, with carefully designed and tested algorithms satisfying a high level of controls to limit risks and performance issues, AI/ML systems may bring increased efficiencies; better assessment, management, and pricing of risks; improved regulatory compliance; and new tools for prudential surveillance and enforcement—all of which will contribute positively to financial stability. On the other hand, AI/ML systems bring new and unique risks arising from the opacity of their decisions, susceptibility to manipulation, robustness issues, and privacy concerns. These could undermine the public’s trust in the integrity and safety of an AI/ML-driven financial system. Furthermore, AI/ML could potentially bring about new sources and transmission channels of systemic risks. More specifically:
	■ AI/ML service providers could become systemically important participants in financial market infrastructure due to the high specialization of AI/ML systems and network effects, which could increase the financial system’s vulnerability to single points of failure.
	■ The concentration of third-party AI/ML algorithm providers could drive greater homogeneity in risk assessments and credit decisions in the financial sector, which, coupled with rising interconnectedness, could create the conditions for a buildup of systemic risks. The likely concentration of data and growing use of alternative data in AI/ML could result in, respectively, the risk of uniformity (herding) and out-of-sample risk that eventually could lead to systemic risk.
	■ The widespread use of AI/ML could potentially increase procyclicality of financial conditions. For instance, credit underwriting and risk management processes are inherently procyclical, reacting to financial conditions in ways that reinforce and amplify those changes (that is, falling default rates lead to more lending, which can further lower default rates). AI/ML may automate and accelerate the procyclicality and potentially obscure it (due to explainability issues).
	■ Moreover, in the case of a tail risk event, an inaccurate risk assessment and reaction by ML algorithms could quickly amplify and spread the shock throughout the financial system and complicate or even undermine the effectiveness of the policy response.
	■ Challenges in interpretation, sustainability of analytical power, and prediction of models raise the concern that economic policies or market strategies based on these models will be challenging to interpret or predict by the relevant counterparties, creating additional asymmetric information in the market, with an uncertain impact on financial stability.
	■ Finally, regulatory gaps could adversely impact financial stability if technological advances outpace existing regulations. Such advances are often led by providers who may fall outside existing regulatory perimeters.21
The rapid evolution of AI/ML has led to a range of regulatory responses. While some jurisdictions have taken a more holistic approach to addressing the issues involved (for example, the Monetary Authority of Singapore22 and De Nederlandsche Bank23), others have concluded that existing regulations and expectations on good governance are sufficient to address the emerging issues. Whether it is by way of new regulations or existing ones, regulators have generally focused on AI/ML governance frameworks, risk management, internal controls, and better controls over the model and data.
Addressing these challenges requires broad regulatory and collaborative efforts. An adequate policy response requires developing clear minimum standards and guidelines for the sector, coupled with stronger focus on securing the necessary technical skills. Given the inherent interconnectivity of issues related to the deployment of AI/ML systems in the financial sector, collaboration among financial institutions, central banks, financial supervisors, and other stakeholders is important to avoid duplication of work and to help counter potential risks. Many leading jurisdictions in the AI/ML sector have relied on well-articulated national AI strategies for promoting AI/ML development while ensuring that regulatory gaps do not materialize. Annex 3 provides an overview of country approaches to developing AI national strategies.
4. Conclusion
The deployment of artificial intelligence (AI) and machine learning (ML) systems in the financial sector will continue to accelerate. This trend is driven by rapid increases in computational powers, data storage capacity, and big data, as well as by significant progress in modeling and use-case adaptations. The COVID-19 pandemic is accelerating the shift toward a more contactless environment and increasingly digital financial services, which will further strengthen the appeal of AI/ML systems to providers of financial services.
Use of AI/ML will bring important benefits but will also raise significant financial policy challenges. AI/ML systems offer financial institutions the potential for significant cost savings and efficiency gains, new markets, and better risk management; bring customers new experiences, products, and lower costs; and offer powerful tools for regulatory compliance and prudential oversight. However, these systems also bring about ethical questions and new unique risks to the financial system’s integrity and safety, of which the full extent is yet to be assessed. The task facing financial sector policymakers is further complicated by the fact that these innovations are still evolving and morphing as new technologies come into play. These developments call for improvements in oversight monitoring frameworks and active engagement with stakeholders to identify possible risks and remedial regulatory actions.
In step with the Bali Fintech Agenda’s call on national authorities to embrace the fintech revolution, regulators should broadly welcome the advancements of AI/ML in finance and undertake the preparations to capture its potential benefits and mitigate its risks. This includes the timely strengthening of institutional capacity, recruiting relevant expertise, building up knowledge, improving external communication with stakeholders, and expanding consumer education. Deployment of AI/ML systems in the financial sector has proven to be most effective when there are national AI strategies in place that involve all relevant public and private bodies.
Cooperation and knowledge sharing at the regional and international level is becoming increasingly important. This would allow for the coordination of actions to support the safe deployment of AI/ML systems and the sharing of experiences and knowledge. Cooperation will be particularly important to ensure that less-developed economies have access to knowledge related to techniques and methods, use cases, and regulatory and supervisory approaches.
Finally, the evolving nature of the AI/ML technology and its applications in finance mean that neither the users, the technology providers and developers, nor the regulators understand, currently, the full extent of the strengths and weaknesses of the technology. Hence, there may be many unexpected pitfalls that are yet to materialize, and countries will need to strengthen their monitoring and prudential oversight.",https://www.elibrary.imf.org/view/journals/087/2021/024/article-A001-en.xml
"Rethinking risk and compliance for the Age of AI
",WEF,"Kay Firth-Butterfield, Lofred Madzou","ARTICLE Artificial Intelligence (AI) has become an imperative for companies across industries. Despite the hype, AI is creating business value and, as a result, is rapidly being adopted around the world. ",2020,"	Artificial Intelligence (AI) is rapidly changing risk management and compliance.
	However, AI can create new types of risks for businesses, such as amplifying bias or leading to opaque decisions.
	Integrated audit software solutions are needed to manage existing and potential risks.
Artificial Intelligence (AI) has become an imperative for companies across industries. Despite the hype, AI is creating business value and, as a result, is rapidly being adopted around the world. Last year, the McKinsey Global Survey reported “a nearly 25 percent year-over-year increase in the use of AI in standard business processes”. The transformative power of AI is already affecting a range of functions, including customer service, brand management, operations, people and culture, and more recently, risk management and compliance.
This latter development should not surprise anyone. At its core, risk management refers to a company’s ability to identify, monitor and mitigate potential risks, while compliance processes are meant to ensure that it operates within legal, internal and ethical boundaries. These are information-intensive activities – they require collecting, recording and especially processing a significant amount of data and as such are particularly suited for deep learning, the dominant paradigm in AI.
Indeed, this statistical technique for classifying patterns – using neural networks with multiple layers – can be effectively leveraged for improving analytical capabilities in risk management and compliance.
AI systems create new types of risks
However, early experience shows that AI can create new types of risks for businesses. In hiring and credit, AI may amplify historical bias against female and minority background applicants, while in healthcare it may lead to opaque decisions because of its black box problem, to name just a few. These risks are amplified by the inherent complexity of deep learning models which may contain hundreds of millions of parameters. This encourages companies to procure third-party vendors’ solutions about which they know little of the inner functioning.

AI is a business imperative, but it's bringing new risk.
Image: Statista/Technalysis Research
Consequently, executives face a fundamental challenge: how to maximise the benefits of AI for various business functions without creating intractable risk and compliance issues?
Previously, we called for the introduction of risk/benefit assessment frameworks to identify and mitigate risks in AI systems. Yet, such frameworks are highly contextual and require high interdisciplinary expertise and multistakeholder collaboration. Not every organisation can afford such talents or have the required processes. Further, it's perfectly reasonable to assume that a given company has deployed different AI solutions for various use cases, each requiring a distinct framework. Designing and keeping track of these frameworks could quickly become an impossible task even for the most experienced risk managers. In this situation, an intuitive response would be to proceed with caution and limit the use of AI for low-risk applications to avoid potential regulatory violations. But, this can only be a temporary solution. In the long run, this would be a self-defeating strategy considering the immense potential of AI for business growth.
So, what is a sensible alternative?
The need for Enterprise Audit Software for AI systems
We argue that maximising the benefits of AI solutions for businesses white mitigating their adverse risks could be partially achieved by using appropriate audit software. There is already a plethora of audit software for ensuring that companies’ processes meet legal and industry standards across industries from finance to healthcare.
What’s needed now is an integrated audit solution which includes the management of risks related to AI. Such a solution should have three core functions:
1. Documenting the behavior of all AI solutions used by a company. This implies monitoring AI solutions and analysing their features distribution to investigate statistical dependencies. Consider the case of an AI solution for hiring: one should have clear insights into which features (e.g. attended university, years of experience, gender, etc.) have the most impact on recommendations.
2. Assessing compliance with a set of defined requirements. Once one understands the outcome of a model (i.e. why a hiring model is making a particular recommendation), it’s important to assess compliance with certain specifications that could range from legislation (such as the EU's Non-Discrimination Law) to organisational guidelines.
3. Enabling cross-department collaboration. This audit software should ease multistakeholder collaboration – especially between risk managers and data scientists who oversee AI solutions – by providing the appropriate information. For instance, risk managers need non-technical explanations about which requirements are met or not, while data science teams may be more interested in the performance characteristics of the model. When a non-compliance issue is identified, the audit software should provide recommendations for the appropriate interventions to the technical teams.

global network is working closely with partners from government, business, academia and civil society to co-design and pilot agile frameworks for governing new and emerging technologies, including artificial intelligence (AI), autonomous vehicles, blockchain, data policy, digital trade, drones, internet of things (IoT), precision medicine and environmental innovations.
Learn more about the groundbreaking work that the Centre for the Fourth Industrial Revolution Network is doing to prepare us for the future.
Want to help us shape the Fourth Industrial Revolution? Contact us to find out how you can become a member or partner.

Developing such audit software for AI systems would go a long way in addressing the risks associated with AI. Yet, responsible AI cannot be fully automated. There is no universal list of requirements that one must meet to mitigate all existing and potential risks, because the context and industry domain will often determine what items are needed. As a consequence, risk managers and their ability to exercise judgment will remain essential. The rise of AI will only enable them to focus on what they do best: engage with other colleagues across departments to design and execute a sound risk-management policy.",https://www.weforum.org/agenda/2020/09/rethinking-risk-management-and-compliance-age-of-ai-artificial-intelligence/